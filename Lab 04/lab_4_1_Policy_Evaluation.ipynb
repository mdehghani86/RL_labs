{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab 4-1 Policy Evaluation",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMNF+Q1jGGOpHOKboLN37v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m7saikat/IE-7374/blob/master/lab_4_1_Policy_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSjcw9R_bnse",
        "colab_type": "text"
      },
      "source": [
        "# **Policy evaluation**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zKjiDCjXp1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tabulate import tabulate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF4cEWNhuF2D",
        "colab_type": "text"
      },
      "source": [
        "Defining a class that constructs a basic MDP problem. A MDP has the following attributes.\n",
        "\n",
        "1. State,\n",
        "2. Possible actions,\n",
        "3. Rewards after each of the action taken.\n",
        "\n",
        "The class initializes a basic grid world as a MDP porblem, with certain features, such as `height` `width` of the grid, which represents the state. Starting position of the agent, `start_state`, `terminal_state` of the agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrpwSEl3WSdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaseGridworld:\n",
        "    \"\"\"\n",
        "    Defines the base class for the Gridworld MDP.\n",
        "    State representation: (x,y); width and height coordinate standard cartisian.\n",
        "    Action representation: (x_offset, y_offset); where offset is distance in the x or y direction from the starting point (0,0).\n",
        "    E.g. (0, 0) is starting point for the agent; x increases in right direction, y increases in up direction.\n",
        "         Action (1, 0) is increments x with +1 and y with 0; Thus state (0,0) + action (1,0) = next_state (1,0). ## RIGHT ACTIONmaention\n",
        "         Action (0, 1) is increments x with 0 and y with +1; Thus state (0,0) + action (0,1) = next_state (0,1).\n",
        "         state (1,2) + action (0,1) = next_state (1,3)\n",
        "    \"\"\"\n",
        "    def __init__(self, width, height, start_state=None, terminal_states=[]):\n",
        "        \"\"\"\n",
        "        Args\n",
        "            width, height -- ints; dimensions of the grid for x and y.\n",
        "            start_state -- tuple; agent start state.\n",
        "            terminal_states -- list of tuples; special states (if any) with specified reward and action (e.g. cliff in Ch 6)\n",
        "        \"\"\"\n",
        "        # note: all coordinates in 0-indexed cartesian x-y; with origin (0,0) in the bottom left\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.start_state = start_state\n",
        "        self.terminal_states = terminal_states\n",
        "\n",
        "        self.reset_state()\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "        # default actions: north, west, south, east\n",
        "        all_actions = [(0,1), (-1,0), (0,-1), (1,0)]\n",
        "        return all_actions\n",
        "\n",
        "    def get_states(self):\n",
        "        return [(x,y) for x in range(self.width) for y in range(self.height)]\n",
        "\n",
        "    def get_state_reward_transition(self, state, action):\n",
        "        # perform action\n",
        "        next_state = np.array(state) + np.array(action)\n",
        "\n",
        "        # clip to grid in case action resulted in off-the-grid state\n",
        "        next_state = self._clip_state_to_grid(next_state)\n",
        "\n",
        "        # make into tuple of ints\n",
        "        next_state = int(next_state[0]), int(next_state[1])\n",
        "\n",
        "        # get reward\n",
        "        reward = self.get_reward(state, action, next_state)\n",
        "        # print (reward)\n",
        "\n",
        "        return next_state, reward\n",
        "    \n",
        "    def get_reward(self, state, action, next_state):\n",
        "        if state in self.terminal_states:\n",
        "            return 0\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    def _clip_state_to_grid(self, state):\n",
        "        '''\n",
        "          Given an interval, values outside the interval are clipped to the interval edges.\n",
        "          For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1.\n",
        "        '''\n",
        "        x, y = state\n",
        "        return np.clip(x, 0, self.width-1), np.clip(y, 0, self.height-1)\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        return tuple(state) in self.terminal_states\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.state = self.start_state\n",
        "        return self.state\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OucIcQAmy4B7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er3b2JFqSrQz",
        "colab_type": "text"
      },
      "source": [
        "Mapping integers to directions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiDC6M1zWtGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------\n",
        "# Display functions\n",
        "# --------------------\n",
        "\n",
        "def action_to_nwse(action):\n",
        "    \"\"\" translate an action from tuple (e.g. (1,0)) to letter coordinates (e.g. '→') \"\"\"\n",
        "    x, y = action\n",
        "    ret = ''\n",
        "    if y == +1: ret += '↑'\n",
        "    if y == -1: ret += '↓'\n",
        "    if x == +1: ret += '← '\n",
        "    if x == -1: ret += '→'\n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VGuNaDlXHTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UniformPolicyAgent:\n",
        "    def __init__(self, mdp, γ=0.9, eps=1e-2, n_iterations=1000):\n",
        "        self.mdp = mdp\n",
        "        self.γ = γ\n",
        "\n",
        "        # initialize values\n",
        "        self.values = np.zeros((self.mdp.width, self.mdp.height))\n",
        "        self.policy = {}\n",
        "\n",
        "        # Iterative policy evaluation algorithm (Ch 4, p 59)\n",
        "        for i in range(n_iterations):\n",
        "            # Variable to hold the values, V(k+1), i.e updated values. \n",
        "            # The function zeros_like is an numpy function that creates a \n",
        "            # similar(same size and shape) zero matrix as that that of the argument \n",
        "            new_values = np.zeros_like(self.values)\n",
        "\n",
        "            for state in self.mdp.get_states():\n",
        "                if state in self.mdp.terminal_states:\n",
        "                    continue\n",
        "\n",
        "                q_values = {}\n",
        "                for action in self.mdp.get_possible_actions(state):\n",
        "\n",
        "                    # uniform action probability: \n",
        "                    action_prob = 1/len(self.mdp.get_possible_actions(state))\n",
        "\n",
        "                    # compute q_value and update value estimate\n",
        "                    q_values[action] = self.compute_q_value(state, action)\n",
        "\n",
        "                    # Bellman equation\n",
        "                    new_values[state] += action_prob * q_values[action]\n",
        "\n",
        "            # if improvement less then eps (after at least 1 iteration), stop iteration\n",
        "            if np.sum(np.abs(new_values - self.values)) < eps:\n",
        "                break\n",
        "\n",
        "            # update values with new_values for the next iteration loop\n",
        "            self.values = new_values\n",
        "\n",
        "            # record optimal policy\n",
        "            self.policy = self.update_policy()\n",
        "\n",
        "    def compute_q_value(self, state, action):\n",
        "        # get next state and reward from the transition model\n",
        "        next_state, reward = self.mdp.get_state_reward_transition(state, action)\n",
        "        return reward + self.γ * self.values[next_state]\n",
        "\n",
        "    def update_policy(self):\n",
        "        policy = {}\n",
        "        for state in self.mdp.get_states():\n",
        "            if state in self.mdp.terminal_states:\n",
        "                continue\n",
        "            q_values = {}\n",
        "            for action in self.mdp.get_possible_actions(state):\n",
        "                q_values[action] = self.compute_q_value(state, action)\n",
        "            policy[state] = [a for a, v in q_values.items() if round(v, 5) == round(max(q_values.values()), 5)]\n",
        "        return policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp0vQ6bhXLEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_state_value_and_policy(iterations = [], γ = 1):\n",
        "    mdp = BaseGridworld(width=4, height=4, terminal_states=[(0,3), (3,0)])\n",
        "\n",
        "    for n_iter in iterations:\n",
        "        agent = UniformPolicyAgent(mdp=mdp, γ=γ, n_iterations=n_iter)\n",
        "\n",
        "\n",
        "        print('#'*120)\n",
        "        print('#', ' '*10, 'k = {}'.format(n_iter))\n",
        "        print('#'*120)\n",
        "\n",
        "        print('V(k) for the random policy:')\n",
        "\n",
        "        print (np.flipud(agent.values.T))\n",
        "      \n",
        "        formatted_grid = tabulate(np.flipud(agent.values.T), tablefmt='grid')\n",
        "        print(formatted_grid)  # transform so (0,0) is bottom-left\n",
        "\n",
        "        grid = [['' for x in range(mdp.width)] for y in range(mdp.height)]\n",
        "        for (x,y), v in agent.policy.items():\n",
        "            grid[y][x] = [action_to_nwse(v_i) for v_i in v]\n",
        "        # invert vertical coordinate so (0,0) is bottom left of the displayed grid\n",
        "        grid = grid[::-1]\n",
        "\n",
        "        print('Greedy policy wrt v(k):')\n",
        "        print(tabulate(grid, tablefmt='grid'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN0B0c8N0Srr",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMzWfmqj0SCH",
        "colab_type": "code",
        "outputId": "bd2110ba-6432-4f2e-dcef-7b75c22c41f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# List of iterations after which you want to know the policy and the state-values\n",
        "iteration_all = [0, 1, 2, 3, 10, 1000]\n",
        "iteration_0 = [0]\n",
        "iteration_1 = [1]\n",
        "iteration_3 = [3]\n",
        "\n",
        "discount= γ = 1\n",
        "\n",
        "compute_state_value_and_policy(iterations=iteration_all, γ=γ)\n",
        "print (\"\"\"\n",
        "# --------------------\n",
        "# Figure 4.1: Convergence of iterative policy evaluation on a small gridworld.\n",
        "# The left column is the sequence of approximations of the state-value function for the random policy\n",
        "# (all actions equally likely). The right column is the sequence of greedy policies corresponding to\n",
        "# the value function estimates (arrows are shown for all actions achieving the maximum, and the numbers\n",
        "# shown are rounded to two significant digits). The last policy is guaranteed only to be an improvement\n",
        "# over the random policy, but in this case it, and all policies after the third iteration, are optimal.\n",
        "# --------------------\n",
        "\"\"\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "########################################################################################################################\n",
            "#            k = 0\n",
            "########################################################################################################################\n",
            "V(k) for the random policy:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "+---+---+---+---+\n",
            "| 0 | 0 | 0 | 0 |\n",
            "+---+---+---+---+\n",
            "| 0 | 0 | 0 | 0 |\n",
            "+---+---+---+---+\n",
            "| 0 | 0 | 0 | 0 |\n",
            "+---+---+---+---+\n",
            "| 0 | 0 | 0 | 0 |\n",
            "+---+---+---+---+\n",
            "Greedy policy wrt v(k):\n",
            "+--+--+--+--+\n",
            "|  |  |  |  |\n",
            "+--+--+--+--+\n",
            "|  |  |  |  |\n",
            "+--+--+--+--+\n",
            "|  |  |  |  |\n",
            "+--+--+--+--+\n",
            "|  |  |  |  |\n",
            "+--+--+--+--+\n",
            "########################################################################################################################\n",
            "#            k = 1\n",
            "########################################################################################################################\n",
            "V(k) for the random policy:\n",
            "[[ 0. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1.  0.]]\n",
            "+----+----+----+----+\n",
            "|  0 | -1 | -1 | -1 |\n",
            "+----+----+----+----+\n",
            "| -1 | -1 | -1 | -1 |\n",
            "+----+----+----+----+\n",
            "| -1 | -1 | -1 | -1 |\n",
            "+----+----+----+----+\n",
            "| -1 | -1 | -1 |  0 |\n",
            "+----+----+----+----+\n",
            "Greedy policy wrt v(k):\n",
            "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
            "|                       | ['→']                 | ['↑', '→', '↓', '← '] | ['↑', '→', '↓', '← '] |\n",
            "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
            "| ['↑']                 | ['↑', '→', '↓', '← '] | ['↑', '→', '↓', '← '] | ['↑', '→', '↓', '← '] |\n",
            "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
            "| ['↑', '→', '↓', '← '] | ['↑', '→', '↓', '← '] | ['↑', '→', '↓', '← '] | ['↓']                 |\n",
            "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
            "| ['↑', '→', '↓', '← '] | ['↑', '→', '↓', '← '] | ['← ']                |                       |\n",
            "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
            "########################################################################################################################\n",
            "#            k = 2\n",
            "########################################################################################################################\n",
            "V(k) for the random policy:\n",
            "[[ 0.   -1.75 -2.   -2.  ]\n",
            " [-1.75 -2.   -2.   -2.  ]\n",
            " [-2.   -2.   -2.   -1.75]\n",
            " [-2.   -2.   -1.75  0.  ]]\n",
            "+-------+-------+-------+-------+\n",
            "|  0    | -1.75 | -2    | -2    |\n",
            "+-------+-------+-------+-------+\n",
            "| -1.75 | -2    | -2    | -2    |\n",
            "+-------+-------+-------+-------+\n",
            "| -2    | -2    | -2    | -1.75 |\n",
            "+-------+-------+-------+-------+\n",
            "| -2    | -2    | -1.75 |  0    |\n",
            "+-------+-------+-------+-------+\n",
            "Greedy policy wrt v(k):\n",
            "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
            "|                       | ['→']                 | ['→']                 | ['↑', '→', '↓', '← '] |\n",
            "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
            "| ['↑']                 | ['↑', '→']            | ['↑', '→', '↓', '← '] | ['↓']                 |\n",
            "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
            "| ['↑']                 | ['↑', '→', '↓', '← '] | ['↓', '← ']           | ['↓']                 |\n",
            "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
            "| ['↑', '→', '↓', '← '] | ['← ']                | ['← ']                |                       |\n",
            "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
            "########################################################################################################################\n",
            "#            k = 3\n",
            "########################################################################################################################\n",
            "V(k) for the random policy:\n",
            "[[ 0.     -2.4375 -2.9375 -3.    ]\n",
            " [-2.4375 -2.875  -3.     -2.9375]\n",
            " [-2.9375 -3.     -2.875  -2.4375]\n",
            " [-3.     -2.9375 -2.4375  0.    ]]\n",
            "+---------+---------+---------+---------+\n",
            "|  0      | -2.4375 | -2.9375 | -3      |\n",
            "+---------+---------+---------+---------+\n",
            "| -2.4375 | -2.875  | -3      | -2.9375 |\n",
            "+---------+---------+---------+---------+\n",
            "| -2.9375 | -3      | -2.875  | -2.4375 |\n",
            "+---------+---------+---------+---------+\n",
            "| -3      | -2.9375 | -2.4375 |  0      |\n",
            "+---------+---------+---------+---------+\n",
            "Greedy policy wrt v(k):\n",
            "+-------------+-------------+-------------+------------+\n",
            "|             | ['→']       | ['→']       | ['→', '↓'] |\n",
            "+-------------+-------------+-------------+------------+\n",
            "| ['↑']       | ['↑', '→']  | ['→', '↓']  | ['↓']      |\n",
            "+-------------+-------------+-------------+------------+\n",
            "| ['↑']       | ['↑', '← '] | ['↓', '← '] | ['↓']      |\n",
            "+-------------+-------------+-------------+------------+\n",
            "| ['↑', '← '] | ['← ']      | ['← ']      |            |\n",
            "+-------------+-------------+-------------+------------+\n",
            "########################################################################################################################\n",
            "#            k = 10\n",
            "########################################################################################################################\n",
            "V(k) for the random policy:\n",
            "[[ 0.         -6.13796997 -8.35235596 -8.96731567]\n",
            " [-6.13796997 -7.73739624 -8.42782593 -8.35235596]\n",
            " [-8.35235596 -8.42782593 -7.73739624 -6.13796997]\n",
            " [-8.96731567 -8.35235596 -6.13796997  0.        ]]\n",
            "+----------+----------+----------+----------+\n",
            "|  0       | -6.13797 | -8.35236 | -8.96732 |\n",
            "+----------+----------+----------+----------+\n",
            "| -6.13797 | -7.7374  | -8.42783 | -8.35236 |\n",
            "+----------+----------+----------+----------+\n",
            "| -8.35236 | -8.42783 | -7.7374  | -6.13797 |\n",
            "+----------+----------+----------+----------+\n",
            "| -8.96732 | -8.35236 | -6.13797 |  0       |\n",
            "+----------+----------+----------+----------+\n",
            "Greedy policy wrt v(k):\n",
            "+-------------+-------------+-------------+------------+\n",
            "|             | ['→']       | ['→']       | ['→', '↓'] |\n",
            "+-------------+-------------+-------------+------------+\n",
            "| ['↑']       | ['↑', '→']  | ['→', '↓']  | ['↓']      |\n",
            "+-------------+-------------+-------------+------------+\n",
            "| ['↑']       | ['↑', '← '] | ['↓', '← '] | ['↓']      |\n",
            "+-------------+-------------+-------------+------------+\n",
            "| ['↑', '← '] | ['← ']      | ['← ']      |            |\n",
            "+-------------+-------------+-------------+------------+\n",
            "########################################################################################################################\n",
            "#            k = 1000\n",
            "########################################################################################################################\n",
            "V(k) for the random policy:\n",
            "[[  0.         -13.99001852 -19.98520925 -21.98344828]\n",
            " [-13.99001852 -17.98697022 -19.98530819 -19.98520925]\n",
            " [-19.98520925 -19.98530819 -17.98697022 -13.99001852]\n",
            " [-21.98344828 -19.98520925 -13.99001852   0.        ]]\n",
            "+----------+----------+----------+----------+\n",
            "|   0      | -13.99   | -19.9852 | -21.9834 |\n",
            "+----------+----------+----------+----------+\n",
            "| -13.99   | -17.987  | -19.9853 | -19.9852 |\n",
            "+----------+----------+----------+----------+\n",
            "| -19.9852 | -19.9853 | -17.987  | -13.99   |\n",
            "+----------+----------+----------+----------+\n",
            "| -21.9834 | -19.9852 | -13.99   |   0      |\n",
            "+----------+----------+----------+----------+\n",
            "Greedy policy wrt v(k):\n",
            "+-------------+-------------+-------------+------------+\n",
            "|             | ['→']       | ['→']       | ['→', '↓'] |\n",
            "+-------------+-------------+-------------+------------+\n",
            "| ['↑']       | ['↑', '→']  | ['→', '↓']  | ['↓']      |\n",
            "+-------------+-------------+-------------+------------+\n",
            "| ['↑']       | ['↑', '← '] | ['↓', '← '] | ['↓']      |\n",
            "+-------------+-------------+-------------+------------+\n",
            "| ['↑', '← '] | ['← ']      | ['← ']      |            |\n",
            "+-------------+-------------+-------------+------------+\n",
            "\n",
            "# --------------------\n",
            "# Figure 4.1: Convergence of iterative policy evaluation on a small gridworld.\n",
            "# The left column is the sequence of approximations of the state-value function for the random policy\n",
            "# (all actions equally likely). The right column is the sequence of greedy policies corresponding to\n",
            "# the value function estimates (arrows are shown for all actions achieving the maximum, and the numbers\n",
            "# shown are rounded to two significant digits). The last policy is guaranteed only to be an improvement\n",
            "# over the random policy, but in this case it, and all policies after the third iteration, are optimal.\n",
            "# --------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}