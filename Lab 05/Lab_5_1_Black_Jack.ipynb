{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_5_1_Black_Jack.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5fraZKvN711",
        "colab_type": "text"
      },
      "source": [
        "## Black Jack - Monte Carlo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6v2qX6u3sRs",
        "colab_type": "text"
      },
      "source": [
        "### Importing Libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwNzL5iuXFHg",
        "colab_type": "text"
      },
      "source": [
        "#### **Description of additional libraries used in this lab**\n",
        "\n",
        "For this lab we need the following additional libraries.\n",
        "\n",
        "* **sys** : We use this library to print the number of episodes the game is played\n",
        "* **gym** : RL Toolkit library\n",
        "* **defaultdict**: A defaultdict works exactly like a normal dict, but it is initialized with a function that takes no arguments and provides the default value for a nonexistent key. \n",
        "* **Axes3D**: Python library to plot 3D images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtwnzDs16cuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkcLJAW8K5fu",
        "colab_type": "text"
      },
      "source": [
        "### 1.Creating gym environment\n",
        "* **Gym**: gym is an open-source RL toolkit which helpes simulating a lot of RL scenarios. Gym can provide us environments for all the labs that we have covered till now. The documentation for gym can be found [here](https://gym.openai.com/docs/). \n",
        "\n",
        ">In this lab we have used `gym` to get a environmental setup of the blackjack game. This is the github [link](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py) to the code for the blackjack environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS1OgJOi7wq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate the env using the make() method\n",
        "env = gym.make('Blackjack-v0')\n",
        "\n",
        "# i.e., we can see total number of actions (action_space) in Blackjack are 2, Hit or Stick\n",
        "env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSl12oWRbNag",
        "colab_type": "text"
      },
      "source": [
        "### 2. Playing a single game of blackjack\n",
        "\n",
        "A basic blackjack game comprises, the following:-\n",
        "\n",
        "* Reward: 0, -1, 1 for draw, loss and win\n",
        "\n",
        "* Action: Action can be hit or miss\n",
        "\n",
        "* Policy: An arbitarity policy.\n",
        "> In this lab, we follow a stochastic policy, whereby if the players hand is more than 18, then the probability that he/she would Stick is 0.8 and that he would hit is 0.2.\n",
        "> However if it is < 18 the probabliity that he/she would Stick is 0.2 and that he would hit is 0.2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU0einb--FhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_episode(env):\n",
        "    episode = []\n",
        "\n",
        "    # Initialize (reset) the game: Blackjack is by nature is a \"Exploring Starts\", \n",
        "    # which means each game starts with a random state\n",
        "    state = env.reset()\n",
        "    while True:\n",
        "      \n",
        "        # This probability represents the initial policy \n",
        "        probs = [0.8, 0.2] if state[0] > 18 else [0.2, 0.8]\n",
        "\n",
        "        # Randomly chooses an action in between [0,1] i.e in between stick and hit\n",
        "        action = np.random.choice(np.arange(2), p=probs)\n",
        "\n",
        "        # Get the next state of card ant its associated reward(-1, 0, or 1)\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Append the state, action and reward to the te episode array, which keeps log of every turn\n",
        "        # This continues until the Terminal State is reach (Episode is complete)\n",
        "        episode.append((state, action, reward))\n",
        "        \n",
        "        # Update \"state\" to \"newstate\"\n",
        "        state = new_state\n",
        "        if done:\n",
        "            break\n",
        "    return episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4jf1qZSIFhU",
        "colab_type": "text"
      },
      "source": [
        "#### 2-1: An Example of Episode\n",
        "\n",
        "[((20, 10, False), 0, 0.0)] : Where 20 is player's hand, 10 is dealer's hand and False means that player's hand is not bust or player hits\n",
        "\n",
        "[((20, 5, True), 0, 1.0)]:  Where 20 is player's hand, 10 is dealer's hand and True means that player's hand is bust or player sticks\n",
        "\n",
        "[((8, 1, False), 1, 0), ((16, 1, False), 1, -1)]: This means that the player had a draw match and then in the next step the player looses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o8ReJPDIB30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "episode = play_episode(env)\n",
        "# each eposidoe x contains x=((players hand,dealer hand,done), action, reward)\n",
        "# If episode is not done, you may see a sequence of s,a,r until you get \"done=True\"\n",
        "print(episode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEhzfDj4U1xL",
        "colab_type": "text"
      },
      "source": [
        "### 3. Monte-carlo prediction (Episodic Tasks)\n",
        "This is the primary method. Plays through several episodes of the environment. This function takes in the number of episodes, discount factor and the blackjack environment and runs a game for `num_epoisode` times. This function plays one game at a time. Having played that game, it runs the function to update `Q-values` for that particular episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "25ADX0YmbFON",
        "colab": {}
      },
      "source": [
        "def mc_predict(env, num_episodes, gamma=1.0):\n",
        "    # Array for storing/managing the total rewards for a state action pair\n",
        "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    # Number of times we have seen the same action pair\n",
        "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    # Average return\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    # Printing the episode number during run time\n",
        "    for i_episode in range(1, num_episodes+1):\n",
        "        episode = play_episode(env)\n",
        "        update_Q(episode, Q, returns_sum, N, gamma)\n",
        "        \n",
        "        # This part is for printing purpuses\n",
        "        if i_episode % 1000 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
        "            sys.stdout.flush()\n",
        "    return Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BABW7LIfmgGM",
        "colab_type": "text"
      },
      "source": [
        "### 3. Updating `Q-Values` : \n",
        "\n",
        "For each time step in the episode we carry out the first visit monte carlo method, checking if this is the first index of this state. Get the discounted reward and add it to the total reward for that state/action pair. Increment the times we have seen this state action pair and finally update the `Q-values`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q80xNtB-JZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_Q(episode, Q,returns_sum, N, gamma=1.0):\n",
        "    for s, a, r in episode:\n",
        "            # Using first visit approach: Obtain the index of the state s that visted for the first time in the episode\n",
        "            # For each iteration and episode\n",
        "            for i,x in enumerate(episode):\n",
        "              # Fetch the state which corresponds to x\n",
        "              if x[0] == s:\n",
        "                # set state_idx to index of x, i.e. back when it occurs\n",
        "                state_idx = i\n",
        "\n",
        "            # Calculating the discounted rewards:  G <- Î³G + R(t+1) for all states \n",
        "            G = 0\n",
        "            for i,x in enumerate(episode[state_idx:]):\n",
        "             G += x[2]*(gamma**i)\n",
        "\n",
        "            # Updating and storing the total reward for a particular state action pair\n",
        "            returns_sum[s][a] += G\n",
        "\n",
        "            # Number of times we have seen the same action pair \n",
        "            N[s][a] += 1.0\n",
        "            \n",
        "            # Calculating average return\n",
        "            Q[s][a] = returns_sum[s][a] / N[s][a]\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZX51U2BfDcQ",
        "colab_type": "text"
      },
      "source": [
        "Plays a single episode with a set policy in the environment given. Records the state, action and reward for each step and returns the all timesteps for the episode.\n",
        "\n",
        "The `env.reset` function starts a fresh game of blackjack, and it returns the initial or starting state of the game.\n",
        "\n",
        "The `env.step` function gets the next state of card, associated reward(-1 or 0) and if game status(over or not) from the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3OYfoKjK-7d",
        "colab_type": "text"
      },
      "source": [
        "### Function for plotting the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dMJ6BC3_njj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_blackjack_values(V):\n",
        "\n",
        "    def get_Z(x, y, usable_ace):\n",
        "        if (x,y,usable_ace) in V:\n",
        "            return V[x,y,usable_ace]\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def get_figure(usable_ace, ax):\n",
        "        x_range = np.arange(11, 22)\n",
        "        y_range = np.arange(1, 11)\n",
        "        X, Y = np.meshgrid(x_range, y_range)\n",
        "        \n",
        "        Z = np.array([get_Z(x,y,usable_ace) for x,y in zip(np.ravel(X), np.ravel(Y))]).reshape(X.shape)\n",
        "\n",
        "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
        "        ax.set_xlabel('Player\\'s Current Sum')\n",
        "        ax.set_ylabel('Dealer\\'s Showing Card')\n",
        "        ax.set_zlabel('State Value')\n",
        "        ax.view_init(ax.elev, -120)\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    ax = fig.add_subplot(211, projection='3d')\n",
        "    ax.set_title('Usable Ace')\n",
        "    get_figure(True, ax)\n",
        "    ax = fig.add_subplot(212, projection='3d')\n",
        "    ax.set_title('No Usable Ace')\n",
        "    get_figure(False, ax)\n",
        "    plt.show()\n",
        "\n",
        "def plot_policy(policy):\n",
        "\n",
        "    def get_Z(x, y, usable_ace):\n",
        "        if (x,y,usable_ace) in policy:\n",
        "            return policy[x,y,usable_ace]\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    def get_figure(usable_ace, ax):\n",
        "        x_range = np.arange(11, 22)\n",
        "        y_range = np.arange(10, 0, -1)\n",
        "        X, Y = np.meshgrid(x_range, y_range)\n",
        "        Z = np.array([[get_Z(x,y,usable_ace) for x in x_range] for y in y_range])\n",
        "        surf = ax.imshow(Z, cmap=plt.get_cmap('Pastel2', 2), vmin=0, vmax=1, extent=[10.5, 21.5, 0.5, 10.5])\n",
        "        plt.xticks(x_range)\n",
        "        plt.yticks(y_range)\n",
        "        plt.gca().invert_yaxis()\n",
        "        ax.set_xlabel('Player\\'s Current Sum')\n",
        "        ax.set_ylabel('Dealer\\'s Showing Card')\n",
        "        ax.grid(color='w', linestyle='-', linewidth=1)\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "        cbar = plt.colorbar(surf, ticks=[0,1], cax=cax)\n",
        "        cbar.ax.set_yticklabels(['0 (STICK)','1 (HIT)'])\n",
        "            \n",
        "    fig = plt.figure(figsize=(15, 15))\n",
        "    ax = fig.add_subplot(121)\n",
        "    ax.set_title('Usable Ace')\n",
        "    get_figure(True, ax)\n",
        "    ax = fig.add_subplot(122)\n",
        "    ax.set_title('No Usable Ace')\n",
        "    get_figure(False, ax)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ3DYxnGMINR",
        "colab_type": "text"
      },
      "source": [
        "### Running the monte carlo update on 500000 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGl3DxeX-OAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#predict the policy values for our test policy\n",
        "Q = mc_predict(env, 500000)\n",
        "#get the state value function for our test policy\n",
        "V_to_plot = dict((k,(k[0]>18)*(np.dot([0.8, 0.2],v)) + (k[0]<=18)*(np.dot([0.2, 0.8],v))) \\\n",
        "         for k, v in Q.items())\n",
        "\n",
        "# plot the state value functions\n",
        "plot_blackjack_values(V_to_plot)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}