{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_05_1_Blackjack_Monte_Carlo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 5-1: Blackjack with Monte Carlo Methods\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapter 5 | Intermediate Level | 75 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Monte Carlo methods learn directly from episodes of experience without requiring a model of the environment's dynamics.\n",
        "        First introduced for RL by <a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_method\" style=\"color: #17a2b8;\">Stanislaw Ulam</a> \n",
        "        during the Manhattan Project, these methods are particularly effective for episodic tasks. This lab implements the\n",
        "        <strong>First-Visit Monte Carlo</strong> algorithm on the classic Blackjack problem from\n",
        "        <a href=\"http://incompleteideas.net/book/the-book-2nd.html\" style=\"color: #17a2b8;\">Sutton & Barto (2018)</a>, Example 5.1.\n",
        "        We use <a href=\"https://gym.openai.com/\" style=\"color: #17a2b8;\">OpenAI Gym</a> for the environment simulation.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Understand Monte Carlo prediction methods</li>\n",
        "        <li>Implement First-Visit MC algorithm</li>\n",
        "        <li>Learn from sampled episodes of experience</li>\n",
        "        <li>Estimate action-value functions Q(s,a)</li>\n",
        "        <li>Visualize value functions and policies</li>\n",
        "        <li>Work with OpenAI Gym environments</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Blackjack Rules</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Goal</code> → Get sum close to 21 without exceeding</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Actions</code> → Hit (draw card) or Stick (stop)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">States</code> → (player_sum, dealer_card, usable_ace)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Rewards</code> → +1 (win), 0 (draw), -1 (lose)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Ace</code> → Can be 1 or 11 (usable if 11)</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Environment Setup and Dependencies\n",
        "\n",
        "We begin by importing necessary libraries including OpenAI Gym for the Blackjack environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 1: Import Libraries and Load Utilities\n",
        "Purpose: Set up the computational environment with all necessary dependencies\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Fetch and execute the pretty print utility from GitHub\n",
        "try:\n",
        "    url = 'https://raw.githubusercontent.com/mdehghani86/RL_labs/master/utility/rl_utility.py'\n",
        "    response = requests.get(url)\n",
        "    exec(response.text)\n",
        "    pretty_print(\"Environment Ready\", \n",
        "                 \"Successfully loaded all dependencies<br>\" +\n",
        "                 \"Libraries: Gym, NumPy, Matplotlib<br>\" +\n",
        "                 \"Ready for Monte Carlo Blackjack implementation\", \n",
        "                 style='success')\n",
        "except Exception as e:\n",
        "    # Fallback definition if GitHub fetch fails\n",
        "    from IPython.display import display, HTML\n",
        "    def pretty_print(title, content, style='info'):\n",
        "        themes = {\n",
        "            'info': {'primary': '#17a2b8', 'secondary': '#0e5a63', 'background': '#f8f9fa'},\n",
        "            'success': {'primary': '#28a745', 'secondary': '#155724', 'background': '#f8fff9'},\n",
        "            'warning': {'primary': '#ffc107', 'secondary': '#e0a800', 'background': '#fffdf5'},\n",
        "            'result': {'primary': '#6f42c1', 'secondary': '#4e2c8e', 'background': '#faf5ff'},\n",
        "            'note': {'primary': '#20c997', 'secondary': '#0d7a5f', 'background': '#f0fdf9'}\n",
        "        }\n",
        "        theme = themes.get(style, themes['info'])\n",
        "        html = f'''\n",
        "        <div style=\"border-radius: 5px; margin: 10px 0; width: 20cm; max-width: 20cm; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
        "            <div style=\"background: linear-gradient(90deg, {theme['primary']} 0%, {theme['secondary']} 100%); padding: 10px 15px; border-radius: 5px 5px 0 0;\">\n",
        "                <strong style=\"color: white; font-size: 14px;\">{title}</strong>\n",
        "            </div>\n",
        "            <div style=\"background: {theme['background']}; padding: 10px 15px; border-radius: 0 0 5px 5px; border-left: 3px solid {theme['primary']};\">        \n",
        "                <div style=\"color: rgba(0,0,0,0.8); font-size: 12px; line-height: 1.5;\">{content}</div>\n",
        "            </div>\n",
        "        </div>\n",
        "        '''\n",
        "        display(HTML(html))\n",
        "    \n",
        "    pretty_print(\"Fallback Mode\", \n",
        "                 f\"Using local pretty_print definition<br>Error: {str(e)}\", \n",
        "                 style='warning')\n",
        "\n",
        "# Configure matplotlib for better visualizations\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Creating the Blackjack Environment\n",
        "\n",
        "### OpenAI Gym Environment\n",
        "\n",
        "OpenAI Gym provides a standardized interface for RL environments. The Blackjack environment simulates the card game with:\n",
        "- **State space**: (player_sum, dealer_showing, usable_ace)\n",
        "- **Action space**: {0: Stick, 1: Hit}\n",
        "- **Reward structure**: Terminal rewards only (+1, 0, -1)\n",
        "\n",
        "The environment follows the rules from Sutton & Barto Example 5.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 2: Initialize Blackjack Environment\n",
        "Purpose: Create and explore the OpenAI Gym Blackjack environment\n",
        "\"\"\"\n",
        "\n",
        "# Create the Blackjack environment using Gym\n",
        "env = gym.make('Blackjack-v0')\n",
        "\n",
        "# Explore environment properties\n",
        "pretty_print(\"Blackjack Environment Created\",\n",
        "             f\"Action space size: {env.action_space.n} actions<br>\" +\n",
        "             \"Actions: 0 = Stick (stop), 1 = Hit (draw card)<br>\" +\n",
        "             \"State: (player_sum, dealer_card, usable_ace)<br>\" +\n",
        "             \"Rewards: +1 (win), 0 (draw), -1 (lose)\",\n",
        "             style='info')\n",
        "\n",
        "# Demonstrate environment interface\n",
        "sample_state = env.reset()\n",
        "pretty_print(\"Sample Initial State\",\n",
        "             f\"State: {sample_state}<br>\" +\n",
        "             f\"Player sum: {sample_state[0]}<br>\" +\n",
        "             f\"Dealer showing: {sample_state[1]}<br>\" +\n",
        "             f\"Usable ace: {sample_state[2]}\",\n",
        "             style='note')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Stochastic Policy for Blackjack\n",
        "\n",
        "### Initial Policy Definition\n",
        "\n",
        "We define a simple stochastic policy for exploration:\n",
        "- If player's sum > 18: P(Stick) = 0.8, P(Hit) = 0.2\n",
        "- If player's sum ≤ 18: P(Stick) = 0.2, P(Hit) = 0.8\n",
        "\n",
        "This policy provides a balance between conservative play (sticking on high sums) and exploration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 3: Episode Generation with Stochastic Policy\n",
        "Purpose: Implement function to play complete episodes using our initial policy\n",
        "\"\"\"\n",
        "\n",
        "def play_episode(env):\n",
        "    \"\"\"\n",
        "    Play a complete episode of Blackjack using a stochastic policy\n",
        "    \n",
        "    The policy is threshold-based:\n",
        "    - Tends to stick (80% probability) when player sum > 18\n",
        "    - Tends to hit (80% probability) when player sum ≤ 18\n",
        "    \n",
        "    This provides exploration while following reasonable Blackjack strategy.\n",
        "    \n",
        "    Args:\n",
        "        env: OpenAI Gym Blackjack environment\n",
        "    \n",
        "    Returns:\n",
        "        episode: List of (state, action, reward) tuples for the complete episode\n",
        "    \"\"\"\n",
        "    episode = []\n",
        "    \n",
        "    # Initialize new game (Blackjack uses \"exploring starts\" - random initial states)\n",
        "    state = env.reset()\n",
        "    \n",
        "    while True:\n",
        "        # Define stochastic policy based on player's sum\n",
        "        # Higher sums (>18) → prefer to stick to avoid busting\n",
        "        # Lower sums (≤18) → prefer to hit to get closer to 21\n",
        "        if state[0] > 18:\n",
        "            # Conservative: mostly stick when close to 21\n",
        "            action_probs = [0.8, 0.2]  # [P(stick), P(hit)]\n",
        "        else:\n",
        "            # Aggressive: mostly hit when far from 21\n",
        "            action_probs = [0.2, 0.8]  # [P(stick), P(hit)]\n",
        "        \n",
        "        # Sample action from probability distribution\n",
        "        action = np.random.choice([0, 1], p=action_probs)\n",
        "        \n",
        "        # Execute action in environment\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        # Record state-action-reward tuple\n",
        "        episode.append((state, action, reward))\n",
        "        \n",
        "        # Update state for next iteration\n",
        "        state = next_state\n",
        "        \n",
        "        # Check if episode is complete (game over)\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    return episode\n",
        "\n",
        "# Test episode generation\n",
        "sample_episode = play_episode(env)\n",
        "pretty_print(\"Sample Episode Generated\",\n",
        "             f\"Episode length: {len(sample_episode)} steps<br>\" +\n",
        "             f\"Final reward: {sample_episode[-1][2]}<br>\" +\n",
        "             f\"Sample step: {sample_episode[0]}\",\n",
        "             style='success')\n",
        "\n",
        "# Display full episode for understanding\n",
        "print(\"\\nComplete episode trajectory:\")\n",
        "for i, (state, action, reward) in enumerate(sample_episode):\n",
        "    action_name = \"Stick\" if action == 0 else \"Hit\"\n",
        "    print(f\"Step {i+1}: State={state}, Action={action_name}, Reward={reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: First-Visit Monte Carlo Algorithm\n",
        "\n",
        "### Theoretical Foundation\n",
        "\n",
        "First-Visit Monte Carlo estimates value functions by averaging returns from the **first visit** to each state in an episode:\n",
        "\n",
        "$$Q(s,a) = \\frac{1}{N(s,a)} \\sum_{\\text{episodes}} G_t$$\n",
        "\n",
        "Where:\n",
        "- $G_t = \\sum_{k=0}^{T-t} \\gamma^k R_{t+k+1}$ is the return from time $t$\n",
        "- $N(s,a)$ is the number of first visits to $(s,a)$\n",
        "- $\\gamma$ is the discount factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 4: Q-Value Update Function\n",
        "Purpose: Implement First-Visit Monte Carlo update rule for action-value estimation\n",
        "\"\"\"\n",
        "\n",
        "def update_Q(episode, Q, returns_sum, N, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Update Q-values using First-Visit Monte Carlo method\n",
        "    \n",
        "    For each state-action pair in the episode:\n",
        "    1. Find the FIRST occurrence (first-visit)\n",
        "    2. Calculate discounted return from that point\n",
        "    3. Update running sum and count\n",
        "    4. Compute new average Q-value\n",
        "    \n",
        "    Args:\n",
        "        episode: List of (state, action, reward) tuples\n",
        "        Q: Action-value function estimates\n",
        "        returns_sum: Cumulative returns for each state-action pair\n",
        "        N: Visit counts for each state-action pair\n",
        "        gamma: Discount factor (default 1.0 for episodic task)\n",
        "    \"\"\"\n",
        "    # Process each unique state-action pair in the episode\n",
        "    visited = set()  # Track visited state-action pairs for first-visit\n",
        "    \n",
        "    for t, (state, action, reward) in enumerate(episode):\n",
        "        # Create hashable state-action pair\n",
        "        sa_pair = (state, action)\n",
        "        \n",
        "        # First-visit check: only update if this is first occurrence\n",
        "        if sa_pair not in visited:\n",
        "            visited.add(sa_pair)\n",
        "            \n",
        "            # Calculate return G from time t onwards\n",
        "            G = 0\n",
        "            for k, (_, _, r) in enumerate(episode[t:]):\n",
        "                # G = r_t + γ*r_{t+1} + γ²*r_{t+2} + ...\n",
        "                G += (gamma ** k) * r\n",
        "            \n",
        "            # Update cumulative sum of returns\n",
        "            returns_sum[state][action] += G\n",
        "            \n",
        "            # Increment visit count\n",
        "            N[state][action] += 1.0\n",
        "            \n",
        "            # Update Q-value as running average\n",
        "            # Q(s,a) = average of all returns from (s,a)\n",
        "            Q[state][action] = returns_sum[state][action] / N[state][action]\n",
        "\n",
        "pretty_print(\"Q-Value Update Function Ready\",\n",
        "             \"First-Visit Monte Carlo update implemented<br>\" +\n",
        "             \"Calculates returns from first occurrence only<br>\" +\n",
        "             \"Maintains running average of returns\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 5: Monte Carlo Prediction Main Loop\n",
        "Purpose: Implement the complete MC prediction algorithm for policy evaluation\n",
        "\"\"\"\n",
        "\n",
        "def mc_predict(env, num_episodes, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Monte Carlo prediction for estimating Q-values of the current policy\n",
        "    \n",
        "    Runs multiple episodes and updates Q-values using first-visit MC.\n",
        "    This implements the prediction (policy evaluation) problem:\n",
        "    Given a policy π, estimate Q^π(s,a) for all state-action pairs.\n",
        "    \n",
        "    Args:\n",
        "        env: OpenAI Gym Blackjack environment\n",
        "        num_episodes: Number of episodes to simulate\n",
        "        gamma: Discount factor (1.0 for undiscounted)\n",
        "    \n",
        "    Returns:\n",
        "        Q: Estimated action-value function\n",
        "    \"\"\"\n",
        "    # Initialize data structures\n",
        "    # defaultdict automatically initializes missing keys with zeros\n",
        "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    pretty_print(\"Starting Monte Carlo Prediction\",\n",
        "                 f\"Running {num_episodes:,} episodes<br>\" +\n",
        "                 f\"Discount factor γ = {gamma}<br>\" +\n",
        "                 \"This may take a few minutes...\",\n",
        "                 style='warning')\n",
        "    \n",
        "    # Run episodes and update Q-values\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Generate complete episode\n",
        "        episode = play_episode(env)\n",
        "        \n",
        "        # Update Q-values based on episode\n",
        "        update_Q(episode, Q, returns_sum, N, gamma)\n",
        "        \n",
        "        # Progress reporting\n",
        "        if i_episode % 10000 == 0:\n",
        "            avg_q = np.mean([q.mean() for q in Q.values()])\n",
        "            print(f\"\\rEpisode {i_episode}/{num_episodes} | Avg Q-value: {avg_q:.4f}\", end=\"\")\n",
        "            sys.stdout.flush()\n",
        "        elif i_episode % 1000 == 0:\n",
        "            print(f\"\\rEpisode {i_episode}/{num_episodes}\", end=\"\")\n",
        "            sys.stdout.flush()\n",
        "    \n",
        "    print()  # New line after progress\n",
        "    \n",
        "    pretty_print(\"Monte Carlo Prediction Complete\",\n",
        "                 f\"Processed {num_episodes:,} episodes<br>\" +\n",
        "                 f\"Estimated Q-values for {len(Q)} states<br>\" +\n",
        "                 f\"Average visits per state: {np.mean([n.sum() for n in N.values()]):.1f}\",\n",
        "                 style='success')\n",
        "    \n",
        "    return Q\n",
        "\n",
        "pretty_print(\"Monte Carlo Prediction Ready\",\n",
        "             \"Complete algorithm implemented<br>\" +\n",
        "             \"Will estimate Q-values through episode sampling\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Visualization Functions\n",
        "\n",
        "We create comprehensive visualizations to understand the learned value function and policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 6: Visualization Helper Functions\n",
        "Purpose: Create 3D plots for value functions and 2D heatmaps for policies\n",
        "\"\"\"\n",
        "\n",
        "def plot_blackjack_values(V):\n",
        "    \"\"\"\n",
        "    Create 3D surface plots of the state-value function\n",
        "    Separate plots for states with and without usable ace\n",
        "    \n",
        "    Args:\n",
        "        V: State-value function dictionary\n",
        "    \"\"\"\n",
        "    def get_Z(x, y, usable_ace):\n",
        "        \"\"\"Get value for a specific state, default to 0 if not visited\"\"\"\n",
        "        if (x, y, usable_ace) in V:\n",
        "            return V[x, y, usable_ace]\n",
        "        else:\n",
        "            return 0\n",
        "    \n",
        "    def get_figure(usable_ace, ax):\n",
        "        \"\"\"Create 3D surface plot for given usable_ace condition\"\"\"\n",
        "        # Define ranges for player sum and dealer card\n",
        "        x_range = np.arange(11, 22)  # Player sum from 11 to 21\n",
        "        y_range = np.arange(1, 11)   # Dealer card from 1 (Ace) to 10\n",
        "        X, Y = np.meshgrid(x_range, y_range)\n",
        "        \n",
        "        # Compute values for all state combinations\n",
        "        Z = np.array([get_Z(x, y, usable_ace) \n",
        "                     for x, y in zip(np.ravel(X), np.ravel(Y))]).reshape(X.shape)\n",
        "        \n",
        "        # Create surface plot\n",
        "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, \n",
        "                               cmap=plt.cm.coolwarm, vmin=-1.0, vmax=1.0,\n",
        "                               alpha=0.8, edgecolor='none')\n",
        "        ax.set_xlabel('Player\\'s Current Sum')\n",
        "        ax.set_ylabel('Dealer\\'s Showing Card')\n",
        "        ax.set_zlabel('State Value')\n",
        "        ax.view_init(elev=30, azim=-120)  # Set viewing angle\n",
        "        ax.set_zlim(-1, 1)\n",
        "    \n",
        "    # Create figure with two subplots\n",
        "    fig = plt.figure(figsize=(15, 12))\n",
        "    \n",
        "    # Subplot 1: States with usable ace\n",
        "    ax1 = fig.add_subplot(211, projection='3d')\n",
        "    ax1.set_title('State Values with Usable Ace', fontsize=14, fontweight='bold')\n",
        "    get_figure(True, ax1)\n",
        "    \n",
        "    # Subplot 2: States without usable ace\n",
        "    ax2 = fig.add_subplot(212, projection='3d')\n",
        "    ax2.set_title('State Values without Usable Ace', fontsize=14, fontweight='bold')\n",
        "    get_figure(False, ax2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_policy(policy):\n",
        "    \"\"\"\n",
        "    Create 2D heatmaps showing the optimal action for each state\n",
        "    \n",
        "    Args:\n",
        "        policy: Dictionary mapping states to actions\n",
        "    \"\"\"\n",
        "    def get_Z(x, y, usable_ace):\n",
        "        \"\"\"Get action for a specific state, default to 1 (hit) if not defined\"\"\"\n",
        "        if (x, y, usable_ace) in policy:\n",
        "            return policy[x, y, usable_ace]\n",
        "        else:\n",
        "            return 1  # Default to hit\n",
        "    \n",
        "    def get_figure(usable_ace, ax):\n",
        "        \"\"\"Create 2D heatmap for given usable_ace condition\"\"\"\n",
        "        x_range = np.arange(11, 22)  # Player sum\n",
        "        y_range = np.arange(10, 0, -1)  # Dealer card (reversed for display)\n",
        "        X, Y = np.meshgrid(x_range, y_range)\n",
        "        \n",
        "        # Compute policy for all states\n",
        "        Z = np.array([[get_Z(x, y, usable_ace) for x in x_range] \n",
        "                     for y in y_range])\n",
        "        \n",
        "        # Create heatmap\n",
        "        surf = ax.imshow(Z, cmap=plt.get_cmap('RdYlBu_r', 2), \n",
        "                        vmin=0, vmax=1, extent=[10.5, 21.5, 0.5, 10.5])\n",
        "        plt.xticks(x_range)\n",
        "        plt.yticks(y_range)\n",
        "        plt.gca().invert_yaxis()\n",
        "        ax.set_xlabel('Player\\'s Current Sum')\n",
        "        ax.set_ylabel('Dealer\\'s Showing Card')\n",
        "        ax.grid(color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "        \n",
        "        # Add colorbar\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "        cbar = plt.colorbar(surf, ticks=[0, 1], cax=cax)\n",
        "        cbar.ax.set_yticklabels(['STICK (0)', 'HIT (1)'])\n",
        "    \n",
        "    # Create figure with two subplots\n",
        "    fig = plt.figure(figsize=(14, 6))\n",
        "    \n",
        "    # Subplot 1: Policy with usable ace\n",
        "    ax1 = fig.add_subplot(121)\n",
        "    ax1.set_title('Policy with Usable Ace', fontsize=12, fontweight='bold')\n",
        "    get_figure(True, ax1)\n",
        "    \n",
        "    # Subplot 2: Policy without usable ace\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    ax2.set_title('Policy without Usable Ace', fontsize=12, fontweight='bold')\n",
        "    get_figure(False, ax2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "pretty_print(\"Visualization Functions Ready\",\n",
        "             \"3D surface plots for value functions<br>\" +\n",
        "             \"2D heatmaps for policy visualization<br>\" +\n",
        "             \"Separate views for usable/non-usable ace states\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Running Monte Carlo Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 7: Execute Monte Carlo Learning\n",
        "Purpose: Run MC prediction with large number of episodes and visualize results\n",
        "\"\"\"\n",
        "\n",
        "# Run Monte Carlo prediction with 500,000 episodes\n",
        "NUM_EPISODES = 500000\n",
        "\n",
        "pretty_print(\"Starting Large-Scale Experiment\",\n",
        "             f\"Episodes to run: {NUM_EPISODES:,}<br>\" +\n",
        "             \"Expected time: 1-2 minutes<br>\" +\n",
        "             \"Learning Q-values for stochastic policy\",\n",
        "             style='warning')\n",
        "\n",
        "# Run MC prediction\n",
        "Q = mc_predict(env, NUM_EPISODES)\n",
        "\n",
        "# Convert Q-values to state values using the stochastic policy\n",
        "# V(s) = Σ_a π(a|s) * Q(s,a)\n",
        "V_to_plot = {}\n",
        "for state, action_values in Q.items():\n",
        "    # Apply our stochastic policy to get expected value\n",
        "    if state[0] > 18:\n",
        "        # Policy: 80% stick, 20% hit\n",
        "        expected_value = 0.8 * action_values[0] + 0.2 * action_values[1]\n",
        "    else:\n",
        "        # Policy: 20% stick, 80% hit\n",
        "        expected_value = 0.2 * action_values[0] + 0.8 * action_values[1]\n",
        "    \n",
        "    V_to_plot[state] = expected_value\n",
        "\n",
        "# Extract greedy policy from Q-values\n",
        "greedy_policy = {}\n",
        "for state, action_values in Q.items():\n",
        "    # Select action with highest Q-value\n",
        "    greedy_policy[state] = np.argmax(action_values)\n",
        "\n",
        "# Analyze results\n",
        "states_visited = len(Q)\n",
        "avg_value = np.mean(list(V_to_plot.values()))\n",
        "stick_states = sum(1 for a in greedy_policy.values() if a == 0)\n",
        "hit_states = sum(1 for a in greedy_policy.values() if a == 1)\n",
        "\n",
        "analysis_text = f\"\"\"\n",
        "<strong>Learning Results:</strong><br><br>\n",
        "• States visited: {states_visited} unique states<br>\n",
        "• Average state value: {avg_value:.4f}<br>\n",
        "• Greedy policy statistics:<br>\n",
        "  - States where optimal is STICK: {stick_states} ({100*stick_states/states_visited:.1f}%)<br>\n",
        "  - States where optimal is HIT: {hit_states} ({100*hit_states/states_visited:.1f}%)<br><br>\n",
        "<strong>Key Insights:</strong><br>\n",
        "• Policy tends to stick with higher player sums<br>\n",
        "• Usable ace affects optimal strategy significantly<br>\n",
        "• Dealer's showing card influences decision boundary\n",
        "\"\"\"\n",
        "\n",
        "pretty_print(\"Analysis Complete\", analysis_text, style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 8: Visualize Learned Value Function\n",
        "Purpose: Create 3D visualizations of the state-value function\n",
        "\"\"\"\n",
        "\n",
        "pretty_print(\"Generating Value Function Plots\",\n",
        "             \"Creating 3D surface plots for state values<br>\" +\n",
        "             \"Separate visualizations for usable/non-usable ace\",\n",
        "             style='info')\n",
        "\n",
        "# Plot the state-value function\n",
        "plot_blackjack_values(V_to_plot)\n",
        "\n",
        "pretty_print(\"Value Function Visualization\",\n",
        "             \"<strong>Interpretation:</strong><br>\" +\n",
        "             \"• Higher values (red): Favorable states likely to win<br>\" +\n",
        "             \"• Lower values (blue): Unfavorable states likely to lose<br>\" +\n",
        "             \"• Peak values around sum 20-21: Best winning positions<br>\" +\n",
        "             \"• Valley for low sums: Poor positions requiring hits\",\n",
        "             style='note')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 9: Visualize Derived Greedy Policy\n",
        "Purpose: Show the optimal policy learned from Q-values\n",
        "\"\"\"\n",
        "\n",
        "pretty_print(\"Generating Policy Heatmaps\",\n",
        "             \"Creating 2D heatmaps showing optimal actions<br>\" +\n",
        "             \"Red = HIT, Blue = STICK\",\n",
        "             style='info')\n",
        "\n",
        "# Plot the greedy policy\n",
        "plot_policy(greedy_policy)\n",
        "\n",
        "pretty_print(\"Policy Visualization\",\n",
        "             \"<strong>Policy Patterns:</strong><br>\" +\n",
        "             \"• Clear threshold around sum 17-20<br>\" +\n",
        "             \"• More conservative with usable ace (can't bust)<br>\" +\n",
        "             \"• Adapts to dealer's showing card<br>\" +\n",
        "             \"• Matches intuitive Blackjack strategy\",\n",
        "             style='note')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Key Findings</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>1. Monte Carlo Convergence:</strong> With 500,000 episodes, the value function converges to stable estimates, demonstrating the law of large numbers in action.</p>\n",
        "        <p><strong>2. Policy Structure:</strong> The learned policy shows a clear decision boundary around player sum 17-20, which aligns with optimal Blackjack strategy.</p>\n",
        "        <p><strong>3. Usable Ace Impact:</strong> States with usable ace show different optimal strategies, as the flexibility of ace prevents busting.</p>\n",
        "        <p><strong>4. Dealer Card Influence:</strong> The optimal policy adapts based on dealer's showing card - more aggressive against weak dealer cards (4-6).</p>\n",
        "        <p><strong>5. First-Visit Efficiency:</strong> First-visit MC provides unbiased estimates while being computationally efficient.</p>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>How would the value function change if we used Every-Visit MC instead of First-Visit?</li>\n",
        "        <li>What happens to convergence speed with different initial policies?</li>\n",
        "        <li>How could we implement MC Control to find the optimal policy directly?</li>\n",
        "        <li>Why is Monte Carlo particularly suitable for Blackjack compared to Dynamic Programming?</li>\n",
        "        <li>How would adding card counting affect the state space and learning?</li>\n",
        "        <li>What are the advantages of MC methods when the model is unknown or complex?</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 5-1: Blackjack with Monte Carlo Methods</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Next: Lab 5-2 - Monte Carlo Control</p>\n",
        "</div>"
      ]
    }
  ]
}
