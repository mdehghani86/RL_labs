{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_05_1_Blackjack_Monte_Carlo_FIXED.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5-1: Blackjack with Monte Carlo Methods\n",
        "\n",
        "**IE 7295 Reinforcement Learning | Sutton & Barto Chapter 5**\n",
        "\n",
        "---\n",
        "\n",
        "## Background\n",
        "\n",
        "Monte Carlo methods learn directly from episodes of experience without requiring a model of the environment's dynamics. This lab implements the **First-Visit Monte Carlo** algorithm on the classic Blackjack problem from Sutton & Barto (2018), Example 5.1.\n",
        "\n",
        "### Learning Objectives\n",
        "- Understand Monte Carlo prediction methods\n",
        "- Implement First-Visit MC algorithm\n",
        "- Learn from sampled episodes of experience\n",
        "- Estimate action-value functions Q(s,a)\n",
        "- Visualize value functions and policies\n",
        "\n",
        "### Blackjack Rules\n",
        "- **Goal:** Get sum close to 21 without exceeding\n",
        "- **Actions:** Hit (draw card) or Stick (stop)\n",
        "- **States:** (player_sum, dealer_card, usable_ace)\n",
        "- **Rewards:** +1 (win), 0 (draw), -1 (lose)\n",
        "- **Ace:** Can be 1 or 11 (usable if 11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 1: Environment Setup and Dependencies\n",
        "\n",
        "Importing libraries and configuring the computational environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Import Libraries\n",
        "import sys\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(f\"Gymnasium version: {gym.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 2: Creating the Blackjack Environment\n",
        "\n",
        "Initializing the OpenAI Gymnasium Blackjack-v1 environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Initialize Blackjack Environment (v1)\n",
        "\n",
        "# FIXED: Using v1 instead of v0\n",
        "env = gym.make('Blackjack-v1')\n",
        "\n",
        "print(f\"Environment: Blackjack-v1\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Number of actions: {env.action_space.n}\")\n",
        "print(\"Actions: 0 = Stick (stop), 1 = Hit (draw card)\")\n",
        "\n",
        "# Demonstrate environment\n",
        "sample_state, _ = env.reset()\n",
        "print(f\"\\nSample initial state: {sample_state}\")\n",
        "print(f\"  Player sum: {sample_state[0]}\")\n",
        "print(f\"  Dealer showing: {sample_state[1]}\")\n",
        "print(f\"  Usable ace: {sample_state[2]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 3: Monte Carlo ES Algorithm Overview\n",
        "\n",
        "### Monte Carlo with Exploring Starts (MC ES)\n",
        "\n",
        "Algorithm for finding optimal policy π ≈ π*\n",
        "\n",
        "---\n",
        "\n",
        "### Algorithm Overview\n",
        "\n",
        "Monte Carlo ES uses **Exploring Starts** to ensure all state-action pairs are visited. Each episode begins with a **random** state-action pair, then follows the current policy. This guarantees exploration while still finding the optimal policy.\n",
        "\n",
        "### Pseudocode\n",
        "\n",
        "![Monte Carlo ES Pseudocode](https://github.com/mdehghani86/RL_labs/blob/master/Lab%2005/MCM_ES.jpg?raw=true)\n",
        "\n",
        "*Figure: Monte Carlo ES Algorithm from Sutton & Barto*\n",
        "\n",
        "---\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Exploring Start:** Random (S₀, A₀) pair\n",
        "2. **Generate Episode:** Follow current policy π\n",
        "3. **Calculate Returns:** G for each (s,a)\n",
        "4. **Update Q:** Average returns for each pair\n",
        "5. **Policy Improvement:** π(s) ← argmax Q(s,a)\n",
        "\n",
        "### Why Exploring Starts?\n",
        "\n",
        "Without exploring starts, a deterministic policy might never visit certain state-action pairs, preventing us from learning their true values.\n",
        "\n",
        "**Solution:** Start each episode with a random (s,a) pair to ensure comprehensive exploration of the state-action space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 4: Stochastic Policy for Exploration\n",
        "\n",
        "Defining an arbitrary exploration policy to generate learning episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Episode Generation with Arbitrary Stochastic Policy\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTANT CONCEPT - ARBITRARY vs OPTIMAL POLICY\n",
        "# ============================================================================\n",
        "#\n",
        "# 1. ARBITRARY POLICY (used during learning):\n",
        "#    - A simple, reasonable policy we start with\n",
        "#    - NOT optimal, but provides exploration\n",
        "#    - In this code: threshold-based stochastic policy\n",
        "#      * If player_sum > 18: 80% stick, 20% hit\n",
        "#      * If player_sum <= 18: 20% stick, 80% hit\n",
        "#    - Purpose: Generate episodes to learn Q-values\n",
        "#\n",
        "# 2. OPTIMAL POLICY (learned from Q-values):\n",
        "#    - The BEST policy derived after learning\n",
        "#    - Greedy with respect to learned Q-values\n",
        "#    - π*(s) = argmax_a Q(s,a)\n",
        "#    - This is what we are trying to find!\n",
        "#\n",
        "# LEARNING PROCESS:\n",
        "#   Arbitrary Policy → Generate Episodes → Learn Q-values → Extract Optimal Policy\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "def play_episode_arbitrary_policy(env):\n",
        "    \"\"\"\n",
        "    Play complete episode using ARBITRARY stochastic policy.\n",
        "    This is NOT the optimal policy - it is our exploration policy.\n",
        "    \n",
        "    Returns:\n",
        "        episode: List of (state, action, reward) tuples\n",
        "    \"\"\"\n",
        "    episode = []\n",
        "    state, _ = env.reset()\n",
        "    \n",
        "    while True:\n",
        "        # ARBITRARY POLICY DEFINITION:\n",
        "        # Simple threshold-based probabilities for exploration\n",
        "        if state[0] > 18:\n",
        "            # High sum: prefer to stick (conservative)\n",
        "            action_probs = [0.8, 0.2]  # [P(stick), P(hit)]\n",
        "        else:\n",
        "            # Low sum: prefer to hit (aggressive)\n",
        "            action_probs = [0.2, 0.8]\n",
        "        \n",
        "        # Sample action from arbitrary policy\n",
        "        action = np.random.choice([0, 1], p=action_probs)\n",
        "        \n",
        "        # Execute action\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    return episode\n",
        "\n",
        "# Test episode generation\n",
        "print(\"Testing arbitrary policy episode generation...\\n\")\n",
        "sample_episode = play_episode_arbitrary_policy(env)\n",
        "print(f\"Episode length: {len(sample_episode)} steps\")\n",
        "print(f\"Final reward: {sample_episode[-1][2]}\")\n",
        "print(f\"\\nFirst 3 steps:\")\n",
        "for i, (state, action, reward) in enumerate(sample_episode[:3]):\n",
        "    action_name = \"Stick\" if action == 0 else \"Hit\"\n",
        "    print(f\"  Step {i+1}: State={state}, Action={action_name}, Reward={reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 5: First-Visit Monte Carlo Q-Value Updates\n",
        "\n",
        "Implementing the core learning algorithm for action-value estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Q-Value Update Function\n",
        "\n",
        "def update_Q(episode, Q, returns_sum, N, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Update Q-values using First-Visit Monte Carlo.\n",
        "    \n",
        "    For each (state, action) pair in episode:\n",
        "      1. Check if this is the FIRST visit to this pair\n",
        "      2. Calculate return G from this point forward\n",
        "      3. Update running average: Q(s,a) = mean(all returns)\n",
        "    \"\"\"\n",
        "    visited = set()\n",
        "    \n",
        "    for t, (state, action, reward) in enumerate(episode):\n",
        "        sa_pair = (state, action)\n",
        "        \n",
        "        # First-visit check\n",
        "        if sa_pair not in visited:\n",
        "            visited.add(sa_pair)\n",
        "            \n",
        "            # Calculate return G from time t\n",
        "            G = sum((gamma ** k) * r for k, (_, _, r) in enumerate(episode[t:]))\n",
        "            \n",
        "            # Update statistics\n",
        "            returns_sum[state][action] += G\n",
        "            N[state][action] += 1.0\n",
        "            \n",
        "            # Update Q as running average\n",
        "            Q[state][action] = returns_sum[state][action] / N[state][action]\n",
        "\n",
        "print(\"Q-value update function ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Monte Carlo Prediction Loop\n",
        "\n",
        "def mc_predict(env, num_episodes, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Monte Carlo prediction for Q-value estimation.\n",
        "    Uses arbitrary policy for exploration.\n",
        "    \"\"\"\n",
        "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    print(f\"Starting Monte Carlo prediction with {num_episodes:,} episodes...\\n\")\n",
        "    \n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        episode = play_episode_arbitrary_policy(env)\n",
        "        update_Q(episode, Q, returns_sum, N, gamma)\n",
        "        \n",
        "        if i_episode % 50000 == 0:\n",
        "            print(f\"Episode {i_episode:,}/{num_episodes:,}\")\n",
        "    \n",
        "    print(\"\\nMonte Carlo prediction complete\")\n",
        "    return Q\n",
        "\n",
        "print(\"MC prediction function ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 6: Visualization Functions\n",
        "\n",
        "Creating 3D value function plots and 2D policy heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 6: FIXED Visualization Functions\n",
        "#\n",
        "# FIXES:\n",
        "# 1. Proper 3D surface plot configuration\n",
        "# 2. Corrected 2D policy heatmap with proper axis alignment\n",
        "# 3. Better colormap handling\n",
        "# 4. Fixed coordinate systems\n",
        "\n",
        "def plot_blackjack_values(V):\n",
        "    \"\"\"\n",
        "    Create properly formatted 3D value function plots.\n",
        "    \n",
        "    This function creates two 3D surface plots showing state values:\n",
        "    - One for states with usable ace\n",
        "    - One for states without usable ace\n",
        "    \"\"\"\n",
        "    def get_Z(player_sum, dealer_card, usable_ace):\n",
        "        \"\"\"\n",
        "        Helper function to get value for a specific state.\n",
        "        \n",
        "        Parameters:\n",
        "        - player_sum: Player's current card sum (12-21)\n",
        "        - dealer_card: Dealer's visible card (1-10, where 1=Ace)\n",
        "        - usable_ace: Boolean for whether player has usable ace\n",
        "        \n",
        "        Returns: State value V(s) if exists, else 0\n",
        "        \"\"\"\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        return V.get(state, 0)\n",
        "    \n",
        "    def create_surface(usable_ace, ax):\n",
        "        \"\"\"\n",
        "        Create a 3D surface plot for a given usable_ace condition.\n",
        "        \n",
        "        KEY COMPONENTS:\n",
        "        ---------------\n",
        "        1. Meshgrid (X, Y): Creates a grid of all (player_sum, dealer_card) combinations\n",
        "        2. Z array: State values at each grid point\n",
        "        3. Surface plot: 3D visualization where height = value\n",
        "        \"\"\"\n",
        "        # STEP 1: Define the state space dimensions\n",
        "        # Player sum ranges from 12 to 21 (cannot play with sum < 12)\n",
        "        player_range = np.arange(12, 22)  # [12, 13, 14, ..., 21]\n",
        "        \n",
        "        # Dealer showing ranges from Ace(1) to 10\n",
        "        dealer_range = np.arange(1, 11)   # [1, 2, 3, ..., 10]\n",
        "        \n",
        "        # STEP 2: Create coordinate grid\n",
        "        # X, Y are 2D arrays where each point represents a state coordinate\n",
        "        # X[i,j] = player_sum, Y[i,j] = dealer_card for all (i,j) combinations\n",
        "        X, Y = np.meshgrid(player_range, dealer_range)\n",
        "        \n",
        "        # STEP 3: Compute Z values (state values) for each coordinate\n",
        "        # Z is a 2D array: Z[i,j] = V(player_range[j], dealer_range[i], usable_ace)\n",
        "        # This creates the height of the surface at each point\n",
        "        Z = np.array([[get_Z(x, y, usable_ace) \n",
        "                      for x in player_range]   # Iterate over player sums (columns)\n",
        "                     for y in dealer_range])   # Iterate over dealer cards (rows)\n",
        "        \n",
        "        # IMPORTANT: Z shape is (len(dealer_range), len(player_range))\n",
        "        # Each row corresponds to a dealer card, each column to a player sum\n",
        "        \n",
        "        # STEP 4: Create the 3D surface plot\n",
        "        surf = ax.plot_surface(\n",
        "            X, Y, Z,                    # Coordinates and heights\n",
        "            cmap=cm.coolwarm,           # Color: blue (low) to red (high)\n",
        "            linewidth=0,                # No grid lines on surface\n",
        "            antialiased=True,           # Smooth rendering\n",
        "            vmin=-1, vmax=1,            # Value range for color mapping\n",
        "            alpha=0.8                   # Slight transparency\n",
        "        )\n",
        "        \n",
        "        # STEP 5: Label axes and set view angle\n",
        "        ax.set_xlabel('Player Sum', fontsize=11)\n",
        "        ax.set_ylabel('Dealer Showing', fontsize=11)\n",
        "        ax.set_zlabel('Value', fontsize=11)\n",
        "        ax.set_zlim(-1, 1)              # Z-axis limits\n",
        "        ax.view_init(elev=25, azim=-130) # Viewing angle (elevation, azimuth)\n",
        "        \n",
        "        return surf\n",
        "    \n",
        "    # Create figure\n",
        "    fig = plt.figure(figsize=(14, 11))\n",
        "    \n",
        "    # With usable ace\n",
        "    ax1 = fig.add_subplot(211, projection='3d')\n",
        "    ax1.set_title('State Values WITH Usable Ace', fontsize=13, fontweight='bold', pad=15)\n",
        "    surf1 = create_surface(True, ax1)\n",
        "    fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=10)\n",
        "    \n",
        "    # Without usable ace\n",
        "    ax2 = fig.add_subplot(212, projection='3d')\n",
        "    ax2.set_title('State Values WITHOUT Usable Ace', fontsize=13, fontweight='bold', pad=15)\n",
        "    surf2 = create_surface(False, ax2)\n",
        "    fig.colorbar(surf2, ax=ax2, shrink=0.5, aspect=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_policy(policy):\n",
        "    \"\"\"\n",
        "    Create properly formatted 2D policy heatmaps.\n",
        "    \n",
        "    Creates two heatmaps showing the optimal action for each state:\n",
        "    - One for states with usable ace\n",
        "    - One for states without usable ace\n",
        "    \n",
        "    Colors: Green = STICK (0), Red = HIT (1)\n",
        "    \"\"\"\n",
        "    def get_action(player_sum, dealer_card, usable_ace):\n",
        "        \"\"\"\n",
        "        Get the policy action for a specific state.\n",
        "        \n",
        "        Returns: 0 (STICK) or 1 (HIT), defaults to 1 if state not in policy\n",
        "        \"\"\"\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        return policy.get(state, 1)  # Default: hit if state unseen\n",
        "    \n",
        "    def create_heatmap(usable_ace, ax):\n",
        "        \"\"\"\n",
        "        Create a 2D heatmap showing policy actions.\n",
        "        \n",
        "        KEY COMPONENTS:\n",
        "        ---------------\n",
        "        1. Policy grid (Z): 2D array where Z[i,j] = action for that state\n",
        "        2. Heatmap visualization: Color-coded policy decisions\n",
        "        3. Axes: Player sum (x-axis), Dealer card (y-axis)\n",
        "        \"\"\"\n",
        "        # STEP 1: Define state space ranges\n",
        "        player_range = range(12, 22)  # Player sum: 12-21\n",
        "        dealer_range = range(1, 11)   # Dealer card: 1-10 (Ace to 10)\n",
        "        \n",
        "        # STEP 2: Build the policy grid\n",
        "        # Z is a 2D array where each element is the action (0=STICK, 1=HIT)\n",
        "        # Rows represent dealer cards, columns represent player sums\n",
        "        Z = np.array([[get_action(player, dealer, usable_ace)\n",
        "                      for player in player_range]    # Columns: player sums\n",
        "                     for dealer in dealer_range])    # Rows: dealer cards\n",
        "        \n",
        "        # IMPORTANT: Z[i,j] = policy action when:\n",
        "        #   - Dealer shows dealer_range[i]\n",
        "        #   - Player has sum player_range[j]\n",
        "        # Z shape: (10 dealer cards, 10 player sums)\n",
        "        \n",
        "        # STEP 3: Create heatmap visualization\n",
        "        im = ax.imshow(\n",
        "            Z,\n",
        "            cmap='RdYlGn_r',        # Red=HIT(1), Green=STICK(0)\n",
        "            aspect='auto',          # Automatic aspect ratio\n",
        "            vmin=0, vmax=1,         # Action values: 0 or 1\n",
        "            extent=[11.5, 21.5, 0.5, 10.5],  # [x_min, x_max, y_min, y_max]\n",
        "            origin='lower',         # Y-axis starts from bottom\n",
        "            interpolation='nearest' # Sharp boundaries between actions\n",
        "        )\n",
        "        \n",
        "        # EXPLANATION OF extent parameter:\n",
        "        # extent=[11.5, 21.5, 0.5, 10.5] maps the array indices to coordinate values\n",
        "        # - X (player sum): from 11.5 to 21.5 (centers values 12-21)\n",
        "        # - Y (dealer card): from 0.5 to 10.5 (centers values 1-10)\n",
        "        \n",
        "        # STEP 4: Configure axis ticks and labels\n",
        "        ax.set_xticks(range(12, 22))           # X-axis: 12, 13, ..., 21\n",
        "        ax.set_yticks(range(1, 11))            # Y-axis: 1, 2, ..., 10\n",
        "        ax.set_yticklabels(['A'] + list(range(2, 11)))  # Display A for Ace (value 1)\n",
        "        \n",
        "        ax.set_xlabel('Player Sum', fontsize=11)\n",
        "        ax.set_ylabel('Dealer Showing', fontsize=11)\n",
        "        \n",
        "        # STEP 5: Add grid for readability\n",
        "        ax.grid(True, color='black', linewidth=0.5, alpha=0.3)\n",
        "        ax.set_axisbelow(False)  # Grid appears on top of heatmap\n",
        "        \n",
        "        # STEP 6: Add colorbar legend\n",
        "        cbar = plt.colorbar(im, ax=ax, ticks=[0, 1], fraction=0.046, pad=0.04)\n",
        "        cbar.ax.set_yticklabels(['STICK', 'HIT'])  # Label the actions\n",
        "        \n",
        "        return im\n",
        "    \n",
        "    # Create figure\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # With usable ace\n",
        "    ax1.set_title('Policy WITH Usable Ace', fontsize=12, fontweight='bold')\n",
        "    create_heatmap(True, ax1)\n",
        "    \n",
        "    # Without usable ace\n",
        "    ax2.set_title('Policy WITHOUT Usable Ace', fontsize=12, fontweight='bold')\n",
        "    create_heatmap(False, ax2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Visualization functions ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Section 7: Running Monte Carlo Experiments\n",
        "\n",
        "Executing large-scale learning and extracting the optimal policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 7: Execute Monte Carlo Learning\n",
        "#\n",
        "# ============================================================================\n",
        "# CRITICAL EXPLANATION - TWO POLICIES:\n",
        "# ============================================================================\n",
        "#\n",
        "# In this cell, we work with TWO different policies:\n",
        "#\n",
        "# 1. ARBITRARY POLICY (for learning):\n",
        "#    --------------------------------\n",
        "#    - The stochastic policy we use to GENERATE episodes\n",
        "#    - Defined in play_episode_arbitrary_policy()\n",
        "#    - Threshold-based: prefer stick if sum>18, hit if sum<=18\n",
        "#    - Purpose: Explore the environment to learn Q-values\n",
        "#    - This is NOT what we are trying to find!\n",
        "#\n",
        "# 2. OPTIMAL POLICY (what we are finding):\n",
        "#    ------------------------------------\n",
        "#    - The GREEDY policy extracted from learned Q-values\n",
        "#    - Defined as: π*(s) = argmax_a Q(s,a)\n",
        "#    - Deterministic: always picks best action\n",
        "#    - This is the GOAL of our learning!\n",
        "#\n",
        "# PROCESS FLOW:\n",
        "#   Arbitrary Policy → Episodes → Q-values → Optimal Policy\n",
        "#   (exploration)     (data)    (learning)  (solution)\n",
        "#\n",
        "# ANALOGY:\n",
        "#   - Arbitrary policy = practice games with exploration\n",
        "#   - Q-values = knowledge learned from practice\n",
        "#   - Optimal policy = tournament strategy using learned knowledge\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "# Run Monte Carlo prediction\n",
        "NUM_EPISODES = 500000\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"LEARNING PHASE: Using ARBITRARY policy for exploration\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Episodes: {NUM_EPISODES:,}\")\n",
        "print(\"Arbitrary policy: Threshold-based stochastic\")\n",
        "print(\"Goal: Learn Q(s,a) values\\n\")\n",
        "\n",
        "# Learn Q-values using arbitrary policy\n",
        "Q = mc_predict(env, NUM_EPISODES)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXTRACTION PHASE: Deriving OPTIMAL policy from Q-values\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Convert Q-values to state values under arbitrary policy\n",
        "# V(s) = Σ π(a|s) * Q(s,a) for arbitrary policy\n",
        "V_arbitrary = {}\n",
        "for state, action_values in Q.items():\n",
        "    if state[0] > 18:\n",
        "        # Arbitrary policy: 80% stick, 20% hit\n",
        "        V_arbitrary[state] = 0.8 * action_values[0] + 0.2 * action_values[1]\n",
        "    else:\n",
        "        # Arbitrary policy: 20% stick, 80% hit\n",
        "        V_arbitrary[state] = 0.2 * action_values[0] + 0.8 * action_values[1]\n",
        "\n",
        "# Extract OPTIMAL policy (greedy w.r.t. Q)\n",
        "# This is the policy we are trying to find!\n",
        "optimal_policy = {}\n",
        "for state, action_values in Q.items():\n",
        "    # Select action with highest Q-value (greedy)\n",
        "    optimal_policy[state] = np.argmax(action_values)\n",
        "    \n",
        "print(\"\\nOptimal policy extracted via greedy selection\")\n",
        "print(\"  π*(s) = argmax_a Q(s,a) for each state\\n\")\n",
        "\n",
        "# Analysis\n",
        "states_count = len(Q)\n",
        "stick_count = sum(1 for a in optimal_policy.values() if a == 0)\n",
        "hit_count = sum(1 for a in optimal_policy.values() if a == 1)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"States explored: {states_count}\")\n",
        "print(f\"\\nOptimal Policy Composition:\")\n",
        "print(f\"  STICK states: {stick_count} ({100*stick_count/states_count:.1f}%)\")\n",
        "print(f\"  HIT states:   {hit_count} ({100*hit_count/states_count:.1f}%)\")\n",
        "print(f\"\\nAverage state value: {np.mean(list(V_arbitrary.values())):.4f}\")\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Learned Q-values from arbitrary policy episodes\")\n",
        "print(\"  - Extracted optimal policy by always choosing best action\")\n",
        "print(\"  - Optimal policy is deterministic and greedy\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 8: Visualize Value Function\n",
        "\n",
        "print(\"Generating 3D value function plots...\\n\")\n",
        "plot_blackjack_values(V_arbitrary)\n",
        "\n",
        "print(\"\\nValue Function Interpretation:\")\n",
        "print(\"  • Red (high): Favorable states likely to win\")\n",
        "print(\"  • Blue (low): Unfavorable states likely to lose\")\n",
        "print(\"  • Peak near sum 20-21: Best positions\")\n",
        "print(\"  • Usable ace provides more flexibility\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 9: Visualize OPTIMAL Policy\n",
        "\n",
        "print(\"Generating optimal policy heatmaps...\\n\")\n",
        "plot_policy(optimal_policy)\n",
        "\n",
        "print(\"\\nOptimal Policy Interpretation:\")\n",
        "print(\"  • Green = STICK (action 0)\")\n",
        "print(\"  • Red = HIT (action 1)\")\n",
        "print(\"  • Clear threshold around sum 17-20\")\n",
        "print(\"  • More aggressive hitting with usable ace\")\n",
        "print(\"  • Adapts to dealer's showing card\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "1. **Policy Learning:** We used an arbitrary exploration policy to generate episodes, then extracted the optimal policy from learned Q-values.\n",
        "\n",
        "2. **Exploration vs Exploitation:** Arbitrary policy provides exploration, optimal policy is purely exploitative (greedy).\n",
        "\n",
        "3. **Usable Ace Impact:** Optimal strategy differs significantly with usable ace - more aggressive hitting since cannot bust.\n",
        "\n",
        "4. **Decision Boundaries:** Clear threshold emerges around sum 17-20 for stick/hit decision.\n",
        "\n",
        "5. **Monte Carlo Strength:** Model-free learning directly from experience converges to optimal behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## Questions for Reflection\n",
        "\n",
        "1. Why do we need an exploration policy if we are trying to find the optimal policy?\n",
        "\n",
        "2. What would happen if we used a purely greedy policy from the start?\n",
        "\n",
        "3. How does First-Visit MC differ from Every-Visit MC?\n",
        "\n",
        "4. Why is MC particularly suitable for Blackjack compared to Dynamic Programming?\n",
        "\n",
        "5. How could we implement ε-greedy exploration instead of arbitrary policy?\n",
        "\n",
        "---\n",
        "\n",
        "**End of Lab 5-1: Blackjack with Monte Carlo Methods**"
      ]
    }
  ]
}
