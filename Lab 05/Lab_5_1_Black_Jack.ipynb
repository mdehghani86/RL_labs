{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_05_1_Blackjack_Monte_Carlo_FIXED.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 5-1: Blackjack with Monte Carlo Methods\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapter 5 | Intermediate Level | 75 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Monte Carlo methods learn directly from episodes of experience without requiring a model of the environment's dynamics.\n",
        "        First introduced for RL by <a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_method\" style=\"color: #17a2b8;\">Stanislaw Ulam</a> \n",
        "        during the Manhattan Project, these methods are particularly effective for episodic tasks. This lab implements the\n",
        "        <strong>First-Visit Monte Carlo</strong> algorithm on the classic Blackjack problem from\n",
        "        <a href=\"http://incompleteideas.net/book/the-book-2nd.html\" style=\"color: #17a2b8;\">Sutton & Barto (2018)</a>, Example 5.1.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Understand Monte Carlo prediction methods</li>\n",
        "        <li>Implement First-Visit MC algorithm</li>\n",
        "        <li>Learn from sampled episodes of experience</li>\n",
        "        <li>Estimate action-value functions Q(s,a)</li>\n",
        "        <li>Visualize value functions and policies</li>\n",
        "        <li>Work with OpenAI Gym environments</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Blackjack Rules</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Goal</code> → Get sum close to 21 without exceeding</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Actions</code> → Hit (draw card) or Stick (stop)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">States</code> → (player_sum, dealer_card, usable_ace)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Rewards</code> → +1 (win), 0 (draw), -1 (lose)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Ace</code> → Can be 1 or 11 (usable if 11)</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 1: Import Libraries\n",
        "\"\"\"\n",
        "import sys\n",
        "import gymnasium as gym  # Updated to gymnasium (replaces gym)\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")\n",
        "print(f\"✓ Gymnasium version: {gym.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Creating the Blackjack Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 2: Initialize Blackjack Environment (v1)\n",
        "\"\"\"\n",
        "# FIXED: Using v1 instead of v0\n",
        "env = gym.make('Blackjack-v1')\n",
        "\n",
        "print(f\"Environment: Blackjack-v1\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Number of actions: {env.action_space.n}\")\n",
        "print(\"\\nActions: 0 = Stick (stop), 1 = Hit (draw card)\")\n",
        "\n",
        "# Demonstrate environment\n",
        "sample_state, _ = env.reset()\n",
        "print(f\"\\nSample initial state: {sample_state}\")\n",
        "print(f\"  Player sum: {sample_state[0]}\")\n",
        "print(f\"  Dealer showing: {sample_state[1]}\")\n",
        "print(f\"  Usable ace: {sample_state[2]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Monte Carlo ES Algorithm Overview\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px; margin-top: 30px;\">\n",
        "    <h2 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 20px; margin: 0; font-weight: 300;\">\n",
        "        Monte Carlo with Exploring Starts (MC ES)\n",
        "    </h2>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        Algorithm for Finding Optimal Policy π ≈ π*\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0;\">Algorithm Overview</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Monte Carlo ES uses <strong>Exploring Starts</strong> to ensure all state-action pairs are visited. \n",
        "        Each episode begins with a <strong>random</strong> state-action pair, then follows the current policy.\n",
        "        This guarantees exploration while still finding the optimal policy.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <img src=\"https://github.com/mdehghani86/RL_labs/blob/master/Lab%2005/MCM_ES.jpg?raw=true\" \n",
        "         alt=\"Monte Carlo ES Pseudocode\" \n",
        "         style=\"width: 70%; max-width: 800px; border: 2px solid #17a2b8; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
        "    <p style=\"color: #666; font-size: 12px; margin-top: 10px; font-style: italic;\">Figure: Monte Carlo ES Algorithm from Sutton & Barto</p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px; margin-top: 20px;\">\n",
        "<tr>\n",
        "<td style=\"background: #e8f5e9; padding: 12px 15px; border-left: 3px solid #4caf50; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #2e7d32; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Key Steps</h4>\n",
        "    <ol style=\"color: #555; line-height: 1.6; margin: 0; padding-left: 20px; font-size: 12px;\">\n",
        "        <li><strong>Exploring Start:</strong> Random (S₀, A₀) pair</li>\n",
        "        <li><strong>Generate Episode:</strong> Follow current policy π</li>\n",
        "        <li><strong>Calculate Returns:</strong> G for each (s,a)</li>\n",
        "        <li><strong>Update Q:</strong> Average returns for each pair</li>\n",
        "        <li><strong>Policy Improvement:</strong> π(s) ← argmax Q(s,a)</li>\n",
        "    </ol>\n",
        "</td>\n",
        "<td style=\"background: #fff3e0; padding: 12px 15px; border-left: 3px solid #ff9800; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #e65100; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Why Exploring Starts?</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <p style=\"margin: 0 0 8px 0;\">Without exploring starts, a deterministic policy might never visit certain state-action pairs, preventing us from learning their true values.</p>\n",
        "        <p style=\"margin: 0;\"><strong>Solution:</strong> Start each episode with a random (s,a) pair to ensure comprehensive exploration of the state-action space.</p>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Stochastic Policy for Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 3: Episode Generation with Arbitrary Stochastic Policy\n",
        "\n",
        "IMPORTANT CONCEPT - ARBITRARY vs OPTIMAL POLICY:\n",
        "==================================================\n",
        "\n",
        "1. ARBITRARY POLICY (used during learning):\n",
        "   - A simple, reasonable policy we start with\n",
        "   - NOT optimal, but provides exploration\n",
        "   - In this code: threshold-based stochastic policy\n",
        "     * If player_sum > 18: 80% stick, 20% hit\n",
        "     * If player_sum ≤ 18: 20% stick, 80% hit\n",
        "   - Purpose: Generate episodes to learn Q-values\n",
        "\n",
        "2. OPTIMAL POLICY (learned from Q-values):\n",
        "   - The BEST policy derived after learning\n",
        "   - Greedy with respect to learned Q-values\n",
        "   - π*(s) = argmax_a Q(s,a)\n",
        "   - This is what we're trying to find!\n",
        "\n",
        "LEARNING PROCESS:\n",
        "   Arbitrary Policy → Generate Episodes → Learn Q-values → Extract Optimal Policy\n",
        "\"\"\"\n",
        "\n",
        "def play_episode_arbitrary_policy(env):\n",
        "    \"\"\"\n",
        "    Play complete episode using ARBITRARY stochastic policy.\n",
        "    This is NOT the optimal policy - it's our exploration policy.\n",
        "    \n",
        "    Returns:\n",
        "        episode: List of (state, action, reward) tuples\n",
        "    \"\"\"\n",
        "    episode = []\n",
        "    state, _ = env.reset()  # Fixed for v1\n",
        "    \n",
        "    while True:\n",
        "        # ARBITRARY POLICY DEFINITION:\n",
        "        # Simple threshold-based probabilities for exploration\n",
        "        if state[0] > 18:\n",
        "            # High sum: prefer to stick (conservative)\n",
        "            action_probs = [0.8, 0.2]  # [P(stick), P(hit)]\n",
        "        else:\n",
        "            # Low sum: prefer to hit (aggressive)\n",
        "            action_probs = [0.2, 0.8]\n",
        "        \n",
        "        # Sample action from arbitrary policy\n",
        "        action = np.random.choice([0, 1], p=action_probs)\n",
        "        \n",
        "        # Execute action\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    return episode\n",
        "\n",
        "# Test episode generation\n",
        "print(\"Testing arbitrary policy episode generation...\\n\")\n",
        "sample_episode = play_episode_arbitrary_policy(env)\n",
        "print(f\"Episode length: {len(sample_episode)} steps\")\n",
        "print(f\"Final reward: {sample_episode[-1][2]}\")\n",
        "print(f\"\\nFirst 3 steps:\")\n",
        "for i, (state, action, reward) in enumerate(sample_episode[:3]):\n",
        "    action_name = \"Stick\" if action == 0 else \"Hit\"\n",
        "    print(f\"  Step {i+1}: State={state}, Action={action_name}, Reward={reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: First-Visit Monte Carlo Q-Value Updates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 4: Q-Value Update Function\n",
        "\"\"\"\n",
        "def update_Q(episode, Q, returns_sum, N, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Update Q-values using First-Visit Monte Carlo.\n",
        "    \n",
        "    For each (state, action) pair in episode:\n",
        "      1. Check if this is the FIRST visit to this pair\n",
        "      2. Calculate return G from this point forward\n",
        "      3. Update running average: Q(s,a) = mean(all returns)\n",
        "    \"\"\"\n",
        "    visited = set()\n",
        "    \n",
        "    for t, (state, action, reward) in enumerate(episode):\n",
        "        sa_pair = (state, action)\n",
        "        \n",
        "        # First-visit check\n",
        "        if sa_pair not in visited:\n",
        "            visited.add(sa_pair)\n",
        "            \n",
        "            # Calculate return G from time t\n",
        "            G = sum((gamma ** k) * r for k, (_, _, r) in enumerate(episode[t:]))\n",
        "            \n",
        "            # Update statistics\n",
        "            returns_sum[state][action] += G\n",
        "            N[state][action] += 1.0\n",
        "            \n",
        "            # Update Q as running average\n",
        "            Q[state][action] = returns_sum[state][action] / N[state][action]\n",
        "\n",
        "print(\"✓ Q-value update function ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 5: Monte Carlo Prediction Loop\n",
        "\"\"\"\n",
        "def mc_predict(env, num_episodes, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Monte Carlo prediction for Q-value estimation.\n",
        "    Uses arbitrary policy for exploration.\n",
        "    \"\"\"\n",
        "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    print(f\"Starting Monte Carlo prediction with {num_episodes:,} episodes...\\n\")\n",
        "    \n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        episode = play_episode_arbitrary_policy(env)\n",
        "        update_Q(episode, Q, returns_sum, N, gamma)\n",
        "        \n",
        "        if i_episode % 50000 == 0:\n",
        "            print(f\"Episode {i_episode:,}/{num_episodes:,}\")\n",
        "    \n",
        "    print(\"\\n✓ Monte Carlo prediction complete\")\n",
        "    return Q\n",
        "\n",
        "print(\"✓ MC prediction function ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: FIXED Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 6: FIXED Visualization Functions\n",
        "\n",
        "FIXES:\n",
        "1. Proper 3D surface plot configuration\n",
        "2. Corrected 2D policy heatmap with proper axis alignment\n",
        "3. Better colormap handling\n",
        "4. Fixed coordinate systems\n",
        "\"\"\"\n",
        "\n",
        "def plot_blackjack_values(V):\n",
        "    \"\"\"\n",
        "    FIXED: Create properly formatted 3D value function plots\n",
        "    \"\"\"\n",
        "    def get_Z(player_sum, dealer_card, usable_ace):\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        return V.get(state, 0)\n",
        "    \n",
        "    def create_surface(usable_ace, ax):\n",
        "        # Create meshgrid\n",
        "        player_range = np.arange(12, 22)  # 12 to 21\n",
        "        dealer_range = np.arange(1, 11)   # Ace(1) to 10\n",
        "        X, Y = np.meshgrid(player_range, dealer_range)\n",
        "        \n",
        "        # Compute Z values\n",
        "        Z = np.array([[get_Z(x, y, usable_ace) \n",
        "                      for x in player_range] \n",
        "                     for y in dealer_range])\n",
        "        \n",
        "        # Create surface\n",
        "        surf = ax.plot_surface(X, Y, Z, \n",
        "                               cmap=cm.coolwarm,\n",
        "                               linewidth=0,\n",
        "                               antialiased=True,\n",
        "                               vmin=-1, vmax=1,\n",
        "                               alpha=0.8)\n",
        "        \n",
        "        ax.set_xlabel('Player Sum', fontsize=11)\n",
        "        ax.set_ylabel('Dealer Showing', fontsize=11)\n",
        "        ax.set_zlabel('Value', fontsize=11)\n",
        "        ax.set_zlim(-1, 1)\n",
        "        ax.view_init(elev=25, azim=-130)\n",
        "        \n",
        "        return surf\n",
        "    \n",
        "    # Create figure\n",
        "    fig = plt.figure(figsize=(14, 11))\n",
        "    \n",
        "    # With usable ace\n",
        "    ax1 = fig.add_subplot(211, projection='3d')\n",
        "    ax1.set_title('State Values WITH Usable Ace', \n",
        "                  fontsize=13, fontweight='bold', pad=15)\n",
        "    surf1 = create_surface(True, ax1)\n",
        "    fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=10)\n",
        "    \n",
        "    # Without usable ace\n",
        "    ax2 = fig.add_subplot(212, projection='3d')\n",
        "    ax2.set_title('State Values WITHOUT Usable Ace', \n",
        "                  fontsize=13, fontweight='bold', pad=15)\n",
        "    surf2 = create_surface(False, ax2)\n",
        "    fig.colorbar(surf2, ax=ax2, shrink=0.5, aspect=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_policy(policy):\n",
        "    \"\"\"\n",
        "    FIXED: Create properly formatted 2D policy heatmaps\n",
        "    \n",
        "    FIXES:\n",
        "    - Correct axis orientation\n",
        "    - Proper coordinate alignment\n",
        "    - Clear labeling\n",
        "    \"\"\"\n",
        "    def get_action(player_sum, dealer_card, usable_ace):\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        return policy.get(state, 1)  # Default: hit\n",
        "    \n",
        "    def create_heatmap(usable_ace, ax):\n",
        "        # Define ranges\n",
        "        player_range = range(12, 22)  # 12-21\n",
        "        dealer_range = range(1, 11)   # 1-10 (Ace to 10)\n",
        "        \n",
        "        # Create policy grid (rows=dealer, cols=player)\n",
        "        Z = np.array([[get_action(player, dealer, usable_ace)\n",
        "                      for player in player_range]\n",
        "                     for dealer in dealer_range])\n",
        "        \n",
        "        # Create heatmap\n",
        "        im = ax.imshow(Z, \n",
        "                       cmap='RdYlGn_r',  # Red=Hit, Green=Stick\n",
        "                       aspect='auto',\n",
        "                       vmin=0, vmax=1,\n",
        "                       extent=[11.5, 21.5, 0.5, 10.5],\n",
        "                       origin='lower',\n",
        "                       interpolation='nearest')\n",
        "        \n",
        "        # Set ticks\n",
        "        ax.set_xticks(range(12, 22))\n",
        "        ax.set_yticks(range(1, 11))\n",
        "        ax.set_yticklabels(['A'] + list(range(2, 11)))\n",
        "        \n",
        "        # Labels\n",
        "        ax.set_xlabel('Player Sum', fontsize=11)\n",
        "        ax.set_ylabel('Dealer Showing', fontsize=11)\n",
        "        \n",
        "        # Grid\n",
        "        ax.grid(True, color='black', linewidth=0.5, alpha=0.3)\n",
        "        ax.set_axisbelow(False)\n",
        "        \n",
        "        # Colorbar\n",
        "        cbar = plt.colorbar(im, ax=ax, ticks=[0, 1], fraction=0.046, pad=0.04)\n",
        "        cbar.ax.set_yticklabels(['STICK', 'HIT'])\n",
        "        \n",
        "        return im\n",
        "    \n",
        "    # Create figure\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # With usable ace\n",
        "    ax1.set_title('Policy WITH Usable Ace', fontsize=12, fontweight='bold')\n",
        "    create_heatmap(True, ax1)\n",
        "    \n",
        "    # Without usable ace\n",
        "    ax2.set_title('Policy WITHOUT Usable Ace', fontsize=12, fontweight='bold')\n",
        "    create_heatmap(False, ax2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"✓ FIXED visualization functions ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Running Monte Carlo Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 7: Execute Monte Carlo Learning\n",
        "\n",
        "CRITICAL EXPLANATION - TWO POLICIES:\n",
        "=====================================\n",
        "\n",
        "In this cell, we work with TWO different policies:\n",
        "\n",
        "1. ARBITRARY POLICY (for learning):\n",
        "   --------------------------------\n",
        "   - The stochastic policy we use to GENERATE episodes\n",
        "   - Defined in play_episode_arbitrary_policy()\n",
        "   - Threshold-based: prefer stick if sum>18, hit if sum≤18\n",
        "   - Purpose: Explore the environment to learn Q-values\n",
        "   - This is NOT what we're trying to find!\n",
        "\n",
        "2. OPTIMAL POLICY (what we're finding):\n",
        "   ------------------------------------\n",
        "   - The GREEDY policy extracted from learned Q-values\n",
        "   - Defined as: π*(s) = argmax_a Q(s,a)\n",
        "   - Deterministic: always picks best action\n",
        "   - This is the GOAL of our learning!\n",
        "\n",
        "PROCESS FLOW:\n",
        "   Arbitrary Policy → Episodes → Q-values → Optimal Policy\n",
        "   (exploration)     (data)    (learning)  (solution)\n",
        "\n",
        "ANALOGY:\n",
        "   - Arbitrary policy = \"practice games\" with exploration\n",
        "   - Q-values = knowledge learned from practice\n",
        "   - Optimal policy = \"tournament strategy\" using learned knowledge\n",
        "\"\"\"\n",
        "\n",
        "# Run Monte Carlo prediction\n",
        "NUM_EPISODES = 500000\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"LEARNING PHASE: Using ARBITRARY policy for exploration\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Episodes: {NUM_EPISODES:,}\")\n",
        "print(\"Arbitrary policy: Threshold-based stochastic\")\n",
        "print(\"Goal: Learn Q(s,a) values\\n\")\n",
        "\n",
        "# Learn Q-values using arbitrary policy\n",
        "Q = mc_predict(env, NUM_EPISODES)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXTRACTION PHASE: Deriving OPTIMAL policy from Q-values\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Convert Q-values to state values under arbitrary policy\n",
        "# V(s) = Σ π(a|s) * Q(s,a) for arbitrary policy\n",
        "V_arbitrary = {}\n",
        "for state, action_values in Q.items():\n",
        "    if state[0] > 18:\n",
        "        # Arbitrary policy: 80% stick, 20% hit\n",
        "        V_arbitrary[state] = 0.8 * action_values[0] + 0.2 * action_values[1]\n",
        "    else:\n",
        "        # Arbitrary policy: 20% stick, 80% hit\n",
        "        V_arbitrary[state] = 0.2 * action_values[0] + 0.8 * action_values[1]\n",
        "\n",
        "# Extract OPTIMAL policy (greedy w.r.t. Q)\n",
        "# This is the policy we're trying to find!\n",
        "optimal_policy = {}\n",
        "for state, action_values in Q.items():\n",
        "    # Select action with highest Q-value (greedy)\n",
        "    optimal_policy[state] = np.argmax(action_values)\n",
        "    \n",
        "print(\"\\n✓ Optimal policy extracted via greedy selection\")\n",
        "print(\"  π*(s) = argmax_a Q(s,a) for each state\\n\")\n",
        "\n",
        "# Analysis\n",
        "states_count = len(Q)\n",
        "stick_count = sum(1 for a in optimal_policy.values() if a == 0)\n",
        "hit_count = sum(1 for a in optimal_policy.values() if a == 1)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"States explored: {states_count}\")\n",
        "print(f\"\\nOptimal Policy Composition:\")\n",
        "print(f\"  STICK states: {stick_count} ({100*stick_count/states_count:.1f}%)\")\n",
        "print(f\"  HIT states:   {hit_count} ({100*hit_count/states_count:.1f}%)\")\n",
        "print(f\"\\nAverage state value: {np.mean(list(V_arbitrary.values())):.4f}\")\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Learned Q-values from arbitrary policy episodes\")\n",
        "print(\"  - Extracted optimal policy by always choosing best action\")\n",
        "print(\"  - Optimal policy is deterministic and greedy\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 8: Visualize Value Function\n",
        "\"\"\"\n",
        "print(\"Generating 3D value function plots...\\n\")\n",
        "plot_blackjack_values(V_arbitrary)\n",
        "\n",
        "print(\"\\nValue Function Interpretation:\")\n",
        "print(\"  • Red (high): Favorable states likely to win\")\n",
        "print(\"  • Blue (low): Unfavorable states likely to lose\")\n",
        "print(\"  • Peak near sum 20-21: Best positions\")\n",
        "print(\"  • Usable ace provides more flexibility\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 9: Visualize OPTIMAL Policy\n",
        "\"\"\"\n",
        "print(\"Generating optimal policy heatmaps...\\n\")\n",
        "plot_policy(optimal_policy)\n",
        "\n",
        "print(\"\\nOptimal Policy Interpretation:\")\n",
        "print(\"  • Green = STICK (action 0)\")\n",
        "print(\"  • Red = HIT (action 1)\")\n",
        "print(\"  • Clear threshold around sum 17-20\")\n",
        "print(\"  • More aggressive hitting with usable ace\")\n",
        "print(\"  • Adapts to dealer's showing card\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase;\">Key Findings</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>1. Policy Learning:</strong> We used an arbitrary exploration policy to generate episodes, then extracted the optimal policy from learned Q-values.</p>\n",
        "        <p><strong>2. Exploration vs Exploitation:</strong> Arbitrary policy provides exploration, optimal policy is purely exploitative (greedy).</p>\n",
        "        <p><strong>3. Usable Ace Impact:</strong> Optimal strategy differs significantly with usable ace - more aggressive hitting since can't bust.</p>\n",
        "        <p><strong>4. Decision Boundaries:</strong> Clear threshold emerges around sum 17-20 for stick/hit decision.</p>\n",
        "        <p><strong>5. Monte Carlo Strength:</strong> Model-free learning directly from experience converges to optimal behavior.</p>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>Why do we need an exploration policy if we're trying to find the optimal policy?</li>\n",
        "        <li>What would happen if we used a purely greedy policy from the start?</li>\n",
        "        <li>How does First-Visit MC differ from Every-Visit MC?</li>\n",
        "        <li>Why is MC particularly suitable for Blackjack vs Dynamic Programming?</li>\n",
        "        <li>How could we implement ε-greedy exploration instead of arbitrary policy?</li>\n",
        "    </ol>\n",
        "</div>"
      ]
    }
  ]
}
