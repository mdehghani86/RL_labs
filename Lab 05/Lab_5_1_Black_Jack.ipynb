{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_05_1_Blackjack_Monte_Carlo_FIXED.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 5-1: Blackjack with Monte Carlo Methods\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapter 5 | Intermediate Level | 75 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Monte Carlo methods learn directly from episodes of experience without requiring a model of the environment. \n",
        "        First introduced for RL by Stanislaw Ulam during the Manhattan Project, these methods are particularly effective \n",
        "        for episodic tasks. This lab implements the <strong>First-Visit Monte Carlo</strong> algorithm on the classic \n",
        "        Blackjack problem from Sutton & Barto (2018), Example 5.1. We explore how Monte Carlo methods estimate value \n",
        "        functions through repeated sampling and averaging of returns.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Understand Monte Carlo prediction methods</li>\n",
        "        <li>Implement First-Visit MC algorithm</li>\n",
        "        <li>Learn from sampled episodes of experience</li>\n",
        "        <li>Estimate action-value functions Q(s,a)</li>\n",
        "        <li>Visualize value functions and policies</li>\n",
        "        <li>Work with OpenAI Gymnasium environments</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Blackjack Rules</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Goal</code> → Get sum close to 21 without exceeding</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Actions</code> → Hit (draw card) or Stick (stop)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">States</code> → (player_sum, dealer_card, usable_ace)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Rewards</code> → +1 (win), 0 (draw), -1 (lose)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Ace</code> → Can be 1 or 11 (usable if 11)</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 1: Environment Setup and Dependencies</h2>\n",
        "</div>\n",
        "\n",
        "We begin by importing the necessary libraries for our Monte Carlo implementation. The key libraries are:\n",
        "- **Gymnasium**: Provides the Blackjack-v1 environment (successor to OpenAI Gym)\n",
        "- **NumPy**: For numerical computations and array operations\n",
        "- **Matplotlib**: For creating visualizations of value functions and policies\n",
        "- **Collections**: For efficient data structures like defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 1: Import Required Libraries\n",
        "\n",
        "Purpose:\n",
        "  - Import all necessary libraries for Monte Carlo implementation\n",
        "  - Configure matplotlib for publication-quality visualizations\n",
        "  - Suppress warnings for cleaner output\n",
        "\n",
        "Key Libraries:\n",
        "  - gymnasium: RL environment (Blackjack-v1)\n",
        "  - numpy: Numerical operations and array handling\n",
        "  - matplotlib: 3D and 2D plotting\n",
        "  - defaultdict: Efficient sparse state storage\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import gymnasium as gym  # Modern replacement for gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure matplotlib for better quality figures\n",
        "plt.rcParams['figure.dpi'] = 100          # Display resolution\n",
        "plt.rcParams['figure.figsize'] = (12, 8)  # Default figure size\n",
        "plt.rcParams['font.size'] = 10            # Font size for labels\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n",
        "print(f\"✓ Gymnasium version: {gym.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 2: Creating the Blackjack Environment</h2>\n",
        "</div>\n",
        "\n",
        "The Blackjack environment simulates the card game with simplified rules. The state space consists of three components:\n",
        "1. **Player sum** (12-21): Current sum of player cards\n",
        "2. **Dealer card** (1-10): The dealer's visible card (1 = Ace, 10 = face cards)\n",
        "3. **Usable ace** (True/False): Whether player has an ace counted as 11\n",
        "\n",
        "The action space has two actions: Stick (0) to stop taking cards, or Hit (1) to draw another card. Rewards are given only at episode termination: +1 for winning, 0 for drawing, and -1 for losing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 2: Initialize Blackjack-v1 Environment\n",
        "\n",
        "Purpose:\n",
        "  - Create the Blackjack environment using Gymnasium\n",
        "  - Verify environment properties (state/action spaces)\n",
        "  - Demonstrate initial state generation\n",
        "\n",
        "Environment Details:\n",
        "  - State: (player_sum, dealer_card, usable_ace)\n",
        "    * player_sum: 12-21 (game starts at 12+)\n",
        "    * dealer_card: 1-10 (Ace through face cards)\n",
        "    * usable_ace: Boolean (True if ace counts as 11)\n",
        "  - Actions: 0=Stick, 1=Hit\n",
        "  - Rewards: {+1, 0, -1} given at episode end only\n",
        "\"\"\"\n",
        "\n",
        "# Create environment using Gymnasium API\n",
        "env = gym.make('Blackjack-v1')  # Use v1 (v0 deprecated)\n",
        "\n",
        "print(f\"Environment: Blackjack-v1\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Number of actions: {env.action_space.n}\")\n",
        "print(\"\\nActions:\")\n",
        "print(\"  0 = Stick (stop drawing cards)\")\n",
        "print(\"  1 = Hit (draw another card)\")\n",
        "\n",
        "# Reset environment to get initial state\n",
        "sample_state, _ = env.reset()  # v1 returns (state, info)\n",
        "print(f\"\\nSample initial state: {sample_state}\")\n",
        "print(f\"  Player sum: {sample_state[0]}\")\n",
        "print(f\"  Dealer showing: {sample_state[1]}\")\n",
        "print(f\"  Usable ace: {sample_state[2]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #6f42c1; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #6f42c1; margin: 0; font-size: 18px;\">Section 3: Monte Carlo ES Algorithm Overview</h2>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #6f42c1;\">\n",
        "    <h3 style=\"color: #6f42c1; font-size: 14px; margin: 0 0 8px 0;\">Monte Carlo with Exploring Starts</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Monte Carlo ES uses <strong>Exploring Starts</strong> to ensure comprehensive exploration of the state-action space. \n",
        "        Each episode begins with a random state-action pair, guaranteeing that all possibilities are visited. After the \n",
        "        initial random selection, the agent follows its current policy for the remainder of the episode. This approach \n",
        "        solves the exploration problem while still converging to the optimal policy through iterative improvement.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <img src=\"https://github.com/mdehghani86/RL_labs/blob/master/Lab%2005/MCM_ES.jpg?raw=true\" \n",
        "         alt=\"Monte Carlo ES Pseudocode\" \n",
        "         style=\"width: 70%; max-width: 800px; border: 2px solid #6f42c1; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
        "    <p style=\"color: #666; font-size: 12px; margin-top: 10px; font-style: italic;\">Figure: Monte Carlo ES Algorithm from Sutton & Barto</p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: #e8f5e9; padding: 12px 15px; border-left: 3px solid #4caf50; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #2e7d32; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Algorithm Steps</h4>\n",
        "    <ol style=\"color: #555; line-height: 1.6; margin: 0; padding-left: 20px; font-size: 12px;\">\n",
        "        <li><strong>Exploring Start:</strong> Choose random (S₀, A₀) pair</li>\n",
        "        <li><strong>Generate Episode:</strong> Follow current policy π from S₁ onward</li>\n",
        "        <li><strong>Calculate Returns:</strong> Compute G for each visited (s,a)</li>\n",
        "        <li><strong>Update Q-values:</strong> Average all returns for each (s,a) pair</li>\n",
        "        <li><strong>Policy Improvement:</strong> Make policy greedy: π(s) ← argmax Q(s,a)</li>\n",
        "    </ol>\n",
        "</td>\n",
        "<td style=\"background: #fff3e0; padding: 12px 15px; border-left: 3px solid #ff9800; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #e65100; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Why Exploring Starts?</h4>\n",
        "    <p style=\"color: #555; font-size: 12px; line-height: 1.6; margin: 0 0 8px 0;\">\n",
        "        Without exploring starts, a deterministic policy might never visit certain state-action pairs, \n",
        "        preventing optimal value estimation. Random initialization ensures every (s,a) pair has non-zero \n",
        "        probability of being explored.\n",
        "    </p>\n",
        "    <p style=\"color: #555; font-size: 12px; line-height: 1.6; margin: 0;\">\n",
        "        <strong>Key Guarantee:</strong> All state-action pairs are visited infinitely often as episodes → ∞\n",
        "    </p>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #28a745; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #28a745; margin: 0; font-size: 18px;\">Section 4: Stochastic Policy for Exploration</h2>\n",
        "</div>\n",
        "\n",
        "In this implementation, we use an **arbitrary stochastic policy** for generating learning episodes. This policy serves as our exploration mechanism during the learning phase. The policy is threshold-based:\n",
        "- When player sum > 18: Prefer to stick (80% probability) to avoid busting\n",
        "- When player sum ≤ 18: Prefer to hit (80% probability) to get closer to 21\n",
        "\n",
        "This is NOT the optimal policy we're trying to find. Rather, it's a reasonable exploration strategy that ensures we visit diverse states and actions. From the Q-values learned using this arbitrary policy, we will later extract the optimal greedy policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 3: Generate Episodes Using Arbitrary Stochastic Policy\n",
        "\n",
        "Purpose:\n",
        "  - Play complete Blackjack episodes for data collection\n",
        "  - Use arbitrary threshold-based policy for exploration\n",
        "  - Record (state, action, reward) tuples for each step\n",
        "\n",
        "Policy Definition (ARBITRARY - not optimal):\n",
        "  - If player_sum > 18:\n",
        "      P(Stick) = 0.8, P(Hit) = 0.2  (conservative)\n",
        "  - If player_sum ≤ 18:\n",
        "      P(Stick) = 0.2, P(Hit) = 0.8  (aggressive)\n",
        "\n",
        "Why Arbitrary Policy?\n",
        "  - Provides reasonable exploration\n",
        "  - Ensures we visit diverse state-action pairs\n",
        "  - We'll learn Q-values from these episodes\n",
        "  - Then extract optimal policy via greedy selection\n",
        "\n",
        "Returns:\n",
        "  episode: List of (state, action, reward) tuples\n",
        "\"\"\"\n",
        "\n",
        "def play_episode_arbitrary_policy(env):\n",
        "    episode = []\n",
        "    state, _ = env.reset()  # Start new game\n",
        "    \n",
        "    while True:\n",
        "        # Define action probabilities based on player's sum\n",
        "        if state[0] > 18:\n",
        "            # High sum: mostly stick to avoid busting\n",
        "            action_probs = [0.8, 0.2]  # [P(stick), P(hit)]\n",
        "        else:\n",
        "            # Low sum: mostly hit to approach 21\n",
        "            action_probs = [0.2, 0.8]  # [P(stick), P(hit)]\n",
        "        \n",
        "        # Sample action from probability distribution\n",
        "        action = np.random.choice([0, 1], p=action_probs)\n",
        "        \n",
        "        # Execute action in environment (v1 returns 5-tuple)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated  # Episode ends on either\n",
        "        \n",
        "        # Store experience tuple\n",
        "        episode.append((state, action, reward))\n",
        "        \n",
        "        # Update state for next iteration\n",
        "        state = next_state\n",
        "        \n",
        "        if done:\n",
        "            break  # Episode complete\n",
        "    \n",
        "    return episode\n",
        "\n",
        "# Test episode generation\n",
        "sample_episode = play_episode_arbitrary_policy(env)\n",
        "print(f\"Sample episode length: {len(sample_episode)} steps\")\n",
        "print(f\"Final reward: {sample_episode[-1][2]}\")\n",
        "print(f\"\\nFirst 3 steps:\")\n",
        "for i, (state, action, reward) in enumerate(sample_episode[:3]):\n",
        "    action_name = \"Stick\" if action == 0 else \"Hit\"\n",
        "    print(f\"  {i+1}. State={state}, Action={action_name}, Reward={reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #dc3545; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #dc3545; margin: 0; font-size: 18px;\">Section 5: First-Visit Monte Carlo Q-Value Updates</h2>\n",
        "</div>\n",
        "\n",
        "The core of Monte Carlo learning is the update of Q-values based on observed returns. We implement the **First-Visit MC** approach:\n",
        "\n",
        "**First-Visit Rule:** For each (state, action) pair, only the FIRST occurrence in an episode is used for updates. Subsequent visits to the same pair are ignored.\n",
        "\n",
        "**Return Calculation:** From time t when (s,a) is first visited:\n",
        "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$$\n",
        "\n",
        "**Q-value Update:** The action-value is the average of all observed returns:\n",
        "$$Q(s,a) = \\frac{1}{N(s,a)} \\sum_{\\text{episodes}} G_t$$\n",
        "\n",
        "Where N(s,a) is the number of times we've made a first-visit to (s,a) across all episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 4: Update Q-Values Using First-Visit Monte Carlo\n",
        "\n",
        "Purpose:\n",
        "  - Update action-value estimates Q(s,a) from episode experience\n",
        "  - Implement FIRST-VISIT rule (only first occurrence counts)\n",
        "  - Maintain running averages of returns\n",
        "\n",
        "Algorithm:\n",
        "  1. For each (state, action) in episode:\n",
        "     - Check if this is FIRST visit to (s,a)\n",
        "     - If yes: calculate return G from this point forward\n",
        "     - Update running sum and count\n",
        "     - Compute new average: Q(s,a) = sum(returns) / count\n",
        "\n",
        "Parameters:\n",
        "  episode: List of (state, action, reward) tuples\n",
        "  Q: Action-value estimates (dict of arrays)\n",
        "  returns_sum: Cumulative sum of returns for each (s,a)\n",
        "  N: Visit counts for each (s,a)\n",
        "  gamma: Discount factor (1.0 for undiscounted)\n",
        "\"\"\"\n",
        "\n",
        "def update_Q(episode, Q, returns_sum, N, gamma=1.0):\n",
        "    # Track which (s,a) pairs we've already processed (first-visit)\n",
        "    visited = set()\n",
        "    \n",
        "    # Process each step in the episode\n",
        "    for t, (state, action, reward) in enumerate(episode):\n",
        "        sa_pair = (state, action)  # Create hashable pair\n",
        "        \n",
        "        # First-visit check: only process if not seen before\n",
        "        if sa_pair not in visited:\n",
        "            visited.add(sa_pair)  # Mark as visited\n",
        "            \n",
        "            # Calculate return G_t from time t onwards\n",
        "            # G_t = r_{t+1} + γ*r_{t+2} + γ²*r_{t+3} + ...\n",
        "            G = sum((gamma ** k) * r \n",
        "                    for k, (_, _, r) in enumerate(episode[t:]))\n",
        "            \n",
        "            # Update cumulative sum of returns\n",
        "            returns_sum[state][action] += G\n",
        "            \n",
        "            # Increment visit counter\n",
        "            N[state][action] += 1.0\n",
        "            \n",
        "            # Update Q-value as running average\n",
        "            # Q(s,a) = average of all returns from (s,a)\n",
        "            Q[state][action] = returns_sum[state][action] / N[state][action]\n",
        "\n",
        "print(\"✓ Q-value update function ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 5: Monte Carlo Prediction - Main Learning Loop\n",
        "\n",
        "Purpose:\n",
        "  - Run multiple episodes to learn Q-values\n",
        "  - Aggregate experience across many games\n",
        "  - Estimate Q(s,a) through averaging\n",
        "\n",
        "Process:\n",
        "  1. Initialize Q, returns_sum, and visit counts (N)\n",
        "  2. For each episode:\n",
        "     - Generate episode using arbitrary policy\n",
        "     - Update Q-values using first-visit MC\n",
        "  3. Return learned Q-values\n",
        "\n",
        "Data Structures:\n",
        "  - Q: defaultdict storing Q(s,a) estimates\n",
        "  - returns_sum: Cumulative returns for averaging\n",
        "  - N: Visit counts for each (s,a) pair\n",
        "\n",
        "Returns:\n",
        "  Q: Dictionary mapping states to action-value arrays\n",
        "\"\"\"\n",
        "\n",
        "def mc_predict(env, num_episodes, gamma=1.0):\n",
        "    # Initialize data structures using defaultdict for sparse storage\n",
        "    # defaultdict automatically creates entries as needed\n",
        "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    print(f\"Starting MC prediction with {num_episodes:,} episodes...\\n\")\n",
        "    \n",
        "    # Main learning loop\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Generate episode using arbitrary exploration policy\n",
        "        episode = play_episode_arbitrary_policy(env)\n",
        "        \n",
        "        # Update Q-values based on observed returns\n",
        "        update_Q(episode, Q, returns_sum, N, gamma)\n",
        "        \n",
        "        # Progress reporting every 50k episodes\n",
        "        if i_episode % 50000 == 0:\n",
        "            print(f\"Episode {i_episode:,}/{num_episodes:,}\")\n",
        "    \n",
        "    print(\"\\n✓ Monte Carlo prediction complete\")\n",
        "    return Q\n",
        "\n",
        "print(\"✓ MC prediction function ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #ffc107; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #e0a800; margin: 0; font-size: 18px;\">Section 6: Visualization Functions</h2>\n",
        "</div>\n",
        "\n",
        "We create two types of visualizations to understand the learned value function and policy:\n",
        "\n",
        "**3D Surface Plots:** Display state values V(s) as a function of player sum and dealer showing card. The height and color of the surface represent the expected value of being in that state. We create separate plots for states with and without a usable ace, as the ace significantly affects strategy.\n",
        "\n",
        "**2D Policy Heatmaps:** Show the optimal action (Stick or Hit) for each state using color coding. Green indicates Stick (action 0) and Red indicates Hit (action 1). These heatmaps provide an intuitive view of the decision boundaries learned by the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 6: Create 3D Surface Plots for Value Functions\n",
        "\n",
        "Purpose:\n",
        "  - Visualize state values V(s) as 3D surfaces\n",
        "  - Show how value varies with player sum and dealer card\n",
        "  - Create separate plots for usable/non-usable ace\n",
        "\n",
        "Visualization Details:\n",
        "  - X-axis: Player sum (12-21)\n",
        "  - Y-axis: Dealer showing card (1-10, where 1=Ace)\n",
        "  - Z-axis: State value V(s)\n",
        "  - Color: Blue (low value) to Red (high value)\n",
        "\n",
        "Function: plot_blackjack_values(V)\n",
        "  Input: V = dictionary of state values\n",
        "  Output: Two 3D surface plots displayed\n",
        "\"\"\"\n",
        "\n",
        "def plot_blackjack_values(V):\n",
        "    def get_Z(player_sum, dealer_card, usable_ace):\n",
        "        \"\"\"Lookup value for state, return 0 if not in V\"\"\"\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        return V.get(state, 0)\n",
        "    \n",
        "    def create_surface(usable_ace, ax):\n",
        "        \"\"\"Create 3D surface plot for given usable_ace condition\"\"\"\n",
        "        # Define state space ranges\n",
        "        player_range = np.arange(12, 22)  # 12 to 21\n",
        "        dealer_range = np.arange(1, 11)   # 1 (Ace) to 10\n",
        "        \n",
        "        # Create meshgrid: X[i,j]=player_sum, Y[i,j]=dealer_card\n",
        "        X, Y = np.meshgrid(player_range, dealer_range)\n",
        "        \n",
        "        # Build Z array: Z[i,j] = V(player_range[j], dealer_range[i], usable_ace)\n",
        "        Z = np.array([[get_Z(x, y, usable_ace) \n",
        "                      for x in player_range]    # Columns: player sums\n",
        "                     for y in dealer_range])    # Rows: dealer cards\n",
        "        \n",
        "        # Create 3D surface plot\n",
        "        surf = ax.plot_surface(\n",
        "            X, Y, Z,                    # Coordinates and heights\n",
        "            cmap=cm.coolwarm,           # Blue to red colormap\n",
        "            linewidth=0,                # No wireframe\n",
        "            antialiased=True,           # Smooth rendering\n",
        "            vmin=-1, vmax=1,            # Value range for colors\n",
        "            alpha=0.8                   # Slight transparency\n",
        "        )\n",
        "        \n",
        "        # Configure axes and labels\n",
        "        ax.set_xlabel('Player Sum', fontsize=11)\n",
        "        ax.set_ylabel('Dealer Showing', fontsize=11)\n",
        "        ax.set_zlabel('Value', fontsize=11)\n",
        "        ax.set_zlim(-1, 1)\n",
        "        ax.view_init(elev=25, azim=-130)  # Viewing angle\n",
        "        return surf\n",
        "    \n",
        "    # Create figure with two subplots\n",
        "    fig = plt.figure(figsize=(14, 11))\n",
        "    \n",
        "    # Plot 1: States WITH usable ace\n",
        "    ax1 = fig.add_subplot(211, projection='3d')\n",
        "    ax1.set_title('State Values WITH Usable Ace', \n",
        "                  fontsize=13, fontweight='bold', pad=15)\n",
        "    surf1 = create_surface(True, ax1)\n",
        "    fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=10)\n",
        "    \n",
        "    # Plot 2: States WITHOUT usable ace\n",
        "    ax2 = fig.add_subplot(212, projection='3d')\n",
        "    ax2.set_title('State Values WITHOUT Usable Ace', \n",
        "                  fontsize=13, fontweight='bold', pad=15)\n",
        "    surf2 = create_surface(False, ax2)\n",
        "    fig.colorbar(surf2, ax=ax2, shrink=0.5, aspect=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"✓ 3D value function plotting ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 7: Create 2D Policy Heatmaps (FIXED FOR DISCRETE VALUES)\n",
        "\n",
        "Purpose:\n",
        "  - Visualize policy π(s) as 2D heatmaps\n",
        "  - Show which action is optimal for each state\n",
        "  - Use discrete colors: Green=Stick, Red=Hit\n",
        "\n",
        "CRITICAL FIX:\n",
        "  - Use pcolormesh instead of imshow for discrete values\n",
        "  - Ensures crisp boundaries between actions\n",
        "  - No interpolation between policy decisions\n",
        "\n",
        "Visualization Details:\n",
        "  - X-axis: Player sum (12-21)\n",
        "  - Y-axis: Dealer card (Ace, 2-10)\n",
        "  - Color: Green = STICK (0), Red = HIT (1)\n",
        "\n",
        "Function: plot_policy(policy)\n",
        "  Input: policy = dictionary mapping states to actions\n",
        "  Output: Two 2D heatmaps displayed side-by-side\n",
        "\"\"\"\n",
        "\n",
        "def plot_policy(policy):\n",
        "    def get_action(player_sum, dealer_card, usable_ace):\n",
        "        \"\"\"Lookup action for state, default to Hit if not in policy\"\"\"\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        return policy.get(state, 1)  # Default: hit\n",
        "    \n",
        "    def create_heatmap(usable_ace, ax):\n",
        "        \"\"\"Create discrete 2D heatmap for given usable_ace condition\"\"\"\n",
        "        # Define state space\n",
        "        player_range = np.arange(12, 22)  # 12-21\n",
        "        dealer_range = np.arange(1, 11)   # 1-10 (Ace to 10)\n",
        "        \n",
        "        # Build policy grid: Z[i,j] = action\n",
        "        # Rows = dealer cards, Columns = player sums\n",
        "        Z = np.array([[get_action(player, dealer, usable_ace)\n",
        "                      for player in player_range]\n",
        "                     for dealer in dealer_range])\n",
        "        \n",
        "        # CRITICAL: Use pcolormesh for discrete values (not imshow)\n",
        "        # This ensures no interpolation between action values\n",
        "        im = ax.pcolormesh(\n",
        "            player_range,               # X coordinates\n",
        "            dealer_range,               # Y coordinates  \n",
        "            Z,                          # Action values (0 or 1)\n",
        "            cmap='RdYlGn_r',           # Red=Hit(1), Green=Stick(0)\n",
        "            edgecolors='black',         # Black grid lines\n",
        "            linewidth=0.5,              # Grid line thickness\n",
        "            vmin=0, vmax=1,             # Discrete action values\n",
        "            shading='flat'              # Flat colors (no interpolation)\n",
        "        )\n",
        "        \n",
        "        # Configure ticks and labels\n",
        "        ax.set_xticks(player_range)\n",
        "        ax.set_yticks(dealer_range)\n",
        "        # Display 'A' for Ace (value 1)\n",
        "        ax.set_yticklabels(['A'] + list(range(2, 11)))\n",
        "        \n",
        "        ax.set_xlabel('Player Sum', fontsize=11)\n",
        "        ax.set_ylabel('Dealer Showing', fontsize=11)\n",
        "        \n",
        "        # Add colorbar with discrete labels\n",
        "        cbar = plt.colorbar(im, ax=ax, ticks=[0.25, 0.75], \n",
        "                           fraction=0.046, pad=0.04)\n",
        "        cbar.ax.set_yticklabels(['STICK (0)', 'HIT (1)'])\n",
        "        \n",
        "        # Set aspect ratio to square\n",
        "        ax.set_aspect('equal')\n",
        "        \n",
        "        return im\n",
        "    \n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Heatmap 1: Policy WITH usable ace\n",
        "    ax1.set_title('Policy WITH Usable Ace', \n",
        "                  fontsize=12, fontweight='bold')\n",
        "    create_heatmap(True, ax1)\n",
        "    \n",
        "    # Heatmap 2: Policy WITHOUT usable ace  \n",
        "    ax2.set_title('Policy WITHOUT Usable Ace', \n",
        "                  fontsize=12, fontweight='bold')\n",
        "    create_heatmap(False, ax2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"✓ 2D policy heatmap plotting ready (DISCRETE)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #007bff; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #007bff; margin: 0; font-size: 18px;\">Section 7: Running Monte Carlo Experiments</h2>\n",
        "</div>\n",
        "\n",
        "Now we execute the complete Monte Carlo learning process. This section demonstrates the distinction between two key concepts:\n",
        "\n",
        "**Arbitrary Exploration Policy:** The stochastic threshold-based policy we defined earlier is used to GENERATE episodes and collect experience. This policy explores the environment but is not necessarily optimal. It serves as our data collection mechanism.\n",
        "\n",
        "**Optimal Policy Extraction:** After learning Q-values from the arbitrary policy's experiences, we extract the optimal policy by selecting the action with highest Q-value in each state: π*(s) = argmax_a Q(s,a). This greedy policy represents what we've learned about the best way to play Blackjack.\n",
        "\n",
        "The learning process flows as: Exploration Policy → Generate Episodes → Learn Q-values → Extract Optimal Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 8: Execute Monte Carlo Learning - Main Experiment\n",
        "\n",
        "Purpose:\n",
        "  - Run large-scale MC prediction (500k episodes)\n",
        "  - Learn Q(s,a) using arbitrary exploration policy\n",
        "  - Extract optimal policy via greedy selection\n",
        "  - Compute state values under both policies\n",
        "\n",
        "CRITICAL CONCEPTS:\n",
        "\n",
        "TWO POLICIES IN PLAY:\n",
        "  1. ARBITRARY POLICY (exploration):\n",
        "     - Used to GENERATE episodes\n",
        "     - Stochastic threshold-based\n",
        "     - Ensures diverse experience\n",
        "     - NOT what we're trying to find\n",
        "  \n",
        "  2. OPTIMAL POLICY (exploitation):\n",
        "     - EXTRACTED from learned Q-values\n",
        "     - Greedy: π*(s) = argmax_a Q(s,a)\n",
        "     - Deterministic best action\n",
        "     - THIS is our learning goal\n",
        "\n",
        "LEARNING FLOW:\n",
        "  Arbitrary Policy → Episodes → Q-values → Optimal Policy\n",
        "  (exploration)    (data)     (learning)  (solution)\n",
        "\"\"\"\n",
        "\n",
        "NUM_EPISODES = 500000\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PHASE 1: LEARNING Q-VALUES\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Episodes to run: {NUM_EPISODES:,}\")\n",
        "print(\"Method: First-Visit Monte Carlo\")\n",
        "print(\"Exploration: Arbitrary stochastic policy\")\n",
        "print(\"Goal: Learn Q(s,a) for all state-action pairs\\n\")\n",
        "\n",
        "# Run Monte Carlo prediction to learn Q-values\n",
        "Q = mc_predict(env, NUM_EPISODES)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PHASE 2: POLICY EXTRACTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Compute state values V(s) under ARBITRARY policy\n",
        "# V(s) = Σ_a π(a|s) * Q(s,a) where π is arbitrary policy\n",
        "V_arbitrary = {}\n",
        "for state, action_values in Q.items():\n",
        "    if state[0] > 18:\n",
        "        # Arbitrary policy: 80% stick, 20% hit\n",
        "        V_arbitrary[state] = 0.8 * action_values[0] + 0.2 * action_values[1]\n",
        "    else:\n",
        "        # Arbitrary policy: 20% stick, 80% hit\n",
        "        V_arbitrary[state] = 0.2 * action_values[0] + 0.8 * action_values[1]\n",
        "\n",
        "# Extract OPTIMAL policy via greedy selection\n",
        "# π*(s) = argmax_a Q(s,a) for each state\n",
        "optimal_policy = {}\n",
        "for state, action_values in Q.items():\n",
        "    # Select action with highest Q-value (greedy)\n",
        "    optimal_policy[state] = np.argmax(action_values)\n",
        "\n",
        "print(\"✓ Optimal policy extracted via: π*(s) = argmax_a Q(s,a)\\n\")\n",
        "\n",
        "# Analyze learned policy\n",
        "states_count = len(Q)\n",
        "stick_count = sum(1 for a in optimal_policy.values() if a == 0)\n",
        "hit_count = sum(1 for a in optimal_policy.values() if a == 1)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nLearning Statistics:\")\n",
        "print(f\"  States explored: {states_count}\")\n",
        "print(f\"  Average state value: {np.mean(list(V_arbitrary.values())):.4f}\")\n",
        "print(f\"\\nOptimal Policy Composition:\")\n",
        "print(f\"  States where optimal action is STICK: {stick_count} ({100*stick_count/states_count:.1f}%)\")\n",
        "print(f\"  States where optimal action is HIT:   {hit_count} ({100*hit_count/states_count:.1f}%)\")\n",
        "print(f\"\\nKey Insight:\")\n",
        "print(f\"  Policy learned to stick more often at higher sums\")\n",
        "print(f\"  This matches optimal Blackjack strategy\")\n",
        "print(\"=\"*70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 9: Visualize Learned Value Function\n",
        "\n",
        "Purpose:\n",
        "  - Display 3D surface plots of state values\n",
        "  - Show how value changes with player sum and dealer card\n",
        "  - Compare states with/without usable ace\n",
        "\n",
        "Interpretation Guide:\n",
        "  - Red (high values): Favorable states, likely to win\n",
        "  - Blue (low values): Unfavorable states, likely to lose\n",
        "  - Peak near sum 20-21: Best winning positions\n",
        "  - Valley at low sums: Poor positions needing improvement\n",
        "\"\"\"\n",
        "\n",
        "print(\"Generating 3D value function plots...\\n\")\n",
        "plot_blackjack_values(V_arbitrary)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VALUE FUNCTION INTERPRETATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"Color Coding:\")\n",
        "print(\"  🔴 Red (high): Favorable states with positive expected return\")\n",
        "print(\"  🔵 Blue (low): Unfavorable states with negative expected return\")\n",
        "print(\"\\nKey Observations:\")\n",
        "print(\"  • Peak values near player sum 20-21 (close to winning)\")\n",
        "print(\"  • Lower values with weak dealer cards (dealer likely to bust)\")\n",
        "print(\"  • Usable ace provides more flexibility and higher values\")\n",
        "print(\"=\"*70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 10: Visualize Optimal Policy (DISCRETE COLORS)\n",
        "\n",
        "Purpose:\n",
        "  - Display 2D heatmaps of optimal policy\n",
        "  - Show STICK vs HIT decisions for each state\n",
        "  - Use discrete colors (no blending)\n",
        "\n",
        "Color Coding:\n",
        "  - 🟢 Green = STICK (action 0)\n",
        "  - 🔴 Red = HIT (action 1)\n",
        "\n",
        "Interpretation Guide:\n",
        "  - Clear decision boundary around sum 17-20\n",
        "  - More aggressive hitting with usable ace (can't bust)\n",
        "  - Adapts to dealer's showing card\n",
        "  - Matches known optimal Blackjack strategy\n",
        "\"\"\"\n",
        "\n",
        "print(\"Generating optimal policy heatmaps (discrete)...\\n\")\n",
        "plot_policy(optimal_policy)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"OPTIMAL POLICY INTERPRETATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"Color Coding:\")\n",
        "print(\"  🟢 Green = STICK (action 0) - Stop drawing cards\")\n",
        "print(\"  🔴 Red = HIT (action 1) - Draw another card\")\n",
        "print(\"\\nPolicy Patterns Observed:\")\n",
        "print(\"  • Clear threshold around player sum 17-20\")\n",
        "print(\"  • More conservative without usable ace (risk of busting)\")\n",
        "print(\"  • More aggressive with usable ace (flexibility)\")\n",
        "print(\"  • Adapts based on dealer's showing card\")\n",
        "print(\"  • Matches expert Blackjack strategy\")\n",
        "print(\"=\"*70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase;\">Key Findings</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>1. Policy Learning:</strong> Used arbitrary exploration policy to generate episodes, then extracted optimal policy from learned Q-values through greedy selection.</p>\n",
        "        <p><strong>2. Exploration vs Exploitation:</strong> Arbitrary policy provides exploration during learning, optimal policy is purely exploitative at decision time.</p>\n",
        "        <p><strong>3. Usable Ace Impact:</strong> Optimal strategy differs significantly with usable ace due to flexibility in avoiding bust.</p>\n",
        "        <p><strong>4. Decision Boundaries:</strong> Clear threshold emerges around sum 17-20 for stick/hit decision, adapting to dealer card.</p>\n",
        "        <p><strong>5. Monte Carlo Strength:</strong> Model-free learning directly from experience converges to near-optimal behavior without environment dynamics.</p>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>Why do we need an exploration policy if we are trying to find the optimal policy?</li>\n",
        "        <li>What would happen if we used a purely greedy policy from the start?</li>\n",
        "        <li>How does First-Visit MC differ from Every-Visit MC in terms of bias and variance?</li>\n",
        "        <li>Why is Monte Carlo particularly suitable for Blackjack compared to Dynamic Programming?</li>\n",
        "        <li>How could we implement epsilon-greedy exploration instead of arbitrary policy?</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 5-1: Blackjack with Monte Carlo Methods</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Next: Lab 5-2 - Monte Carlo Control</p>\n",
        "</div>"
      ]
    }
  ]
}
