{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_05_1_Blackjack_Monte_Carlo_ES_Complete.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 5-1: Blackjack with Monte Carlo ES\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton and Barto Chapter 5 Figure 5.2 | 75 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        This lab implements <strong>Monte Carlo ES (Exploring Starts)</strong> exactly as described in Sutton and Barto Figure 5.2. \n",
        "        The algorithm learns the optimal Blackjack policy without requiring a model of the environment. The key insight is \n",
        "        <strong>Exploring Starts</strong>: each episode begins with a randomly selected state-action pair, guaranteeing that all \n",
        "        state-action pairs are visited infinitely often. After the initial random action, the agent follows its current greedy policy. \n",
        "        This combination ensures both exploration and convergence to the optimal policy through Generalized Policy Iteration.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Implement Monte Carlo ES from Figure 5.2</li>\n",
        "        <li>Understand exploring starts mechanism</li>\n",
        "        <li>Apply first-visit MC for Q-value estimation</li>\n",
        "        <li>Implement greedy policy improvement</li>\n",
        "        <li>Reproduce textbook Blackjack results</li>\n",
        "        <li>Visualize value functions and policies</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Blackjack Rules</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Goal</code> → Sum close to 21 without exceeding</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Actions</code> → 0=Stick (stop), 1=Hit (draw card)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">State</code> → (player_sum, dealer_card, usable_ace)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Rewards</code> → +1 (win), 0 (draw), -1 (lose)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Ace</code> → Can be 1 or 11 (usable if counted as 11)</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 1: Environment Setup and Dependencies</h2>\n",
        "</div>\n",
        "\n",
        "We begin by importing all necessary libraries for the Monte Carlo ES implementation:\n",
        "\n",
        "- **Gymnasium**: Provides the Blackjack-v1 environment with modern API (replaces deprecated gym)\n",
        "- **NumPy**: For numerical computations, array operations, and random number generation\n",
        "- **Matplotlib**: For creating 3D surface plots (value functions) and 2D heatmaps (policies)\n",
        "- **defaultdict**: Efficient sparse storage for Q-values and returns lists\n",
        "- **pretty_print utility**: For formatted, color-coded output messages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 1: Import Libraries and Initialize Environment\n",
        "\n",
        "PURPOSE:\n",
        "  - Import all required libraries for MC ES implementation\n",
        "  - Load pretty_print utility for formatted output\n",
        "  - Configure matplotlib for high-quality visualizations\n",
        "  - Create Blackjack-v1 environment\n",
        "\n",
        "KEY LIBRARIES:\n",
        "  - gymnasium: Modern RL environment library (v1 API with 5-tuple returns)\n",
        "  - numpy: Numerical operations, averaging, argmax for greedy selection\n",
        "  - defaultdict: Auto-initializing dictionaries for sparse Q-value storage\n",
        "  - matplotlib: 3D surface plots and 2D heatmaps with discrete colors\n",
        "\n",
        "CONFIGURATION:\n",
        "  - Figure DPI: 100 (display quality)\n",
        "  - Figure size: 12x8 inches (default)\n",
        "  - Font size: 10pt (readable labels)\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import gymnasium as gym  # Modern replacement for OpenAI gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from mpl_toolkits.mplot3d import Axes3D  # For 3D surface plots\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm  # Colormaps for visualizations\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress matplotlib warnings\n",
        "\n",
        "# Configure matplotlib for publication-quality figures\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Load pretty_print utility from GitHub repository\n",
        "try:\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/mdehghani86/RL_labs/master/utility/rl_utility.py'\n",
        "    response = requests.get(url)\n",
        "    exec(response.text)  # Execute utility code to load pretty_print function\n",
        "    \n",
        "    pretty_print(\"Environment Ready\", \n",
        "                 f\"Gymnasium version: {gym.__version__}<br>\" +\n",
        "                 \"NumPy, Matplotlib loaded successfully<br>\" +\n",
        "                 \"Implementing Monte Carlo ES from Figure 5.2<br>\" +\n",
        "                 \"All dependencies ready\", \n",
        "                 style='success')\nexcept Exception as e:\n",
        "    # Fallback if GitHub fetch fails\n",
        "    print(f\"Libraries loaded successfully\")\n",
        "    print(f\"Gymnasium version: {gym.__version__}\")\n",
        "    print(f\"Note: pretty_print unavailable ({e})\")\n",
        "\n",
        "# Create Blackjack environment using Gymnasium v1 API\n",
        "# v1 uses modern API: reset() returns (state, info), step() returns 5-tuple\n",
        "env = gym.make('Blackjack-v1')\n",
        "\n",
        "pretty_print(\"Blackjack Environment Created\",\n",
        "             f\"<strong>Action Space:</strong> {env.action_space.n} actions<br>\" +\n",
        "             \"• 0 = Stick (stop drawing cards)<br>\" +\n",
        "             \"• 1 = Hit (draw another card)<br><br>\" +\n",
        "             \"<strong>State Space:</strong><br>\" +\n",
        "             \"• player_sum: 12-21 (game starts at 12+)<br>\" +\n",
        "             \"• dealer_card: 1-10 (Ace=1, face cards=10)<br>\" +\n",
        "             \"• usable_ace: True/False<br><br>\" +\n",
        "             \"<strong>Rewards:</strong> Terminal only (+1 win, 0 draw, -1 lose)\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 2: Monte Carlo ES Algorithm - Pseudocode and Implementation</h2>\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <img src=\"https://github.com/mdehghani86/RL_labs/blob/master/Lab%2005/MCM_ES.jpg?raw=true\" \n",
        "         alt=\"Monte Carlo ES Pseudocode\" \n",
        "         style=\"width: 70%; max-width: 800px; border: 2px solid #17a2b8; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
        "    <p style=\"color: #666; font-size: 12px; margin-top: 10px; font-style: italic;\">Figure 5.2: Monte Carlo ES Algorithm from Sutton and Barto</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0;\">Algorithm Overview</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0 0 10px 0; font-size: 13px;\">\n",
        "        Monte Carlo ES solves the exploration problem through <strong>Exploring Starts</strong>. Each episode begins with a \n",
        "        random state-action pair (S₀, A₀), then follows the current greedy policy. This ensures:\n",
        "    </p>\n",
        "    <ul style=\"color: #555; line-height: 1.6; margin: 0 0 10px 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>All state-action pairs are explored (random start)</li>\n",
        "        <li>Policy exploits current knowledge (greedy after start)</li>\n",
        "        <li>Convergence to optimal policy π* (GPI pattern)</li>\n",
        "    </ul>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        The algorithm alternates between <strong>policy evaluation</strong> (updating Q-values) and \n",
        "        <strong>policy improvement</strong> (making policy greedy), converging to optimality.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 2: Episode Generation with Exploring Starts\n",
        "\n",
        "PURPOSE:\n",
        "  - Generate complete Blackjack episodes using exploring starts mechanism\n",
        "  - First action is RANDOM (exploring start - ensures exploration)\n",
        "  - Subsequent actions follow GREEDY policy (exploitation)\n",
        "\n",
        "EXPLORING STARTS EXPLANATION:\n",
        "  Without exploring starts, a deterministic greedy policy might never try certain\n",
        "  actions in certain states. By randomly selecting the first action, we guarantee\n",
        "  that every state-action pair (s,a) is visited infinitely often as episodes → ∞.\n",
        "  This solves the exploration problem without needing epsilon-greedy or other mechanisms.\n",
        "\n",
        "ALGORITHM:\n",
        "  1. Reset environment → get initial state S₀\n",
        "  2. Select RANDOM first action A₀ (exploring start)\n",
        "  3. Execute A₀, observe R₁, S₁\n",
        "  4. Record (S₀, A₀, R₁)\n",
        "  5. For rest of episode:\n",
        "     a) Select action from current policy π (greedy)\n",
        "     b) Execute action, observe reward and next state\n",
        "     c) Record (state, action, reward)\n",
        "     d) Continue until termination\n",
        "\n",
        "PARAMETERS:\n",
        "  env: Gymnasium Blackjack environment\n",
        "  policy: Dictionary mapping states to actions (greedy policy)\n",
        "\n",
        "RETURNS:\n",
        "  episode: List of (state, action, reward) tuples representing complete episode\n",
        "\"\"\"\n",
        "\n",
        "def generate_episode_with_exploring_starts(env, policy):\n",
        "    episode = []\n",
        "    \n",
        "    # Initialize episode - get starting state\n",
        "    state, _ = env.reset()  # v1 API returns (state, info)\n",
        "    \n",
        "    # CRITICAL: EXPLORING START - select RANDOM first action\n",
        "    # This is the KEY innovation that ensures exploration\n",
        "    # Guarantees all (state, action) pairs are visited\n",
        "    action = env.action_space.sample()  # Uniform random: 0 or 1\n",
        "    \n",
        "    # Execute first action\n",
        "    # v1 API returns: (next_state, reward, terminated, truncated, info)\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated  # Episode ends on either condition\n",
        "    \n",
        "    # Record first step (with exploring start action)\n",
        "    episode.append((state, action, reward))\n",
        "    \n",
        "    # Continue episode following GREEDY policy\n",
        "    # From this point onward, we exploit our current knowledge\n",
        "    state = next_state\n",
        "    while not done:\n",
        "        # Get greedy action from current policy\n",
        "        # If state not in policy yet (early episodes), default to random\n",
        "        action = policy.get(state, env.action_space.sample())\n",
        "        \n",
        "        # Execute greedy action\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        # Record step\n",
        "        episode.append((state, action, reward))\n",
        "        \n",
        "        # Move to next state\n",
        "        state = next_state\n",
        "    \n",
        "    return episode\n",
        "\n",
        "pretty_print(\"Episode Generation Ready\",\n",
        "             \"<strong>Exploring Starts Mechanism:</strong><br>\" +\n",
        "             \"• First action: <strong>Random</strong> (exploration guarantee)<br>\" +\n",
        "             \"• Subsequent actions: <strong>Greedy</strong> (exploitation)<br>\" +\n",
        "             \"• Ensures all (s,a) pairs visited infinitely often<br>\" +\n",
        "             \"• No need for epsilon-greedy or other exploration\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 3: Monte Carlo ES - Main Learning Algorithm\n",
        "\n",
        "PURPOSE:\n",
        "  - Implement complete MC ES algorithm from Sutton and Barto Figure 5.2\n",
        "  - Learn optimal action-value function Q*(s,a)\n",
        "  - Extract optimal policy π*(s) = argmax_a Q*(s,a)\n",
        "\n",
        "ALGORITHM (Figure 5.2 - Line by Line):\n",
        "  Initialize:\n",
        "    - π(s) ∈ A(s) arbitrarily for all s ∈ S\n",
        "    - Q(s,a) ∈ ℝ arbitrarily for all s ∈ S, a ∈ A(s)\n",
        "    - Returns(s,a) ← empty list for all s ∈ S, a ∈ A(s)\n",
        "  \n",
        "  Loop forever (for each episode):\n",
        "    1. Choose S₀ ∈ S, A₀ ∈ A(S₀) randomly (exploring start)\n",
        "    2. Generate episode from S₀, A₀ following π\n",
        "    3. G ← 0\n",
        "    4. Loop for each step of episode, t = T-1, T-2, ..., 0:\n",
        "       a) G ← γG + R_{t+1}\n",
        "       b) Unless pair S_t, A_t appears earlier in episode:\n",
        "          - Append G to Returns(S_t, A_t)\n",
        "          - Q(S_t, A_t) ← average(Returns(S_t, A_t))\n",
        "          - π(S_t) ← argmax_a Q(S_t, a)\n",
        "\n",
        "KEY DATA STRUCTURES:\n",
        "  - Q: defaultdict(lambda: np.zeros(2))\n",
        "       Stores Q(s,a) estimates. Auto-initializes to [0, 0] for unseen states.\n",
        "       Q[state][0] = value of STICK, Q[state][1] = value of HIT\n",
        "  \n",
        "  - returns: defaultdict(list)\n",
        "       Stores all observed returns for each (s,a) pair.\n",
        "       returns[(state, action)] = [G1, G2, G3, ...]\n",
        "       Q(s,a) = mean of this list\n",
        "  \n",
        "  - policy: dict\n",
        "       Maps states to greedy actions.\n",
        "       policy[state] = argmax_a Q[state][a]\n",
        "\n",
        "PARAMETERS:\n",
        "  env: Blackjack environment\n",
        "  num_episodes: Number of episodes to run (default 500,000)\n",
        "\n",
        "RETURNS:\n",
        "  Q: Final action-value estimates\n",
        "  policy: Final greedy policy\n",
        "\"\"\"\n",
        "\n",
        "def monte_carlo_es(env, num_episodes=500000):\n",
        "    # Initialize Q(s,a) arbitrarily (to zeros)\n",
        "    # defaultdict automatically creates zero arrays for new states\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    # Initialize Returns(s,a) as empty lists\n",
        "    # Will store all observed returns for averaging\n",
        "    returns = defaultdict(list)\n",
        "    \n",
        "    # Initialize policy π arbitrarily (will become greedy)\n",
        "    policy = {}\n",
        "    \n",
        "    pretty_print(\"Starting Monte Carlo ES\",\n",
        "                 f\"<strong>Configuration:</strong><br>\" +\n",
        "                 f\"• Episodes: {num_episodes:,}<br>\" +\n",
        "                 \"• Method: First-Visit MC with Exploring Starts<br>\" +\n",
        "                 \"• Discount factor: γ = 1.0 (undiscounted)<br>\" +\n",
        "                 \"• Expected runtime: 2-3 minutes<br><br>\" +\n",
        "                 \"<strong>This implements Figure 5.2 exactly</strong>\",\n",
        "                 style='warning')\n",
        "    \n",
        "    # Main learning loop - iterate over episodes\n",
        "    for episode_num in range(1, num_episodes + 1):\n",
        "        # Generate episode using exploring starts\n",
        "        episode = generate_episode_with_exploring_starts(env, policy)\n",
        "        \n",
        "        # Track which (state, action) pairs we've already processed\n",
        "        # This implements the FIRST-VISIT rule\n",
        "        visited_state_actions = set()\n",
        "        \n",
        "        # Process episode BACKWARDS to efficiently calculate returns\n",
        "        # This is more efficient than calculating returns forward\n",
        "        # G accumulates reward from end of episode back to start\n",
        "        G = 0  # Return (undiscounted since gamma=1 for Blackjack)\n",
        "        \n",
        "        # Loop through episode backwards: T-1, T-2, ..., 0\n",
        "        for t in range(len(episode) - 1, -1, -1):\n",
        "            state, action, reward = episode[t]\n",
        "            \n",
        "            # Calculate return: G = r_{t+1} + γ*G\n",
        "            # Since γ=1 for Blackjack: G = reward + G\n",
        "            G = reward + G\n",
        "            \n",
        "            # Create hashable (state, action) tuple for tracking\n",
        "            state_action = (state, action)\n",
        "            \n",
        "            # FIRST-VISIT CHECK: only update if this is first occurrence\n",
        "            # This implements the \"Unless S_t,A_t appears earlier\" condition\n",
        "            if state_action not in visited_state_actions:\n",
        "                visited_state_actions.add(state_action)\n",
        "                \n",
        "                # Append G to Returns(S_t, A_t)\n",
        "                returns[state_action].append(G)\n",
        "                \n",
        "                # Q(S_t, A_t) ← average(Returns(S_t, A_t))\n",
        "                # This is POLICY EVALUATION step\n",
        "                Q[state][action] = np.mean(returns[state_action])\n",
        "                \n",
        "                # π(S_t) ← argmax_a Q(S_t, a)\n",
        "                # This is POLICY IMPROVEMENT step (make policy greedy)\n",
        "                # argmax returns index of maximum value (0 or 1)\n",
        "                policy[state] = np.argmax(Q[state])\n",
        "        \n",
        "        # Progress reporting every 100,000 episodes\n",
        "        if episode_num % 100000 == 0:\n",
        "            avg_q = np.mean([np.max(q) for q in Q.values()])\n",
        "            print(f\"Episode {episode_num:,}/{num_episodes:,} | Avg V(s): {avg_q:.3f}\")\n",
        "    \n",
        "    pretty_print(\"Monte Carlo ES Complete\",\n",
        "                 f\"<strong>Learning Results:</strong><br>\" +\n",
        "                 f\"• Processed: {num_episodes:,} episodes<br>\" +\n",
        "                 f\"• States learned: {len(Q):,} unique states<br>\" +\n",
        "                 f\"• Policy states: {len(policy):,}<br>\" +\n",
        "                 f\"• Q-values converged<br>\" +\n",
        "                 f\"• Policy is greedy w.r.t. Q\",\n",
        "                 style='success')\n",
        "    \n",
        "    return Q, policy\n",
        "\n",
        "pretty_print(\"MC ES Algorithm Loaded\",\n",
        "             \"<strong>Ready to learn optimal Blackjack policy</strong><br>\" +\n",
        "             \"Algorithm matches textbook Figure 5.2 line-by-line<br>\" +\n",
        "             \"Uses first-visit MC with exploring starts\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 3: Visualization Functions</h2>\n",
        "</div>\n",
        "\n",
        "We create two types of visualizations to understand the learned value function and policy:\n",
        "\n",
        "**1. 3D Surface Plots (Value Function):**\n",
        "- Display V*(s) = max_a Q(s,a) as a 3D surface\n",
        "- X-axis: Player sum (12-21)\n",
        "- Y-axis: Dealer showing card (1-10, where 1=Ace)\n",
        "- Z-axis (height) and color: State value\n",
        "- Blue (cold) = low value (likely to lose)\n",
        "- Red (warm) = high value (likely to win)\n",
        "- Separate plots for usable/non-usable ace\n",
        "\n",
        "**2. 2D Policy Heatmaps (Optimal Policy):**\n",
        "- Display π*(s) = argmax_a Q(s,a) as discrete color grid\n",
        "- Green = STICK (action 0)\n",
        "- Red = HIT (action 1)\n",
        "- Uses **pcolormesh** for discrete values (no interpolation)\n",
        "- Black gridlines separate cells\n",
        "- Separate heatmaps for usable/non-usable ace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 4: 3D Value Function Visualization\n",
        "\n",
        "PURPOSE:\n",
        "  - Create 3D surface plots of optimal state-value function V*(s)\n",
        "  - Show how value changes with player sum and dealer card\n",
        "  - Compare states with and without usable ace\n",
        "\n",
        "VALUE FUNCTION:\n",
        "  V*(s) = max_a Q(s,a)\n",
        "  This is the value of being in state s under the optimal policy\n",
        "\n",
        "VISUALIZATION DETAILS:\n",
        "  - Surface height and color represent V*(s)\n",
        "  - Colormap: coolwarm (blue→white→red)\n",
        "  - Value range: [-1, +1] (lose to win)\n",
        "  - Viewing angle: elevation=25°, azimuth=-130°\n",
        "  - Two subplots: one for each usable_ace condition\n",
        "\"\"\"\n",
        "\n",
        "def plot_value_function(Q, title=\"Optimal State-Value Function\"):\n",
        "    def get_Z(player_sum, dealer_card, usable_ace):\n",
        "        \"\"\"\n",
        "        Get optimal value V*(s) = max_a Q(s,a) for a state.\n",
        "        Returns 0 for unvisited states.\n",
        "        \"\"\"\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        if state in Q:\n",
        "            # V*(s) = max over actions\n",
        "            return np.max(Q[state])\n",
        "        return 0  # Default for unvisited states\n",
        "    \n",
        "    def create_surface(usable_ace, ax):\n",
        "        \"\"\"\n",
        "        Create 3D surface plot for given usable_ace condition.\n",
        "        \"\"\"\n",
        "        # Define state space ranges\n",
        "        player_range = np.arange(12, 22)  # 12 to 21\n",
        "        dealer_range = np.arange(1, 11)   # 1 (Ace) to 10\n",
        "        \n",
        "        # Create meshgrid for 3D plotting\n",
        "        # X[i,j] = player_range[j], Y[i,j] = dealer_range[i]\n",
        "        X, Y = np.meshgrid(player_range, dealer_range)\n",
        "        \n",
        "        # Build value array Z[i,j] = V*(player_range[j], dealer_range[i], usable_ace)\n",
        "        # Each row corresponds to a dealer card\n",
        "        # Each column corresponds to a player sum\n",
        "        Z = np.array([[get_Z(x, y, usable_ace) \n",
        "                      for x in player_range]  # Columns: player sums\n",
        "                     for y in dealer_range])  # Rows: dealer cards\n",
        "        \n",
        "        # Create 3D surface plot\n",
        "        surf = ax.plot_surface(\n",
        "            X, Y, Z,                # Coordinates and heights\n",
        "            cmap=cm.coolwarm,       # Blue (cold/bad) to Red (warm/good)\n",
        "            linewidth=0,            # No wireframe lines\n",
        "            antialiased=True,       # Smooth rendering\n",
        "            vmin=-1, vmax=1,        # Value range for consistent color mapping\n",
        "            alpha=0.8               # Slight transparency\n",
        "        )\n",
        "        \n",
        "        # Configure axes labels and limits\n",
        "        ax.set_xlabel('Player Sum', fontsize=11)\n",
        "        ax.set_ylabel('Dealer Showing', fontsize=11)\n",
        "        ax.set_zlabel('Value V*(s)', fontsize=11)\n",
        "        ax.set_zlim(-1, 1)  # Z-axis range\n",
        "        \n",
        "        # Set viewing angle for best perspective\n",
        "        ax.view_init(elev=25, azim=-130)\n",
        "        \n",
        "        return surf\n",
        "    \n",
        "    # Create figure with two 3D subplots (stacked vertically)\n",
        "    fig = plt.figure(figsize=(14, 11))\n",
        "    \n",
        "    # Subplot 1: With usable ace\n",
        "    ax1 = fig.add_subplot(211, projection='3d')\n",
        "    ax1.set_title(f'{title} - WITH Usable Ace', \n",
        "                  fontsize=13, fontweight='bold', pad=15)\n",
        "    surf1 = create_surface(True, ax1)\n",
        "    fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=10)\n",
        "    \n",
        "    # Subplot 2: Without usable ace\n",
        "    ax2 = fig.add_subplot(212, projection='3d')\n",
        "    ax2.set_title(f'{title} - WITHOUT Usable Ace', \n",
        "                  fontsize=13, fontweight='bold', pad=15)\n",
        "    surf2 = create_surface(False, ax2)\n",
        "    fig.colorbar(surf2, ax=ax2, shrink=0.5, aspect=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "pretty_print(\"3D Value Visualization Ready\",\n",
        "             \"<strong>Surface Plot Features:</strong><br>\" +\n",
        "             \"• Height = State value V*(s)<br>\" +\n",
        "             \"• Color: Blue (low) to Red (high)<br>\" +\n",
        "             \"• Two plots: with/without usable ace<br>\" +\n",
        "             \"• Smooth surface interpolation\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 5: 2D Policy Heatmap with DISCRETE Colors\n",
        "\n",
        "PURPOSE:\n",
        "  - Visualize optimal policy π*(s) = argmax_a Q(s,a) as 2D heatmap\n",
        "  - Show STICK vs HIT decisions for each state\n",
        "  - Use discrete colors with NO interpolation\n",
        "\n",
        "CRITICAL FIX:\n",
        "  This uses pcolormesh instead of imshow to ensure DISCRETE color values.\n",
        "  - pcolormesh: Each cell is solid color (no blending)\n",
        "  - imshow: Would interpolate between values (creates gradients)\n",
        "  For policy visualization, we need crisp boundaries between actions.\n",
        "\n",
        "COLOR CODING:\n",
        "  - Green = STICK (action 0) - stop drawing cards\n",
        "  - Red = HIT (action 1) - draw another card\n",
        "  - Colormap: RdYlGn_r (Red-Yellow-Green reversed)\n",
        "\n",
        "GRID STRUCTURE:\n",
        "  - Rows: Dealer showing card (1-10, displayed as A,2,3,...,10)\n",
        "  - Columns: Player sum (12-21)\n",
        "  - Black gridlines: Separate each state\n",
        "\"\"\"\n",
        "\n",
        "def plot_policy(policy, title=\"Optimal Policy\"):\n",
        "    def get_action(player_sum, dealer_card, usable_ace):\n",
        "        \"\"\"\n",
        "        Get optimal action π*(s) for a state.\n",
        "        Returns 1 (HIT) as default for unvisited states.\n",
        "        \"\"\"\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        return policy.get(state, 1)  # Default to HIT\n",
        "    \n",
        "    def create_heatmap(usable_ace, ax):\n",
        "        \"\"\"\n",
        "        Create discrete policy heatmap for given usable_ace condition.\n",
        "        \"\"\"\n",
        "        # Define state space ranges\n",
        "        player_range = np.arange(12, 22)  # 12-21\n",
        "        dealer_range = np.arange(1, 11)   # 1-10 (Ace to 10)\n",
        "        \n",
        "        # Build policy grid Z[i,j] = action\n",
        "        # Rows = dealer cards, Columns = player sums\n",
        "        Z = np.array([[get_action(player, dealer, usable_ace)\n",
        "                      for player in player_range]  # Columns\n",
        "                     for dealer in dealer_range])  # Rows\n",
        "        \n",
        "        # CRITICAL: Use pcolormesh for DISCRETE values\n",
        "        # This ensures no interpolation between action values\n",
        "        # shading='flat' means each cell gets a solid color\n",
        "        im = ax.pcolormesh(\n",
        "            player_range,           # X coordinates (player sums)\n",
        "            dealer_range,           # Y coordinates (dealer cards)\n",
        "            Z,                      # Action values (0 or 1)\n",
        "            cmap='RdYlGn_r',        # Colormap: Red=Hit(1), Green=Stick(0)\n",
        "            edgecolors='black',     # Black gridlines between cells\n",
        "            linewidth=0.5,          # Gridline thickness\n",
        "            vmin=0, vmax=1,         # Action value range [0,1]\n",
        "            shading='flat'          # No interpolation (discrete colors)\n",
        "        )\n",
        "        \n",
        "        # Configure axes ticks and labels\n",
        "        ax.set_xticks(player_range)\n",
        "        ax.set_yticks(dealer_range)\n",
        "        # Display 'A' for Ace (dealer_card=1), then 2,3,...,10\n",
        "        ax.set_yticklabels(['A'] + list(range(2, 11)))\n",
        "        \n",
        "        ax.set_xlabel('Player Sum', fontsize=11)\n",
        "        ax.set_ylabel('Dealer Showing', fontsize=11)\n",
        "        \n",
        "        # Set equal aspect ratio (square cells)\n",
        "        ax.set_aspect('equal')\n",
        "        \n",
        "        # Add colorbar with discrete action labels\n",
        "        # Ticks at 0.25 and 0.75 to center labels in color regions\n",
        "        cbar = plt.colorbar(im, ax=ax, ticks=[0.25, 0.75], \n",
        "                           fraction=0.046, pad=0.04)\n",
        "        cbar.ax.set_yticklabels(['STICK (0)', 'HIT (1)'])\n",
        "    \n",
        "    # Create figure with two subplots side by side\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Heatmap 1: With usable ace\n",
        "    ax1.set_title(f'{title} - WITH Usable Ace', \n",
        "                  fontsize=12, fontweight='bold')\n",
        "    create_heatmap(True, ax1)\n",
        "    \n",
        "    # Heatmap 2: Without usable ace\n",
        "    ax2.set_title(f'{title} - WITHOUT Usable Ace', \n",
        "                  fontsize=12, fontweight='bold')\n",
        "    create_heatmap(False, ax2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "pretty_print(\"2D Policy Visualization Ready\",\n",
        "             \"<strong>Discrete Policy Heatmaps:</strong><br>\" +\n",
        "             \"• Uses pcolormesh for crisp boundaries<br>\" +\n",
        "             \"• Green = STICK (0), Red = HIT (1)<br>\" +\n",
        "             \"• No color interpolation (discrete actions)<br>\" +\n",
        "             \"• Black gridlines separate cells<br>\" +\n",
        "             \"• Equal aspect ratio (square cells)\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 4: Run Monte Carlo ES Experiment</h2>\n",
        "</div>\n",
        "\n",
        "Now we execute the complete learning process. We run 500,000 episodes, which is sufficient for:\n",
        "1. All state-action pairs to be visited many times\n",
        "2. Q-value estimates to converge to true values Q*(s,a)\n",
        "3. The greedy policy to converge to the optimal policy π*(s)\n",
        "4. Results to match the textbook figures\n",
        "\n",
        "The learning process typically takes 2-3 minutes on modern hardware. We'll see progress updates every 100,000 episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 6: Execute Monte Carlo ES Learning\n",
        "\n",
        "PURPOSE:\n",
        "  - Run MC ES algorithm for 500,000 episodes\n",
        "  - Learn optimal Q-values and policy\n",
        "  - Analyze learned policy statistics\n",
        "\n",
        "EXPECTED BEHAVIOR:\n",
        "  - Policy should converge to optimal Blackjack strategy\n",
        "  - Stick more often at high sums (19-21)\n",
        "  - Hit more often at low sums (12-16)\n",
        "  - Different strategies with/without usable ace\n",
        "  - Results should match textbook Figure 5.2\n",
        "\"\"\"\n",
        "\n",
        "# Run Monte Carlo ES with 500,000 episodes\n",
        "Q, policy = monte_carlo_es(env, num_episodes=500000)\n",
        "\n",
        "# Analyze learned policy statistics\n",
        "# Count how many states have each action as optimal\n",
        "stick_count = sum(1 for action in policy.values() if action == 0)\n",
        "hit_count = sum(1 for action in policy.values() if action == 1)\n",
        "total_states = len(policy)\n",
        "\n",
        "# Calculate average value across all states\n",
        "avg_value = np.mean([np.max(q) for q in Q.values()])\n",
        "\n",
        "pretty_print(\"Learning Complete - Policy Analysis\",\n",
        "             f\"<strong>Policy Statistics:</strong><br>\" +\n",
        "             f\"• Total states in policy: {total_states}<br>\" +\n",
        "             f\"• Average state value: {avg_value:.3f}<br><br>\" +\n",
        "             f\"<strong>Action Distribution:</strong><br>\" +\n",
        "             f\"• STICK (action 0): {stick_count} states ({100*stick_count/total_states:.1f}%)<br>\" +\n",
        "             f\"• HIT (action 1): {hit_count} states ({100*hit_count/total_states:.1f}%)<br><br>\" +\n",
        "             f\"<strong>Expected Pattern:</strong><br>\" +\n",
        "             \"• More STICK at high player sums (20-21)<br>\" +\n",
        "             \"• More HIT at low player sums (12-16)<br>\" +\n",
        "             \"• Decision boundary around sum 17-19<br>\" +\n",
        "             \"• Different behavior with usable ace\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 7: Visualize Optimal Value Function\n",
        "\n",
        "PURPOSE:\n",
        "  - Display 3D surface plots of learned value function V*(s)\n",
        "  - Show how values vary across state space\n",
        "  - Compare usable vs non-usable ace scenarios\n",
        "\n",
        "INTERPRETATION GUIDE:\n",
        "  - Red peaks: Best states (high probability of winning)\n",
        "  - Blue valleys: Worst states (high probability of losing)\n",
        "  - Gradient: How value changes with player sum and dealer card\n",
        "\"\"\"\n",
        "\n",
        "pretty_print(\"Generating 3D Value Function Plots\",\n",
        "             \"Creating surface plots of V*(s) = max_a Q(s,a)<br>\" +\n",
        "             \"This may take a moment to render...\",\n",
        "             style='info')\n",
        "\n",
        "# Generate 3D surface plots\n",
        "plot_value_function(Q, \"Optimal State-Value Function V*\")\n",
        "\n",
        "pretty_print(\"Value Function Interpretation\",\n",
        "             \"<strong>Color Coding:</strong><br>\" +\n",
        "             \"• 🔴 Red (high): Favorable states, likely to win<br>\" +\n",
        "             \"• 🔵 Blue (low): Unfavorable states, likely to lose<br>\" +\n",
        "             \"• White: Neutral states<br><br>\" +\n",
        "             \"<strong>Key Observations:</strong><br>\" +\n",
        "             \"• Peak values near player sum 20-21 (close to winning)<br>\" +\n",
        "             \"• Lower values at low sums (far from 21)<br>\" +\n",
        "             \"• Usable ace provides higher values (flexibility to hit)<br>\" +\n",
        "             \"• Values vary with dealer showing card strength\",\n",
        "             style='note')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 8: Visualize Optimal Policy\n",
        "\n",
        "PURPOSE:\n",
        "  - Display 2D heatmaps of learned optimal policy π*(s)\n",
        "  - Show STICK vs HIT decisions with discrete colors\n",
        "  - Compare with textbook Figure 5.2 results\n",
        "\n",
        "INTERPRETATION GUIDE:\n",
        "  - Look for clear decision boundaries\n",
        "  - Policy should stick at high sums, hit at low sums\n",
        "  - Usable ace allows more aggressive hitting\n",
        "\"\"\"\n",
        "\n",
        "pretty_print(\"Generating Optimal Policy Heatmaps\",\n",
        "             \"Creating discrete policy visualizations<br>\" +\n",
        "             \"<strong>Green = STICK</strong>, <strong>Red = HIT</strong>\",\n",
        "             style='info')\n",
        "\n",
        "# Generate 2D policy heatmaps with discrete colors\n",
        "plot_policy(policy, \"Optimal Policy π* (from MC ES)\")\n",
        "\n",
        "pretty_print(\"Policy Interpretation\",\n",
        "             \"<strong>Policy Patterns (should match Figure 5.2):</strong><br>\" +\n",
        "             \"• Clear boundary around player sum 17-20<br>\" +\n",
        "             \"• STICK (green) dominates at high sums (20-21)<br>\" +\n",
        "             \"• HIT (red) dominates at low sums (12-16)<br>\" +\n",
        "             \"• Transition zone at middle sums (17-19)<br>\" +\n",
        "             \"• More aggressive with usable ace (cannot bust)<br>\" +\n",
        "             \"• Adapts to dealer showing card (weaker dealer = more stick)<br><br>\" +\n",
        "             \"<strong>✓ This should match Sutton and Barto Figure 5.2</strong>\",\n",
        "             style='note')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase;\">Key Findings and Insights</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>1. Exploring Starts Effectiveness:</strong> Random initial actions successfully ensured comprehensive exploration of all state-action pairs without requiring ongoing exploration mechanisms like epsilon-greedy. The guarantee that all (s,a) pairs are visited infinitely often leads to convergence.</p>\n",
        "        <p><strong>2. Policy Convergence:</strong> The greedy policy converged to the optimal policy, matching textbook results with clear decision boundaries. The policy shows rational behavior: stick at high sums to avoid busting, hit at low sums to improve hand.</p>\n",
        "        <p><strong>3. Usable Ace Impact:</strong> States with usable ace show higher values and more aggressive hitting strategy. The flexibility of counting ace as 1 or 11 prevents busting, allowing the player to take more risks.</p>\n",
        "        <p><strong>4. First-Visit MC Unbiased Estimation:</strong> By averaging returns only from first visits to each (s,a) pair, we obtained unbiased estimates that converged to true action values Q*(s,a) as the number of episodes increased.</p>\n",
        "        <p><strong>5. Generalized Policy Iteration Pattern:</strong> The interleaved pattern of policy evaluation (updating Q-values) and policy improvement (making policy greedy) is a key characteristic of GPI, which guarantees convergence to the optimal policy.</p>\n",
        "        <p><strong>6. Computational Efficiency:</strong> Despite running 500,000 episodes, the algorithm completed in 2-3 minutes due to efficient numpy operations and the simplicity of the Blackjack environment.</p>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>Why does exploring starts solve the exploration problem without needing epsilon-greedy?</li>\n",
        "        <li>How would the algorithm behave if we used every-visit MC instead of first-visit?</li>\n",
        "        <li>What would happen if we used a discount factor gamma less than 1.0 for Blackjack?</li>\n",
        "        <li>Could we implement MC ES in continuing (non-episodic) tasks? Why or why not?</li>\n",
        "        <li>How does the variance of Q-value estimates decrease as we collect more episodes?</li>\n",
        "        <li>Why is the decision boundary different between usable and non-usable ace states?</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center; border-radius: 8px;\">\n",
        "    <p style=\"margin: 0; font-size: 13px; font-weight: 600;\">End of Lab 5-1: Monte Carlo ES for Blackjack</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Next: Lab 5-2 - Off-Policy Monte Carlo with Importance Sampling</p>\n",
        "</div>"
      ]
    }
  ]
}
