{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_05_1_Blackjack_Monte_Carlo_ES.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 5-1: Blackjack with Monte Carlo ES\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton and Barto Chapter 5 Figure 5.2 | 75 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        This lab implements <strong>Monte Carlo ES (Exploring Starts)</strong> exactly as described in Sutton and Barto Figure 5.2. \n",
        "        The algorithm finds the optimal Blackjack policy without requiring a model of the environment. The key innovation is \n",
        "        <strong>Exploring Starts</strong>: each episode begins with a randomly selected state-action pair, guaranteeing that all \n",
        "        state-action pairs are visited infinitely often. After the initial random action, the agent follows its current greedy policy. \n",
        "        This combination ensures both exploration (through random starts) and exploitation (through greedy policy), leading to \n",
        "        convergence to the optimal policy.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Implement Monte Carlo ES from Figure 5.2</li>\n",
        "        <li>Understand exploring starts mechanism</li>\n",
        "        <li>Apply first-visit MC to action-value estimation</li>\n",
        "        <li>Implement greedy policy improvement</li>\n",
        "        <li>Reproduce textbook Blackjack results</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0;\">Blackjack Rules</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\">Goal: Sum close to 21 without exceeding</div>\n",
        "        <div style=\"padding: 2px 0;\">Actions: 0=Stick (stop), 1=Hit (draw)</div>\n",
        "        <div style=\"padding: 2px 0;\">State: (player_sum, dealer_card, usable_ace)</div>\n",
        "        <div style=\"padding: 2px 0;\">Rewards: +1 win, 0 draw, -1 lose</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 1: Environment Setup and Dependencies</h2>\n",
        "</div>\n",
        "\n",
        "We begin by importing necessary libraries and initializing the Blackjack environment. The key libraries are:\n",
        "- **Gymnasium**: Provides the Blackjack-v1 environment with proper episode handling\n",
        "- **NumPy**: For numerical computations and array operations\n",
        "- **Matplotlib**: For creating 3D value function plots and 2D policy heatmaps\n",
        "- **defaultdict**: For efficient sparse storage of Q-values and returns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 1: Import Libraries and Initialize Environment\n",
        "\n",
        "Purpose:\n",
        "  - Import required libraries for MC ES implementation\n",
        "  - Load pretty_print utility for formatted output\n",
        "  - Configure matplotlib for visualization\n",
        "  - Create Blackjack environment\n",
        "\n",
        "Key Components:\n",
        "  - gymnasium: Modern RL environment library (v1 API)\n",
        "  - defaultdict: Efficient storage for sparse Q-values\n",
        "  - matplotlib: 3D surface plots and 2D heatmaps\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure matplotlib for publication-quality figures\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Load pretty_print utility from GitHub\n",
        "try:\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/mdehghani86/RL_labs/master/utility/rl_utility.py'\n",
        "    response = requests.get(url)\n",
        "    exec(response.text)\n",
        "    pretty_print(\"Environment Ready\", \n",
        "                 f\"Gymnasium version: {gym.__version__}<br>\" +\n",
        "                 \"Implementing Monte Carlo ES from Figure 5.2<br>\" +\n",
        "                 \"All libraries loaded successfully\", \n",
        "                 style='success')\n",
        "except Exception as e:\n",
        "    print(f\"Libraries loaded (pretty_print unavailable: {e})\")\n",
        "\n",
        "# Create Blackjack environment (v1 uses modern API)\n",
        "env = gym.make('Blackjack-v1')\n",
        "\n",
        "pretty_print(\"Blackjack Environment Created\",\n",
        "             f\"Action space: {env.action_space.n} actions<br>\" +\n",
        "             \"0 = Stick (stop drawing), 1 = Hit (draw card)<br>\" +\n",
        "             \"State: (player_sum, dealer_showing, usable_ace)<br>\" +\n",
        "             \"Terminal rewards: +1 (win), 0 (draw), -1 (lose)\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 2: Monte Carlo ES Algorithm Implementation</h2>\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <img src=\"https://github.com/mdehghani86/RL_labs/blob/master/Lab%2005/MCM_ES.jpg?raw=true\" \n",
        "         alt=\"Monte Carlo ES Pseudocode\" \n",
        "         style=\"width: 70%; border: 2px solid #17a2b8; border-radius: 8px;\">\n",
        "    <p style=\"color: #666; font-size: 12px; margin-top: 10px;\">Figure 5.2: Monte Carlo ES from Sutton and Barto</p>\n",
        "</div>\n",
        "\n",
        "The Monte Carlo ES algorithm consists of three key components:\n",
        "\n",
        "1. **Episode Generation with Exploring Starts**: Each episode begins with a random action (exploring start), then follows the current greedy policy for all subsequent actions. This ensures comprehensive exploration.\n",
        "\n",
        "2. **First-Visit MC Update**: For each state-action pair visited in an episode, we calculate the return from the first visit and update Q-values as a running average.\n",
        "\n",
        "3. **Greedy Policy Improvement**: After updating Q-values, we immediately improve the policy by making it greedy with respect to the new Q-values: π(s) = argmax_a Q(s,a)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 2: Episode Generation with Exploring Starts\n",
        "\n",
        "Purpose:\n",
        "  - Generate episodes using exploring starts mechanism\n",
        "  - First action: RANDOM (ensures exploration)\n",
        "  - Subsequent actions: GREEDY (follows current policy)\n",
        "\n",
        "Algorithm:\n",
        "  1. Reset environment to get initial state\n",
        "  2. Select RANDOM first action (exploring start)\n",
        "  3. Execute first action and record (s, a, r)\n",
        "  4. For rest of episode: follow greedy policy\n",
        "  5. Continue until episode terminates\n",
        "\n",
        "Parameters:\n",
        "  env: Gymnasium Blackjack environment\n",
        "  policy: Dictionary mapping states to actions\n",
        "\n",
        "Returns:\n",
        "  episode: List of (state, action, reward) tuples\n",
        "\"\"\"\n",
        "\n",
        "def generate_episode_with_exploring_starts(env, policy):\n",
        "    episode = []\n",
        "    \n",
        "    # Initialize episode\n",
        "    state, _ = env.reset()  # v1 returns (state, info)\n",
        "    \n",
        "    # CRITICAL: Exploring start - select RANDOM first action\n",
        "    # This ensures all state-action pairs are explored\n",
        "    action = env.action_space.sample()  # Uniform random: 0 or 1\n",
        "    \n",
        "    # Execute first action\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated  # Episode ends on either\n",
        "    episode.append((state, action, reward))\n",
        "    \n",
        "    # Continue episode following GREEDY policy\n",
        "    state = next_state\n",
        "    while not done:\n",
        "        # Follow current policy (greedy w.r.t. Q)\n",
        "        # If state not in policy yet, default to random\n",
        "        action = policy.get(state, env.action_space.sample())\n",
        "        \n",
        "        # Execute action\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        # Record step\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    \n",
        "    return episode\n",
        "\n",
        "pretty_print(\"Episode Generation Ready\",\n",
        "             \"<strong>Exploring Starts Mechanism:</strong><br>\" +\n",
        "             \"• First action: Random (exploration)<br>\" +\n",
        "             \"• Subsequent actions: Greedy (exploitation)<br>\" +\n",
        "             \"• Guarantees all (s,a) pairs are visited\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 3: Monte Carlo ES - Main Learning Algorithm\n",
        "\n",
        "Purpose:\n",
        "  - Implement complete MC ES algorithm from Figure 5.2\n",
        "  - Learn optimal Q-values through episode sampling\n",
        "  - Extract optimal policy via greedy improvement\n",
        "\n",
        "Algorithm (Figure 5.2):\n",
        "  1. Initialize Q(s,a) arbitrarily for all s,a\n",
        "  2. Initialize Returns(s,a) as empty list for all s,a\n",
        "  3. Initialize policy π arbitrarily (will become greedy)\n",
        "  4. Loop for each episode:\n",
        "     a) Generate episode with exploring starts\n",
        "     b) For each (s,a) appearing in episode (first-visit):\n",
        "        - Calculate return G from first visit\n",
        "        - Append G to Returns(s,a)\n",
        "        - Update Q(s,a) as average of Returns(s,a)\n",
        "        - Update policy: π(s) = argmax_a Q(s,a)\n",
        "\n",
        "Key Data Structures:\n",
        "  - Q: defaultdict storing Q(s,a) estimates\n",
        "  - returns: defaultdict storing list of returns for each (s,a)\n",
        "  - policy: dict mapping states to greedy actions\n",
        "\n",
        "Parameters:\n",
        "  env: Blackjack environment\n",
        "  num_episodes: Number of episodes to run\n",
        "\n",
        "Returns:\n",
        "  Q: Final action-value estimates\n",
        "  policy: Final greedy policy\n",
        "\"\"\"\n",
        "\n",
        "def monte_carlo_es(env, num_episodes=500000):\n",
        "    # Initialize Q(s,a) arbitrarily\n",
        "    # defaultdict creates entries automatically with zero arrays\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    # Initialize Returns(s,a) as empty lists\n",
        "    # Will store all observed returns for averaging\n",
        "    returns = defaultdict(list)\n",
        "    \n",
        "    # Initialize policy (will become greedy)\n",
        "    policy = {}\n",
        "    \n",
        "    pretty_print(\"Starting Monte Carlo ES\",\n",
        "                 f\"Episodes: {num_episodes:,}<br>\" +\n",
        "                 \"Method: First-Visit MC with Exploring Starts<br>\" +\n",
        "                 \"This will take a few minutes...\",\n",
        "                 style='warning')\n",
        "    \n",
        "    # Main learning loop\n",
        "    for episode_num in range(1, num_episodes + 1):\n",
        "        # Generate episode using exploring starts\n",
        "        episode = generate_episode_with_exploring_starts(env, policy)\n",
        "        \n",
        "        # Track which state-action pairs we've seen (for first-visit)\n",
        "        visited_state_actions = set()\n",
        "        \n",
        "        # Process episode BACKWARDS to calculate returns efficiently\n",
        "        # G accumulates reward from end of episode back to start\n",
        "        G = 0  # Return (undiscounted, gamma=1 for Blackjack)\n",
        "        \n",
        "        for t in range(len(episode) - 1, -1, -1):\n",
        "            state, action, reward = episode[t]\n",
        "            \n",
        "            # Accumulate return: G = r_t + G (since gamma=1)\n",
        "            G = reward + G\n",
        "            \n",
        "            # Create state-action tuple for tracking\n",
        "            state_action = (state, action)\n",
        "            \n",
        "            # First-visit check: only update if this is first occurrence\n",
        "            if state_action not in visited_state_actions:\n",
        "                visited_state_actions.add(state_action)\n",
        "                \n",
        "                # Append return to list for this state-action pair\n",
        "                returns[state_action].append(G)\n",
        "                \n",
        "                # Update Q(s,a) as average of all observed returns\n",
        "                # Q(s,a) = mean(Returns(s,a))\n",
        "                Q[state][action] = np.mean(returns[state_action])\n",
        "                \n",
        "                # Policy improvement: make policy greedy w.r.t. Q\n",
        "                # π(s) = argmax_a Q(s,a)\n",
        "                policy[state] = np.argmax(Q[state])\n",
        "        \n",
        "        # Progress reporting\n",
        "        if episode_num % 100000 == 0:\n",
        "            print(f\"Episode {episode_num:,}/{num_episodes:,}\")\n",
        "    \n",
        "    pretty_print(\"Monte Carlo ES Complete\",\n",
        "                 f\"Processed {num_episodes:,} episodes<br>\" +\n",
        "                 f\"Learned Q-values for {len(Q)} states<br>\" +\n",
        "                 f\"Policy is greedy w.r.t. learned Q-values\",\n",
        "                 style='success')\n",
        "    \n",
        "    return Q, policy\n",
        "\n",
        "pretty_print(\"MC ES Algorithm Loaded\",\n",
        "             \"Ready to learn optimal Blackjack policy<br>\" +\n",
        "             \"Algorithm matches textbook Figure 5.2\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 3: Visualization Functions</h2>\n",
        "</div>\n",
        "\n",
        "We create two types of visualizations:\n",
        "\n",
        "**3D Surface Plots**: Show the optimal state-value function V*(s) = max_a Q(s,a) as a 3D surface. The x-axis is player sum, y-axis is dealer showing card, and z-axis (height/color) is the value. We create separate plots for states with and without a usable ace.\n",
        "\n",
        "**2D Policy Heatmaps**: Display the optimal action for each state using discrete colors. Green indicates STICK (action 0) and Red indicates HIT (action 1). Using pcolormesh ensures crisp boundaries between actions with no interpolation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 4: Create 3D Value Function Visualization\n",
        "\n",
        "Purpose:\n",
        "  - Plot optimal state-value function V*(s) = max_a Q(s,a)\n",
        "  - Show how value changes with player sum and dealer card\n",
        "  - Separate plots for usable/non-usable ace states\n",
        "\n",
        "Visualization Details:\n",
        "  - X-axis: Player sum (12-21)\n",
        "  - Y-axis: Dealer showing (1=Ace through 10)\n",
        "  - Z-axis/Color: State value (-1 to +1)\n",
        "  - Blue: Low value (likely to lose)\n",
        "  - Red: High value (likely to win)\n",
        "\"\"\"\n",
        "\n",
        "def plot_value_function(Q, title=\"Optimal Value Function\"):\n",
        "    def get_Z(player_sum, dealer_card, usable_ace):\n",
        "        \"\"\"Get optimal value V(s) = max_a Q(s,a) for state\"\"\"\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        if state in Q:\n",
        "            return np.max(Q[state])  # V*(s) = max over actions\n",
        "        return 0  # Default for unvisited states\n",
        "    \n",
        "    def create_surface(usable_ace, ax):\n",
        "        \"\"\"Create 3D surface plot for given usable_ace condition\"\"\"\n",
        "        # Define state space ranges\n",
        "        player_range = np.arange(12, 22)  # 12 to 21\n",
        "        dealer_range = np.arange(1, 11)   # 1 (Ace) to 10\n",
        "        \n",
        "        # Create coordinate meshgrid\n",
        "        X, Y = np.meshgrid(player_range, dealer_range)\n",
        "        \n",
        "        # Build value array: Z[i,j] = V(player_range[j], dealer_range[i], usable_ace)\n",
        "        Z = np.array([[get_Z(x, y, usable_ace) \n",
        "                      for x in player_range]  # Columns: player sums\n",
        "                     for y in dealer_range])  # Rows: dealer cards\n",
        "        \n",
        "        # Create 3D surface\n",
        "        surf = ax.plot_surface(\n",
        "            X, Y, Z,\n",
        "            cmap=cm.coolwarm,    # Blue (cold/bad) to Red (warm/good)\n",
        "            linewidth=0,\n",
        "            antialiased=True,\n",
        "            vmin=-1, vmax=1,     # Value range for color mapping\n",
        "            alpha=0.8\n",
        "        )\n",
        "        \n",
        "        # Configure axes\n",
        "        ax.set_xlabel('Player Sum')\n",
        "        ax.set_ylabel('Dealer Showing')\n",
        "        ax.set_zlabel('Value')\n",
        "        ax.set_zlim(-1, 1)\n",
        "        ax.view_init(elev=25, azim=-130)  # Set viewing angle\n",
        "        return surf\n",
        "    \n",
        "    # Create figure with two subplots\n",
        "    fig = plt.figure(figsize=(14, 10))\n",
        "    \n",
        "    # Plot 1: With usable ace\n",
        "    ax1 = fig.add_subplot(211, projection='3d')\n",
        "    ax1.set_title(f'{title} - Usable Ace', fontweight='bold', pad=15)\n",
        "    surf1 = create_surface(True, ax1)\n",
        "    fig.colorbar(surf1, ax=ax1, shrink=0.5)\n",
        "    \n",
        "    # Plot 2: Without usable ace\n",
        "    ax2 = fig.add_subplot(212, projection='3d')\n",
        "    ax2.set_title(f'{title} - No Usable Ace', fontweight='bold', pad=15)\n",
        "    surf2 = create_surface(False, ax2)\n",
        "    fig.colorbar(surf2, ax=ax2, shrink=0.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "pretty_print(\"3D Value Visualization Ready\",\n",
        "             \"Plots V*(s) = max_a Q(s,a) as 3D surface<br>\" +\n",
        "             \"Color: Blue (low) to Red (high)\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 5: Create 2D Policy Heatmap (DISCRETE COLORS)\n",
        "\n",
        "Purpose:\n",
        "  - Visualize optimal policy π*(s) = argmax_a Q(s,a)\n",
        "  - Show STICK vs HIT decisions for each state\n",
        "  - Use discrete colors (no blending)\n",
        "\n",
        "CRITICAL FIX:\n",
        "  - Uses pcolormesh (NOT imshow) for discrete values\n",
        "  - Ensures crisp boundaries between actions\n",
        "  - No interpolation between policy decisions\n",
        "\n",
        "Color Coding:\n",
        "  - Green = STICK (action 0)\n",
        "  - Red = HIT (action 1)\n",
        "\"\"\"\n",
        "\n",
        "def plot_policy(policy, title=\"Optimal Policy\"):\n",
        "    def get_action(player_sum, dealer_card, usable_ace):\n",
        "        \"\"\"Get optimal action for state\"\"\"\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        return policy.get(state, 1)  # Default to HIT if not in policy\n",
        "    \n",
        "    def create_heatmap(usable_ace, ax):\n",
        "        \"\"\"Create discrete policy heatmap\"\"\"\n",
        "        # Define state space\n",
        "        player_range = np.arange(12, 22)  # 12-21\n",
        "        dealer_range = np.arange(1, 11)   # 1-10 (Ace to 10)\n",
        "        \n",
        "        # Build policy grid: Z[i,j] = action\n",
        "        Z = np.array([[get_action(p, d, usable_ace)\n",
        "                      for p in player_range]  # Columns: player\n",
        "                     for d in dealer_range])  # Rows: dealer\n",
        "        \n",
        "        # CRITICAL: Use pcolormesh for DISCRETE values\n",
        "        # This prevents interpolation between actions\n",
        "        im = ax.pcolormesh(\n",
        "            player_range,           # X coordinates\n",
        "            dealer_range,           # Y coordinates\n",
        "            Z,                      # Action values (0 or 1)\n",
        "            cmap='RdYlGn_r',        # Red=Hit(1), Green=Stick(0)\n",
        "            edgecolors='black',     # Black gridlines\n",
        "            linewidth=0.5,\n",
        "            vmin=0, vmax=1,         # Discrete action range\n",
        "            shading='flat'          # No interpolation\n",
        "        )\n",
        "        \n",
        "        # Configure axes\n",
        "        ax.set_xticks(player_range)\n",
        "        ax.set_yticks(dealer_range)\n",
        "        ax.set_yticklabels(['A'] + list(range(2, 11)))  # A for Ace\n",
        "        ax.set_xlabel('Player Sum')\n",
        "        ax.set_ylabel('Dealer Showing')\n",
        "        ax.set_aspect('equal')  # Square cells\n",
        "        \n",
        "        # Add colorbar with discrete labels\n",
        "        cbar = plt.colorbar(im, ax=ax, ticks=[0.25, 0.75], \n",
        "                           fraction=0.046, pad=0.04)\n",
        "        cbar.ax.set_yticklabels(['STICK (0)', 'HIT (1)'])\n",
        "    \n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Heatmap 1: With usable ace\n",
        "    ax1.set_title(f'{title} - Usable Ace', fontweight='bold')\n",
        "    create_heatmap(True, ax1)\n",
        "    \n",
        "    # Heatmap 2: Without usable ace\n",
        "    ax2.set_title(f'{title} - No Usable Ace', fontweight='bold')\n",
        "    create_heatmap(False, ax2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "pretty_print(\"2D Policy Visualization Ready\",\n",
        "             \"<strong>Discrete policy heatmaps:</strong><br>\" +\n",
        "             \"• Uses pcolormesh for crisp boundaries<br>\" +\n",
        "             \"• Green = STICK, Red = HIT<br>\" +\n",
        "             \"• No interpolation between actions\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 4: Run Monte Carlo ES Experiment</h2>\n",
        "</div>\n",
        "\n",
        "Now we execute the complete learning process with 500,000 episodes. This large number of episodes is necessary to ensure:\n",
        "1. All state-action pairs are visited sufficiently often\n",
        "2. Q-value estimates converge to true values\n",
        "3. The policy converges to the optimal policy\n",
        "4. Results match the textbook figures\n",
        "\n",
        "The learning typically takes 2-3 minutes on modern hardware."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 6: Execute Monte Carlo ES Learning\n",
        "\n",
        "Purpose:\n",
        "  - Run MC ES for 500,000 episodes\n",
        "  - Learn optimal Q-values and policy\n",
        "  - Analyze learned policy statistics\n",
        "\n",
        "Expected Results:\n",
        "  - Policy should match textbook Figure 5.2\n",
        "  - Stick more often at higher player sums (17-21)\n",
        "  - Hit more often at lower player sums (12-16)\n",
        "  - Different behavior with/without usable ace\n",
        "\"\"\"\n",
        "\n",
        "# Run Monte Carlo ES\n",
        "Q, policy = monte_carlo_es(env, num_episodes=500000)\n",
        "\n",
        "# Analyze learned policy\n",
        "stick_count = sum(1 for action in policy.values() if action == 0)\n",
        "hit_count = sum(1 for action in policy.values() if action == 1)\n",
        "total_states = len(policy)\n",
        "\n",
        "pretty_print(\"Learning Complete - Policy Statistics\",\n",
        "             f\"<strong>States in learned policy:</strong> {total_states}<br><br>\" +\n",
        "             f\"<strong>Action Distribution:</strong><br>\" +\n",
        "             f\"• STICK (0): {stick_count} states ({100*stick_count/total_states:.1f}%)<br>\" +\n",
        "             f\"• HIT (1): {hit_count} states ({100*hit_count/total_states:.1f}%)<br><br>\" +\n",
        "             \"<strong>Expected Pattern:</strong><br>\" +\n",
        "             \"Policy should stick more at high sums (20-21)<br>\" +\n",
        "             \"Policy should hit more at low sums (12-16)\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 7: Visualize Optimal Value Function\n",
        "\n",
        "Purpose:\n",
        "  - Display 3D plots of learned value function\n",
        "  - Show how values change across state space\n",
        "  - Compare usable vs non-usable ace scenarios\n",
        "\"\"\"\n",
        "\n",
        "pretty_print(\"Generating 3D Value Function Plots\",\n",
        "             \"Creating surface plots of V*(s) = max_a Q(s,a)\",\n",
        "             style='info')\n",
        "\n",
        "plot_value_function(Q, \"Optimal State-Value Function V*\")\n",
        "\n",
        "pretty_print(\"Value Function Interpretation\",\n",
        "             \"<strong>Color Coding:</strong><br>\" +\n",
        "             \"• Red (high): Favorable states likely to win<br>\" +\n",
        "             \"• Blue (low): Unfavorable states likely to lose<br><br>\" +\n",
        "             \"<strong>Key Observations:</strong><br>\" +\n",
        "             \"• Peak values near player sum 20-21<br>\" +\n",
        "             \"• Usable ace provides higher values (flexibility)<br>\" +\n",
        "             \"• Values vary with dealer showing card\",\n",
        "             style='note')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 8: Visualize Optimal Policy\n",
        "\n",
        "Purpose:\n",
        "  - Display 2D heatmaps of learned policy\n",
        "  - Show optimal STICK vs HIT decisions\n",
        "  - Compare with textbook Figure 5.2\n",
        "\"\"\"\n",
        "\n",
        "pretty_print(\"Generating Policy Heatmaps\",\n",
        "             \"Creating discrete policy visualizations<br>\" +\n",
        "             \"Green = STICK, Red = HIT\",\n",
        "             style='info')\n",
        "\n",
        "plot_policy(policy, \"Optimal Policy π* (from MC ES)\")\n",
        "\n",
        "pretty_print(\"Policy Interpretation\",\n",
        "             \"<strong>Policy Patterns:</strong><br>\" +\n",
        "             \"• Clear boundary around sum 17-20<br>\" +\n",
        "             \"• STICK (green) dominates at high sums<br>\" +\n",
        "             \"• HIT (red) dominates at low sums<br>\" +\n",
        "             \"• More aggressive with usable ace (can't bust)<br>\" +\n",
        "             \"• Adapts to dealer showing card<br><br>\" +\n",
        "             \"<strong>This should match Sutton and Barto Figure 5.2</strong>\",\n",
        "             style='note')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0;\">Key Findings</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <strong>1. Exploring Starts Effectiveness:</strong> Random initial actions ensured comprehensive exploration of all state-action pairs, avoiding the need for ongoing exploration like epsilon-greedy.<br><br>\n",
        "        <strong>2. Policy Convergence:</strong> The greedy policy converged to the optimal policy, matching textbook results with clear decision boundaries around player sum 17-20.<br><br>\n",
        "        <strong>3. Usable Ace Impact:</strong> States with usable ace show higher values and more aggressive hitting strategy due to the flexibility of ace preventing busting.<br><br>\n",
        "        <strong>4. First-Visit MC:</strong> Averaging returns from first visits provided unbiased Q-value estimates that converged to true action values.<br><br>\n",
        "        <strong>5. Generalized Policy Iteration:</strong> The interleaved pattern of policy evaluation (Q-value updates) and policy improvement (greedy selection) led to optimal policy.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 5-1: Monte Carlo ES</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Next: Lab 5-2 - Off-Policy MC with Importance Sampling</p>\n",
        "</div>"
      ]
    }
  ]
}
