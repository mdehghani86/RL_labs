{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_05_1_Blackjack_MC_ES_Complete.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px;\">\n",
        "    <h1 style=\"margin: 0; font-size: 24px;\">Lab 5-1: Blackjack with Monte Carlo ES</h1>\n",
        "    <p style=\"font-size: 13px; margin: 6px 0 0 0;\">IE 7295 RL | Sutton and Barto Ch 5 Figure 5.2</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup\n",
        "\n",
        "Import all necessary libraries for MC ES implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CELL 1: Imports and Environment Setup\n",
        "# This cell loads all required libraries and creates the Blackjack environment\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Load pretty_print utility\n",
        "try:\n",
        "    import requests\n",
        "    exec(requests.get('https://raw.githubusercontent.com/mdehghani86/RL_labs/master/utility/rl_utility.py').text)\n",
        "    pretty_print(\"Ready\", \"All libraries loaded\", style='success')\n",
        "except:\n",
        "    print(\"Libraries loaded\")\n",
        "\n",
        "# Create Blackjack environment\n",
        "env = gym.make('Blackjack-v1')\n",
        "print(f\"Environment created: {env}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Episode Generation\n",
        "\n",
        "**CRITICAL**: This implements EXPLORING STARTS\n",
        "- First action = RANDOM (exploration)\n",
        "- Rest = GREEDY (exploitation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CELL 2: Episode Generation with Exploring Starts\n",
        "#\n",
        "# PURPOSE: Generate episodes using the exploring starts mechanism\n",
        "# \n",
        "# EXPLORING STARTS EXPLAINED:\n",
        "#   - First action: RANDOM - ensures all (s,a) pairs explored\n",
        "#   - Remaining actions: GREEDY - follow current policy\n",
        "#   - This guarantees sufficient exploration while exploiting learned knowledge\n",
        "#\n",
        "# PARAMETERS:\n",
        "#   env: Blackjack environment\n",
        "#   policy: Current greedy policy (dict mapping states to actions)\n",
        "#\n",
        "# RETURNS:\n",
        "#   episode: List of (state, action, reward) tuples\n",
        "\n",
        "def generate_episode_es(env, policy):\n",
        "    episode = []\n",
        "    state, _ = env.reset()\n",
        "    \n",
        "    # EXPLORING START: Select random first action\n",
        "    # This is THE KEY to exploring starts - guarantees exploration\n",
        "    action = env.action_space.sample()  # Random: 0 or 1\n",
        "    \n",
        "    # Execute first action\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    episode.append((state, action, reward))\n",
        "    \n",
        "    # Follow GREEDY policy for rest of episode\n",
        "    state = next_state\n",
        "    while not done:\n",
        "        # Get greedy action from policy (or random if state not seen yet)\n",
        "        action = policy.get(state, env.action_space.sample())\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    \n",
        "    return episode\n",
        "\n",
        "print(\"Episode generation ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: MC ES Algorithm\n",
        "\n",
        "Implements Figure 5.2 from textbook:\n",
        "1. Generate episode with exploring starts\n",
        "2. For each (s,a) - first visit only:\n",
        "   - Calculate return G\n",
        "   - Update Q(s,a) = average of returns\n",
        "3. Make policy greedy: π(s) = argmax Q(s,a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CELL 3: Monte Carlo ES Main Algorithm\n",
        "#\n",
        "# PURPOSE: Learn optimal Q-values and policy using MC ES\n",
        "#\n",
        "# ALGORITHM (Figure 5.2):\n",
        "#   1. Initialize Q(s,a) arbitrarily\n",
        "#   2. Initialize Returns(s,a) as empty lists\n",
        "#   3. Initialize policy π arbitrarily\n",
        "#   4. Loop for each episode:\n",
        "#      a) Generate episode with exploring starts\n",
        "#      b) For each (s,a) in episode (FIRST-VISIT only):\n",
        "#         - Calculate return G\n",
        "#         - Append G to Returns(s,a)\n",
        "#         - Q(s,a) = average(Returns(s,a))\n",
        "#         - π(s) = argmax_a Q(s,a)\n",
        "#\n",
        "# DATA STRUCTURES:\n",
        "#   Q: dict of arrays - Q[state][action] = value estimate\n",
        "#   returns: dict of lists - returns[(state,action)] = list of observed returns\n",
        "#   policy: dict - policy[state] = best action\n",
        "\n",
        "def monte_carlo_es(env, num_episodes=500000):\n",
        "    # Initialize Q-values (start at zero)\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    # Initialize returns storage\n",
        "    returns = defaultdict(list)\n",
        "    \n",
        "    # Initialize policy (will become greedy)\n",
        "    policy = {}\n",
        "    \n",
        "    print(f\"Starting MC ES: {num_episodes:,} episodes\")\n",
        "    \n",
        "    # Main learning loop\n",
        "    for ep_num in range(1, num_episodes + 1):\n",
        "        # Generate episode with exploring starts\n",
        "        episode = generate_episode_es(env, policy)\n",
        "        \n",
        "        # Track visited (s,a) pairs for first-visit check\n",
        "        visited = set()\n",
        "        \n",
        "        # Calculate returns backward through episode\n",
        "        G = 0  # Return (gamma=1 for Blackjack)\n",
        "        \n",
        "        # Process episode BACKWARD (makes return calculation easy)\n",
        "        for t in range(len(episode) - 1, -1, -1):\n",
        "            state, action, reward = episode[t]\n",
        "            G = reward + G  # Accumulate return\n",
        "            \n",
        "            sa = (state, action)\n",
        "            \n",
        "            # FIRST-VISIT CHECK: only update if first time seeing (s,a)\n",
        "            if sa not in visited:\n",
        "                visited.add(sa)\n",
        "                \n",
        "                # Store return\n",
        "                returns[sa].append(G)\n",
        "                \n",
        "                # Update Q as AVERAGE of all returns\n",
        "                Q[state][action] = np.mean(returns[sa])\n",
        "                \n",
        "                # POLICY IMPROVEMENT: Make greedy\n",
        "                policy[state] = np.argmax(Q[state])\n",
        "        \n",
        "        # Progress\n",
        "        if ep_num % 100000 == 0:\n",
        "            print(f\"Episode {ep_num:,}\")\n",
        "    \n",
        "    print(\"MC ES complete\")\n",
        "    return Q, policy\n",
        "\n",
        "print(\"MC ES algorithm loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CELL 4: Visualization Functions\n",
        "#\n",
        "# Two visualization types:\n",
        "#   1. 3D surface: V(s) = max Q(s,a)\n",
        "#   2. 2D heatmap: π(s) with DISCRETE colors\n",
        "\n",
        "def plot_value(Q):\n",
        "    def get_v(ps, dc, ua):\n",
        "        return np.max(Q[(ps,dc,ua)]) if (ps,dc,ua) in Q else 0\n",
        "    \n",
        "    def surf(ua, ax):\n",
        "        pr, dr = np.arange(12,22), np.arange(1,11)\n",
        "        X, Y = np.meshgrid(pr, dr)\n",
        "        Z = np.array([[get_v(x,y,ua) for x in pr] for y in dr])\n",
        "        s = ax.plot_surface(X,Y,Z,cmap=cm.coolwarm,vmin=-1,vmax=1,alpha=0.8)\n",
        "        ax.set_xlabel('Player'); ax.set_ylabel('Dealer'); ax.set_zlabel('Value')\n",
        "        ax.view_init(25,-130)\n",
        "        return s\n",
        "    \n",
        "    fig = plt.figure(figsize=(14,10))\n",
        "    ax1 = fig.add_subplot(211, projection='3d')\n",
        "    ax1.set_title('With Ace')\n",
        "    fig.colorbar(surf(True,ax1), ax=ax1, shrink=0.5)\n",
        "    ax2 = fig.add_subplot(212, projection='3d')\n",
        "    ax2.set_title('No Ace')\n",
        "    fig.colorbar(surf(False,ax2), ax=ax2, shrink=0.5)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_policy(pol):\n",
        "    # CRITICAL: Uses pcolormesh for DISCRETE colors (no blending)\n",
        "    def get_a(ps, dc, ua):\n",
        "        return pol.get((ps,dc,ua), 1)\n",
        "    \n",
        "    def hm(ua, ax):\n",
        "        pr, dr = np.arange(12,22), np.arange(1,11)\n",
        "        Z = np.array([[get_a(p,d,ua) for p in pr] for d in dr])\n",
        "        # pcolormesh = discrete, no interpolation\n",
        "        im = ax.pcolormesh(pr,dr,Z,cmap='RdYlGn_r',edgecolors='black',\n",
        "                          linewidth=0.5,vmin=0,vmax=1,shading='flat')\n",
        "        ax.set_xticks(pr); ax.set_yticks(dr)\n",
        "        ax.set_yticklabels(['A']+list(range(2,11)))\n",
        "        ax.set_xlabel('Player'); ax.set_ylabel('Dealer')\n",
        "        ax.set_aspect('equal')\n",
        "        cb = plt.colorbar(im,ax=ax,ticks=[0.25,0.75])\n",
        "        cb.ax.set_yticklabels(['STICK','HIT'])\n",
        "    \n",
        "    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5))\n",
        "    ax1.set_title('With Ace'); hm(True,ax1)\n",
        "    ax2.set_title('No Ace'); hm(False,ax2)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "print(\"Viz functions ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CELL 5: Run MC ES\n",
        "Q, policy = monte_carlo_es(env, 500000)\n",
        "\n",
        "# Stats\n",
        "stick = sum(1 for a in policy.values() if a==0)\n",
        "total = len(policy)\n",
        "print(f\"\\nPolicy: {total} states, {stick} STICK ({100*stick/total:.1f}%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CELL 6: Visualize Results\n",
        "print(\"Value function:\")\n",
        "plot_value(Q)\n",
        "print(\"\\nPolicy (should match Figure 5.2):\")\n",
        "plot_policy(policy)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
