{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_05_1_Blackjack_Monte_Carlo_ES.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 5-1: Blackjack with Monte Carlo ES\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapter 5 | Figure 5.2 | 75 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        This lab implements <strong>Monte Carlo ES (Exploring Starts)</strong> exactly as described in Sutton & Barto Figure 5.2. \n",
        "        MC ES finds the optimal policy without requiring a model of the environment. The key insight is <strong>Exploring Starts</strong>: \n",
        "        each episode begins with a random state-action pair, ensuring all pairs are explored. After the initial random selection, \n",
        "        the agent follows the current greedy policy. This guarantees both exploration and convergence to optimality.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Implement Monte Carlo ES algorithm (Figure 5.2)</li>\n",
        "        <li>Understand exploring starts mechanism</li>\n",
        "        <li>Learn Q-values through episode sampling</li>\n",
        "        <li>Extract optimal policy via greedy improvement</li>\n",
        "        <li>Reproduce textbook results exactly</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Blackjack Rules</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Goal</code> → Get sum close to 21 without exceeding</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Actions</code> → Hit (1) or Stick (0)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">States</code> → (player_sum, dealer_card, usable_ace)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Rewards</code> → +1 (win), 0 (draw), -1 (lose)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Ace</code> → Can be 1 or 11 (usable if 11)</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 1: Environment Setup</h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 1: Import Libraries and Setup\n",
        "\"\"\"\n",
        "import sys\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "try:\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/mdehghani86/RL_labs/master/utility/rl_utility.py'\n",
        "    response = requests.get(url)\n",
        "    exec(response.text)\n",
        "    pretty_print(\"Environment Ready\", \n",
        "                 f\"Gymnasium version: {gym.__version__}<br>\" +\n",
        "                 \"Ready to implement MC ES from Figure 5.2\", \n",
        "                 style='success')\n",
        "except:\n",
        "    print(f\"Gymnasium version: {gym.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 2: Create Blackjack Environment\n",
        "\"\"\"\n",
        "env = gym.make('Blackjack-v1')\n",
        "\n",
        "pretty_print(\"Blackjack Environment\",\n",
        "             f\"Actions: 0=Stick, 1=Hit<br>\" +\n",
        "             \"State: (player_sum, dealer_card, usable_ace)<br>\" +
        "             \"Rewards: +1 (win), 0 (draw), -1 (lose)\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 2: Monte Carlo ES Algorithm (Figure 5.2)</h2>\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <img src=\"https://github.com/mdehghani86/RL_labs/blob/master/Lab%2005/MCM_ES.jpg?raw=true\" \n",
        "         alt=\"Monte Carlo ES Pseudocode\" \n",
        "         style=\"width: 70%; border: 2px solid #17a2b8; border-radius: 8px;\">\n",
        "    <p style=\"color: #666; font-size: 12px; margin-top: 10px;\">Figure 5.2: Monte Carlo ES from Sutton & Barto</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 3: Generate Episode with Exploring Starts\n",
        "\n",
        "CRITICAL: This implements EXPLORING STARTS exactly as in textbook\n",
        "  - First action is RANDOM (exploring start)\n",
        "  - Subsequent actions follow current GREEDY policy\n",
        "\"\"\"\n",
        "\n",
        "def generate_episode_with_exploring_starts(env, policy):\n",
        "    episode = []\n",
        "    \n",
        "    # Start episode\n",
        "    state, _ = env.reset()\n",
        "    \n",
        "    # EXPLORING STARTS: Choose random first action\n",
        "    action = env.action_space.sample()  # Random action (0 or 1)\n",
        "    \n",
        "    # Execute first action\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    episode.append((state, action, reward))\n",
        "    \n",
        "    # Continue episode following GREEDY policy\n",
        "    state = next_state\n",
        "    while not done:\n",
        "        # Follow current policy (greedy w.r.t. Q)\n",
        "        action = policy.get(state, env.action_space.sample())\n",
        "        \n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    \n",
        "    return episode\n",
        "\n",
        "pretty_print(\"Episode Generation Ready\",\n",
        "             \"Implements exploring starts mechanism<br>\" +\n",
        "             \"First action: Random (exploration)<br>\" +\n",
        "             \"Rest: Greedy policy (exploitation)\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 4: Monte Carlo ES - Main Algorithm\n",
        "\n",
        "This implements Figure 5.2 from Sutton & Barto EXACTLY:\n",
        "  1. Initialize Q and policy arbitrarily\n",
        "  2. For each episode:\n",
        "     a) Generate episode with exploring starts\n",
        "     b) For each (s,a) in episode (first-visit):\n",
        "        - Calculate return G\n",
        "        - Update Q(s,a) as average of returns\n",
        "        - Update policy to be greedy: π(s) = argmax Q(s,a)\n",
        "\"\"\"\n",
        "\n",
        "def monte_carlo_es(env, num_episodes=500000):\n",
        "    # Initialize Q(s,a) arbitrarily\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    # Initialize returns(s,a) - empty list for each state-action\n",
        "    returns = defaultdict(list)\n",
        "    \n",
        "    # Initialize policy arbitrarily (will become greedy)\n",
        "    policy = {}\n",
        "    \n",
        "    pretty_print(\"Starting Monte Carlo ES\",\n",
        "                 f\"Episodes: {num_episodes:,}<br>\" +\n",
        "                 \"Algorithm: Figure 5.2 from Sutton & Barto<br>\" +\n",
        "                 \"This will take a few minutes...\",\n",
        "                 style='warning')\n",
        "    \n",
        "    for episode_num in range(1, num_episodes + 1):\n",
        "        # Generate episode using exploring starts\n",
        "        episode = generate_episode_with_exploring_starts(env, policy)\n",
        "        \n",
        "        # Track visited state-action pairs (for first-visit)\n",
        "        visited_state_actions = set()\n",
        "        \n",
        "        # Process episode backwards to calculate returns\n",
        "        G = 0  # Return\n",
        "        \n",
        "        for t in range(len(episode) - 1, -1, -1):\n",
        "            state, action, reward = episode[t]\n",
        "            G = reward + G  # Undiscounted (gamma=1)\n",
        "            \n",
        "            state_action = (state, action)\n",
        "            \n",
        "            # First-visit check\n",
        "            if state_action not in visited_state_actions:\n",
        "                visited_state_actions.add(state_action)\n",
        "                \n",
        "                # Append return to list\n",
        "                returns[state_action].append(G)\n",
        "                \n",
        "                # Update Q as average of returns\n",
        "                Q[state][action] = np.mean(returns[state_action])\n",
        "                \n",
        "                # Policy improvement: make policy greedy\n",
        "                policy[state] = np.argmax(Q[state])\n",
        "        \n",
        "        if episode_num % 100000 == 0:\n",
        "            print(f\"Episode {episode_num:,}/{num_episodes:,}\")\n",
        "    \n",
        "    pretty_print(\"Monte Carlo ES Complete\",\n",
        "                 f\"Processed {num_episodes:,} episodes<br>\" +\n",
        "                 f\"Learned Q-values for {len(Q)} states<br>\" +\n",
        "                 \"Optimal policy extracted\",\n",
        "                 style='success')\n",
        "    \n",
        "    return Q, policy\n",
        "\n",
        "pretty_print(\"MC ES Algorithm Ready\",\n",
        "             \"Implements textbook algorithm exactly\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 3: Visualization Functions</h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 5: Visualization Functions\n",
        "\"\"\"\n",
        "\n",
        "def plot_value_function(Q, title=\"State-Value Function\"):\n",
        "    def get_Z(player_sum, dealer_card, usable_ace):\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        if state in Q:\n",
        "            return np.max(Q[state])  # V(s) = max_a Q(s,a)\n",
        "        return 0\n",
        "    \n",
        "    def create_surface(usable_ace, ax):\n",
        "        player_range = np.arange(12, 22)\n",
        "        dealer_range = np.arange(1, 11)\n",
        "        X, Y = np.meshgrid(player_range, dealer_range)\n",
        "        Z = np.array([[get_Z(x, y, usable_ace) for x in player_range] for y in dealer_range])\n",
        "        \n",
        "        surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0,\n",
        "                               antialiased=True, vmin=-1, vmax=1, alpha=0.8)\n",
        "        ax.set_xlabel('Player Sum')\n",
        "        ax.set_ylabel('Dealer Showing')\n",
        "        ax.set_zlabel('Value')\n",
        "        ax.set_zlim(-1, 1)\n",
        "        ax.view_init(elev=25, azim=-130)\n",
        "        return surf\n",
        "    \n",
        "    fig = plt.figure(figsize=(14, 11))\n",
        "    ax1 = fig.add_subplot(211, projection='3d')\n",
        "    ax1.set_title(f'{title} - Usable Ace', fontweight='bold', pad=15)\n",
        "    surf1 = create_surface(True, ax1)\n",
        "    fig.colorbar(surf1, ax=ax1, shrink=0.5)\n",
        "    \n",
        "    ax2 = fig.add_subplot(212, projection='3d')\n",
        "    ax2.set_title(f'{title} - No Usable Ace', fontweight='bold', pad=15)\n",
        "    surf2 = create_surface(False, ax2)\n",
        "    fig.colorbar(surf2, ax=ax2, shrink=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_policy(policy, title=\"Optimal Policy\"):\n",
        "    def get_action(player_sum, dealer_card, usable_ace):\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        return policy.get(state, 1)\n",
        "    \n",
        "    def create_heatmap(usable_ace, ax):\n",
        "        player_range = np.arange(12, 22)\n",
        "        dealer_range = np.arange(1, 11)\n",
        "        Z = np.array([[get_action(p, d, usable_ace) for p in player_range] for d in dealer_range])\n",
        "        \n",
        "        im = ax.pcolormesh(player_range, dealer_range, Z, cmap='RdYlGn_r',\n",
        "                          edgecolors='black', linewidth=0.5, vmin=0, vmax=1, shading='flat')\n",
        "        ax.set_xticks(player_range)\n",
        "        ax.set_yticks(dealer_range)\n",
        "        ax.set_yticklabels(['A'] + list(range(2, 11)))\n",
        "        ax.set_xlabel('Player Sum')\n",
        "        ax.set_ylabel('Dealer Showing')\n",
        "        ax.set_aspect('equal')\n",
        "        cbar = plt.colorbar(im, ax=ax, ticks=[0.25, 0.75], fraction=0.046, pad=0.04)\n",
        "        cbar.ax.set_yticklabels(['STICK (0)', 'HIT (1)'])\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    ax1.set_title(f'{title} - Usable Ace', fontweight='bold')\n",
        "    create_heatmap(True, ax1)\n",
        "    ax2.set_title(f'{title} - No Usable Ace', fontweight='bold')\n",
        "    create_heatmap(False, ax2)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "pretty_print(\"Visualization Functions Ready\",\n",
        "             \"3D surfaces for values<br>\" +\n",
        "             \"2D heatmaps for policy\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 4: Run Experiments</h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 6: Run Monte Carlo ES\n",
        "\"\"\"\n",
        "Q, policy = monte_carlo_es(env, num_episodes=500000)\n",
        "\n",
        "# Analyze results\n",
        "stick_count = sum(1 for a in policy.values() if a == 0)\n",
        "hit_count = sum(1 for a in policy.values() if a == 1)\n",
        "total = len(policy)\n",
        "\n",
        "pretty_print(\"Learning Complete\",\n",
        "             f\"States in policy: {total}<br>\" +\n",
        "             f\"STICK actions: {stick_count} ({100*stick_count/total:.1f}%)<br>\" +\n",
        "             f\"HIT actions: {hit_count} ({100*hit_count/total:.1f}%)\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 7: Visualize Results\n",
        "\"\"\"\n",
        "pretty_print(\"Generating Visualizations\",\n",
        "             \"Creating value function and policy plots...\",\n",
        "             style='info')\n",
        "\n",
        "plot_value_function(Q, \"Optimal State-Value Function V*\")\n",
        "plot_policy(policy, \"Optimal Policy π*\")\n",
        "\n",
        "pretty_print(\"Results Match Textbook\",\n",
        "             \"Policy should match Figure 5.2 from Sutton & Barto<br>\" +\n",
        "             \"Stick on 20-21, Hit on lower sums<br>\" +\n",
        "             \"Different behavior with/without usable ace\",\n",
        "             style='note')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0;\">Key Points</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <strong>1. Exploring Starts:</strong> Random initial action ensures all state-action pairs are explored.<br>\n",
        "        <strong>2. Policy Improvement:</strong> Policy becomes greedy after each episode (GPI pattern).<br>\n",
        "        <strong>3. Convergence:</strong> Converges to optimal policy π* matching textbook results.<br>\n",
        "        <strong>4. First-Visit MC:</strong> Only first occurrence of (s,a) in episode is used for updates.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 5-1: Monte Carlo ES</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Next: Lab 5-2 - Off-Policy MC with Importance Sampling</p>\n",
        "</div>"
      ]
    }
  ]
}
