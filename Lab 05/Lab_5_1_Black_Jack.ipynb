{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_05_1_Blackjack_Monte_Carlo_ES.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 5-1: Blackjack with Monte Carlo ES\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton and Barto Chapter 5 Figure 5.2 | 75 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        This lab implements <strong>Monte Carlo ES (Exploring Starts)</strong> exactly as described in Sutton and Barto Figure 5.2. \n",
        "        MC ES learns the optimal Blackjack policy without requiring a model of the environment. The key mechanism is \n",
        "        <strong>Exploring Starts</strong>: each episode begins with a randomly selected state-action pair, guaranteeing that \n",
        "        all state-action pairs are explored infinitely often. After the random first action, the agent follows its current \n",
        "        greedy policy. This ensures both exploration and convergence to optimality.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Implement Monte Carlo ES from Figure 5.2</li>\n",
        "        <li>Understand exploring starts mechanism</li>\n",
        "        <li>Apply first-visit MC for Q-value estimation</li>\n",
        "        <li>Implement greedy policy improvement</li>\n",
        "        <li>Reproduce textbook Blackjack results</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Blackjack Rules</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Goal</code> → Get sum close to 21 without exceeding</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Actions</code> → 0=Stick (stop), 1=Hit (draw card)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">States</code> → (player_sum, dealer_card, usable_ace)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Rewards</code> → +1 (win), 0 (draw), -1 (lose)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Ace</code> → Can be 1 or 11 (usable if 11)</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 1: Environment Setup and Dependencies</h2>\n",
        "</div>\n",
        "\n",
        "We begin by importing all necessary libraries:\n",
        "- **Gymnasium**: Modern RL environment library (replaces OpenAI Gym)\n",
        "- **NumPy**: Numerical computations and array operations\n",
        "- **Matplotlib**: 3D surface plots and 2D heatmaps\n",
        "- **defaultdict**: Efficient sparse storage for Q-values and returns\n",
        "- **pretty_print**: Custom utility for formatted output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 1: Import Libraries and Initialize Environment\n",
        "\n",
        "Purpose:\n",
        "  - Import all required libraries for MC ES implementation\n",
        "  - Load pretty_print utility from GitHub for formatted output\n",
        "  - Configure matplotlib for publication-quality visualizations\n",
        "  - Create Blackjack-v1 environment\n",
        "\n",
        "Key Libraries:\n",
        "  - gymnasium: Provides Blackjack-v1 environment with v1 API\n",
        "  - numpy: Array operations, random sampling, statistical functions\n",
        "  - defaultdict: Automatically initializes missing keys with default values\n",
        "  - matplotlib: 3D plotting (Axes3D), colormaps (cm), pyplot interface\n",
        "\n",
        "Environment Details:\n",
        "  - Blackjack-v1 uses modern Gymnasium API\n",
        "  - State: tuple (player_sum, dealer_card, usable_ace)\n",
        "  - Action: 0 (Stick) or 1 (Hit)\n",
        "  - Reward: Terminal only (+1 win, 0 draw, -1 lose)\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import gymnasium as gym  # Modern replacement for gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure matplotlib for better quality figures\n",
        "plt.rcParams['figure.dpi'] = 100          # Display resolution\n",
        "plt.rcParams['figure.figsize'] = (12, 8)  # Default figure size\n",
        "plt.rcParams['font.size'] = 10            # Font size for labels\n",
        "\n",
        "# Load pretty_print utility from GitHub repository\n",
        "try:\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/mdehghani86/RL_labs/master/utility/rl_utility.py'\n",
        "    response = requests.get(url)\n",
        "    exec(response.text)  # Execute utility code\n",
        "    pretty_print(\"Environment Setup Complete\", \n",
        "                 f\"Gymnasium version: {gym.__version__}<br>\" +\n",
        "                 \"NumPy, Matplotlib loaded successfully<br>\" +\n",
        "                 \"Ready to implement Monte Carlo ES\", \n",
        "                 style='success')\n",
        "except Exception as e:\n",
        "    # Fallback if GitHub fetch fails\n",
        "    print(f\"Libraries loaded (pretty_print unavailable: {str(e)})\")\n",
        "    print(f\"Gymnasium version: {gym.__version__}\")\n",
        "\n",
        "# Create Blackjack environment using v1 API\n",
        "env = gym.make('Blackjack-v1')  # v1 is current, v0 deprecated\n",
        "\n",
        "# Display environment information\n",
        "pretty_print(\"Blackjack Environment Created\",\n",
        "             f\"<strong>Action Space:</strong> {env.action_space.n} actions<br>\" +\n",
        "             \"• Action 0: Stick (stop drawing cards)<br>\" +\n",
        "             \"• Action 1: Hit (draw another card)<br><br>\" +\n",
        "             \"<strong>State Space:</strong><br>\" +\n",
        "             \"• player_sum: Current sum of player cards (12-21)<br>\" +\n",
        "             \"• dealer_card: Dealer's visible card (1-10, Ace=1)<br>\" +\n",
        "             \"• usable_ace: Boolean, True if ace counts as 11<br><br>\" +\n",
        "             \"<strong>Reward Structure:</strong><br>\" +\n",
        "             \"• +1 for winning, 0 for draw, -1 for losing<br>\" +\n",
        "             \"• Rewards given only at episode termination\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 2: Monte Carlo ES Algorithm - Pseudocode</h2>\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <img src=\"https://github.com/mdehghani86/RL_labs/blob/master/Lab%2005/MCM_ES.jpg?raw=true\" \n",
        "         alt=\"Monte Carlo ES Pseudocode\" \n",
        "         style=\"width: 70%; max-width: 800px; border: 2px solid #17a2b8; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
        "    <p style=\"color: #666; font-size: 12px; margin-top: 10px; font-style: italic;\">Figure 5.2: Monte Carlo ES Algorithm from Sutton and Barto</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #e8f5e9; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #4caf50;\">\n",
        "    <h3 style=\"color: #2e7d32; font-size: 14px; margin: 0 0 8px 0;\">Algorithm Components</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        <strong>1. Exploring Starts:</strong> Each episode begins with a random state-action pair, ensuring all pairs are explored.<br><br>\n",
        "        <strong>2. First-Visit MC:</strong> For each (s,a) pair, only the first occurrence in an episode is used for Q-value updates.<br><br>\n",
        "        <strong>3. Greedy Policy Improvement:</strong> After each episode, policy becomes greedy: π(s) ← argmax_a Q(s,a).<br><br>\n",
        "        <strong>4. Running Average:</strong> Q(s,a) is updated as the average of all observed returns from (s,a).\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 3: Episode Generation with Exploring Starts</h2>\n",
        "</div>\n",
        "\n",
        "The exploring starts mechanism is THE KEY innovation of MC ES. Without it, a deterministic greedy policy would never explore alternative actions. By starting each episode with a random action, we guarantee that:\n",
        "1. All state-action pairs are visited infinitely often\n",
        "2. We can still follow the greedy policy for most of the episode\n",
        "3. Exploration happens naturally without ongoing ε-greedy behavior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 2: Generate Episode with Exploring Starts\n",
        "\n",
        "Purpose:\n",
        "  - Generate complete Blackjack episodes using exploring starts mechanism\n",
        "  - First action: RANDOM (ensures exploration of all state-action pairs)\n",
        "  - Subsequent actions: GREEDY (follows current policy for exploitation)\n",
        "\n",
        "Algorithm:\n",
        "  1. Reset environment to get initial state\n",
        "  2. Select RANDOM first action (exploring start)\n",
        "  3. Execute first action and record (state, action, reward)\n",
        "  4. For remaining steps:\n",
        "     a) Select action using greedy policy\n",
        "     b) Execute action\n",
        "     c) Record (state, action, reward)\n",
        "     d) Continue until episode terminates\n",
        "\n",
        "Parameters:\n",
        "  env: Gymnasium Blackjack-v1 environment\n",
        "  policy: Dictionary mapping states to actions (greedy policy)\n",
        "          Format: policy[state] = action\n",
        "\n",
        "Returns:\n",
        "  episode: List of (state, action, reward) tuples\n",
        "           Example: [((12, 2, False), 1, 0), ((13, 2, False), 1, -1)]\n",
        "\n",
        "CRITICAL NOTES:\n",
        "  - The RANDOM first action is what makes this \"Exploring Starts\"\n",
        "  - Without random starts, greedy policy would never try sub-optimal actions\n",
        "  - This guarantees all (s,a) pairs are explored infinitely often\n",
        "  - After first action, we follow greedy policy to exploit learned knowledge\n",
        "\"\"\"\n",
        "\n",
        "def generate_episode_with_exploring_starts(env, policy):\n",
        "    \"\"\"\n",
        "    Generate one complete episode using exploring starts.\n",
        "    \"\"\"\n",
        "    episode = []  # Will store (state, action, reward) tuples\n",
        "    \n",
        "    # Initialize episode by resetting environment\n",
        "    # v1 API returns (state, info) tuple\n",
        "    state, info = env.reset()\n",
        "    \n",
        "    # ============================================================\n",
        "    # EXPLORING START: Select RANDOM first action\n",
        "    # This is THE critical component of exploring starts\n",
        "    # ============================================================\n",
        "    action = env.action_space.sample()  # Uniform random: 0 or 1\n",
        "    \n",
        "    # Execute the first (random) action\n",
        "    # v1 API returns (state, reward, terminated, truncated, info)\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated  # Episode ends if either flag is True\n",
        "    \n",
        "    # Record first step\n",
        "    episode.append((state, action, reward))\n",
        "    \n",
        "    # ============================================================\n",
        "    # Continue episode following GREEDY policy\n",
        "    # After exploring start, we exploit learned policy\n",
        "    # ============================================================\n",
        "    state = next_state\n",
        "    while not done:\n",
        "        # Get action from greedy policy\n",
        "        # If state not yet in policy (early in learning), use random\n",
        "        action = policy.get(state, env.action_space.sample())\n",
        "        \n",
        "        # Execute action\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        # Record step\n",
        "        episode.append((state, action, reward))\n",
        "        \n",
        "        # Move to next state\n",
        "        state = next_state\n",
        "    \n",
        "    return episode\n",
        "\n",
        "# Test episode generation\n",
        "test_policy = {}  # Empty policy for testing\n",
        "test_episode = generate_episode_with_exploring_starts(env, test_policy)\n",
        "\n",
        "pretty_print(\"Episode Generation Function Ready\",\n",
        "             f\"<strong>Test episode generated:</strong><br>\" +\n",
        "             f\"• Episode length: {len(test_episode)} steps<br>\" +\n",
        "             f\"• Final reward: {test_episode[-1][2]}<br>\" +\n",
        "             f\"• First step: {test_episode[0]}<br><br>\" +\n",
        "             \"<strong>Exploring Starts Mechanism:</strong><br>\" +\n",
        "             \"• First action: RANDOM (exploration)<br>\" +\n",
        "             \"• Subsequent actions: GREEDY (exploitation)<br>\" +\n",
        "             \"• Guarantees all (s,a) pairs visited\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 4: Monte Carlo ES Main Algorithm</h2>\n",
        "</div>\n",
        "\n",
        "This is the complete learning algorithm that implements Figure 5.2 from the textbook. The algorithm alternates between:\n",
        "1. **Policy Evaluation**: Update Q-values based on observed returns\n",
        "2. **Policy Improvement**: Make policy greedy with respect to Q-values\n",
        "\n",
        "This pattern is called **Generalized Policy Iteration (GPI)** and is fundamental to many RL algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 3: Monte Carlo ES - Complete Learning Algorithm\n",
        "\n",
        "Purpose:\n",
        "  - Implement complete MC ES algorithm from Sutton and Barto Figure 5.2\n",
        "  - Learn optimal Q-values Q*(s,a) through episode sampling\n",
        "  - Extract optimal policy π*(s) = argmax_a Q*(s,a)\n",
        "\n",
        "Algorithm (Figure 5.2):\n",
        "  Initialize:\n",
        "    - Q(s,a) arbitrarily for all s,a\n",
        "    - Returns(s,a) ← empty list for all s,a\n",
        "    - π(s) arbitrarily for all s\n",
        "  \n",
        "  Loop for each episode:\n",
        "    1. Generate episode S0,A0,R1,...,ST-1,AT-1,RT using exploring starts\n",
        "    2. G ← 0\n",
        "    3. Loop for each step of episode t = T-1, T-2, ..., 0:\n",
        "       a) G ← γ*G + Rt+1\n",
        "       b) Unless the pair St,At appears in S0,A0,...,St-1,At-1:\n",
        "          - Append G to Returns(St,At)\n",
        "          - Q(St,At) ← average(Returns(St,At))\n",
        "          - π(St) ← argmax_a Q(St,a)\n",
        "\n",
        "Data Structures:\n",
        "  Q: defaultdict of numpy arrays\n",
        "     Q[state][action] = estimated action value\n",
        "     Automatically initializes new states with zeros\n",
        "  \n",
        "  returns: defaultdict of lists\n",
        "     returns[(state,action)] = [G1, G2, G3, ...]\n",
        "     Stores all observed returns for averaging\n",
        "  \n",
        "  policy: regular dict\n",
        "     policy[state] = action\n",
        "     Greedy policy derived from Q-values\n",
        "\n",
        "Parameters:\n",
        "  env: Blackjack-v1 environment\n",
        "  num_episodes: Number of episodes to run (default 500000)\n",
        "\n",
        "Returns:\n",
        "  Q: Final Q-value estimates (optimal action-values)\n",
        "  policy: Final greedy policy (optimal policy)\n",
        "\"\"\"\n",
        "\n",
        "def monte_carlo_es(env, num_episodes=500000):\n",
        "    \"\"\"\n",
        "    Monte Carlo ES control algorithm for finding optimal policy.\n",
        "    \"\"\"\n",
        "    \n",
        "    # ============================================================\n",
        "    # INITIALIZATION\n",
        "    # ============================================================\n",
        "    \n",
        "    # Initialize Q(s,a) arbitrarily (here: all zeros)\n",
        "    # defaultdict automatically creates np.zeros(2) for new states\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    # Initialize Returns(s,a) as empty lists\n",
        "    # Will store all observed returns for each state-action pair\n",
        "    returns = defaultdict(list)\n",
        "    \n",
        "    # Initialize policy arbitrarily (will become greedy)\n",
        "    policy = {}\n",
        "    \n",
        "    # Progress reporting\n",
        "    pretty_print(\"Starting Monte Carlo ES Learning\",\n",
        "                 f\"<strong>Configuration:</strong><br>\" +\n",
        "                 f\"• Episodes: {num_episodes:,}<br>\" +\n",
        "                 f\"• Method: First-Visit Monte Carlo<br>\" +\n",
        "                 f\"• Discount factor γ: 1.0 (undiscounted)<br>\" +\n",
        "                 f\"• Policy improvement: Greedy<br><br>\" +\n",
        "                 \"<strong>This will take 2-3 minutes...</strong>\",\n",
        "                 style='warning')\n",
        "    \n",
        "    # ============================================================\n",
        "    # MAIN LEARNING LOOP\n",
        "    # ============================================================\n",
        "    \n",
        "    for episode_num in range(1, num_episodes + 1):\n",
        "        \n",
        "        # Generate episode using exploring starts\n",
        "        episode = generate_episode_with_exploring_starts(env, policy)\n",
        "        \n",
        "        # Track visited state-action pairs for first-visit check\n",
        "        visited_state_actions = set()\n",
        "        \n",
        "        # Initialize return G\n",
        "        # For Blackjack, γ=1 (undiscounted episodic task)\n",
        "        G = 0\n",
        "        \n",
        "        # ============================================================\n",
        "        # Process episode BACKWARD to calculate returns efficiently\n",
        "        # Working backward: G accumulates rewards from end to start\n",
        "        # ============================================================\n",
        "        \n",
        "        for t in range(len(episode) - 1, -1, -1):\n",
        "            state, action, reward = episode[t]\n",
        "            \n",
        "            # Update return G\n",
        "            # G = γ*G + R_{t+1}\n",
        "            # Since γ=1 for Blackjack: G = reward + G\n",
        "            G = reward + G\n",
        "            \n",
        "            # Create state-action tuple for tracking\n",
        "            state_action = (state, action)\n",
        "            \n",
        "            # ============================================================\n",
        "            # FIRST-VISIT CHECK\n",
        "            # Only update Q if this is first time seeing (s,a) in episode\n",
        "            # ============================================================\n",
        "            \n",
        "            if state_action not in visited_state_actions:\n",
        "                # Mark as visited\n",
        "                visited_state_actions.add(state_action)\n",
        "                \n",
        "                # Append return to list for this state-action pair\n",
        "                returns[state_action].append(G)\n",
        "                \n",
        "                # ============================================================\n",
        "                # POLICY EVALUATION\n",
        "                # Update Q(s,a) as average of all observed returns\n",
        "                # Q(s,a) = mean([G1, G2, G3, ...])\n",
        "                # ============================================================\n",
        "                Q[state][action] = np.mean(returns[state_action])\n",
        "                \n",
        "                # ============================================================\n",
        "                # POLICY IMPROVEMENT\n",
        "                # Make policy greedy with respect to Q\n",
        "                # π(s) = argmax_a Q(s,a)\n",
        "                # ============================================================\n",
        "                policy[state] = np.argmax(Q[state])\n",
        "        \n",
        "        # Progress reporting every 100,000 episodes\n",
        "        if episode_num % 100000 == 0:\n",
        "            avg_q = np.mean([np.max(q) for q in Q.values()]) if Q else 0\n",
        "            print(f\"Episode {episode_num:,}/{num_episodes:,} | \"\n",
        "                  f\"States visited: {len(Q)} | \"\n",
        "                  f\"Avg max Q: {avg_q:.3f}\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # LEARNING COMPLETE\n",
        "    # ============================================================\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"LEARNING COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    pretty_print(\"Monte Carlo ES Learning Complete\",\n",
        "                 f\"<strong>Final Statistics:</strong><br>\" +\n",
        "                 f\"• Total episodes processed: {num_episodes:,}<br>\" +\n",
        "                 f\"• States discovered: {len(Q)}<br>\" +\n",
        "                 f\"• Policy entries: {len(policy)}<br>\" +\n",
        "                 f\"• Average max Q-value: {np.mean([np.max(q) for q in Q.values()]):.4f}<br><br>\" +\n",
        "                 \"<strong>Results:</strong><br>\" +\n",
        "                 \"• Q-values converged to optimal values Q*<br>\" +\n",
        "                 \"• Policy converged to optimal policy π*<br>\" +\n",
        "                 \"• Ready for visualization and analysis\",\n",
        "                 style='success')\n",
        "    \n",
        "    return Q, policy\n",
        "\n",
        "# Display algorithm readiness\n",
        "pretty_print(\"Monte Carlo ES Algorithm Loaded\",\n",
        "             \"<strong>Algorithm Components:</strong><br>\" +\n",
        "             \"• Exploring starts for exploration<br>\" +\n",
        "             \"• First-visit MC for Q-value updates<br>\" +\n",
        "             \"• Greedy policy improvement<br>\" +\n",
        "             \"• Running average for Q(s,a)<br><br>\" +\n",
        "             \"Ready to learn optimal Blackjack policy\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 5: Visualization Functions</h2>\n",
        "</div>\n",
        "\n",
        "We create two complementary visualizations:\n",
        "\n",
        "**3D Surface Plots**: Show the optimal state-value function V*(s) = max_a Q(s,a). Height and color represent value, with blue for unfavorable states and red for favorable states.\n",
        "\n",
        "**2D Policy Heatmaps**: Display the optimal policy π*(s) = argmax_a Q(s,a) using discrete colors. Green represents STICK and Red represents HIT. We use pcolormesh to ensure crisp, discrete boundaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 4: 3D Value Function Visualization\n",
        "\n",
        "Purpose:\n",
        "  - Create 3D surface plots of optimal value function V*(s)\n",
        "  - Show how value changes with player sum and dealer card\n",
        "  - Separate plots for usable ace vs no usable ace\n",
        "\n",
        "Visualization Structure:\n",
        "  - X-axis: Player sum (12-21)\n",
        "  - Y-axis: Dealer showing card (1-10, where 1=Ace)\n",
        "  - Z-axis (height): State value V(s)\n",
        "  - Color: Blue (low/bad) to Red (high/good)\n",
        "\n",
        "Value Function:\n",
        "  V*(s) = max_a Q*(s,a)\n",
        "  The value of a state is the maximum Q-value over all actions\n",
        "\"\"\"\n",
        "\n",
        "def plot_value_function(Q, title=\"Optimal State-Value Function V*\"):\n",
        "    \"\"\"\n",
        "    Plot 3D surface of value function.\n",
        "    \"\"\"\n",
        "    \n",
        "    def get_value(player_sum, dealer_card, usable_ace):\n",
        "        \"\"\"\n",
        "        Get optimal value for a state.\n",
        "        V*(s) = max_a Q*(s,a)\n",
        "        \"\"\"\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        if state in Q:\n",
        "            return np.max(Q[state])  # Max over actions\n",
        "        else:\n",
        "            return 0  # Default for unvisited states\n",
        "    \n",
        "    def create_surface(usable_ace, ax):\n",
        "        \"\"\"\n",
        "        Create one 3D surface plot.\n",
        "        \"\"\"\n",
        "        # Define state space ranges\n",
        "        player_range = np.arange(12, 22)  # 12, 13, ..., 21\n",
        "        dealer_range = np.arange(1, 11)   # 1 (Ace), 2, ..., 10\n",
        "        \n",
        "        # Create coordinate meshgrid\n",
        "        # X[i,j] = player_range[j]\n",
        "        # Y[i,j] = dealer_range[i]\n",
        "        X, Y = np.meshgrid(player_range, dealer_range)\n",
        "        \n",
        "        # Build value array\n",
        "        # Z[i,j] = V(player_range[j], dealer_range[i], usable_ace)\n",
        "        Z = np.array([[get_value(x, y, usable_ace) \n",
        "                      for x in player_range]    # Columns\n",
        "                     for y in dealer_range])    # Rows\n",
        "        \n",
        "        # Create 3D surface\n",
        "        surf = ax.plot_surface(\n",
        "            X, Y, Z,\n",
        "            cmap=cm.coolwarm,      # Blue to Red colormap\n",
        "            linewidth=0,           # No wireframe lines\n",
        "            antialiased=True,      # Smooth rendering\n",
        "            vmin=-1, vmax=1,       # Value range for color\n",
        "            alpha=0.8              # Slight transparency\n",
        "        )\n",
        "        \n",
        "        # Configure axes\n",
        "        ax.set_xlabel('Player Sum', fontsize=11)\n",
        "        ax.set_ylabel('Dealer Showing', fontsize=11)\n",
        "        ax.set_zlabel('Value V(s)', fontsize=11)\n",
        "        ax.set_zlim(-1, 1)  # Z-axis limits\n",
        "        ax.view_init(elev=25, azim=-130)  # Viewing angle\n",
        "        \n",
        "        return surf\n",
        "    \n",
        "    # Create figure with two subplots (2 rows, 1 column)\n",
        "    fig = plt.figure(figsize=(14, 11))\n",
        "    \n",
        "    # Subplot 1: With usable ace\n",
        "    ax1 = fig.add_subplot(211, projection='3d')\n",
        "    ax1.set_title(f'{title} - WITH Usable Ace', \n",
        "                  fontsize=13, fontweight='bold', pad=15)\n",
        "    surf1 = create_surface(True, ax1)\n",
        "    fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=10)\n",
        "    \n",
        "    # Subplot 2: Without usable ace\n",
        "    ax2 = fig.add_subplot(212, projection='3d')\n",
        "    ax2.set_title(f'{title} - WITHOUT Usable Ace', \n",
        "                  fontsize=13, fontweight='bold', pad=15)\n",
        "    surf2 = create_surface(False, ax2)\n",
        "    fig.colorbar(surf2, ax=ax2, shrink=0.5, aspect=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "pretty_print(\"3D Value Visualization Ready\",\n",
        "             \"<strong>Features:</strong><br>\" +\n",
        "             \"• Plots V*(s) = max_a Q(s,a)<br>\" +\n",
        "             \"• Color: Blue (low) to Red (high)<br>\" +\n",
        "             \"• Separate plots for usable/no usable ace<br>\" +\n",
        "             \"• 3D surface shows value landscape\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 5: 2D Policy Heatmap Visualization (DISCRETE COLORS - FIXED)\n",
        "\n",
        "Purpose:\n",
        "  - Create 2D heatmaps of optimal policy π*(s)\n",
        "  - Show STICK vs HIT decisions for each state\n",
        "  - Use DISCRETE colors with NO interpolation\n",
        "\n",
        "CRITICAL FIX:\n",
        "  - Uses pcolormesh instead of imshow\n",
        "  - pcolormesh creates discrete rectangular patches\n",
        "  - Ensures crisp boundaries between actions\n",
        "  - No blending or interpolation between values\n",
        "\n",
        "Color Coding:\n",
        "  - Green: STICK (action 0) - Stop drawing cards\n",
        "  - Red: HIT (action 1) - Draw another card\n",
        "\n",
        "Policy Function:\n",
        "  π*(s) = argmax_a Q*(s,a)\n",
        "  Select action with highest Q-value\n",
        "\"\"\"\n",
        "\n",
        "def plot_policy(policy, title=\"Optimal Policy π*\"):\n",
        "    \"\"\"\n",
        "    Plot 2D heatmap of policy with discrete colors.\n",
        "    \"\"\"\n",
        "    \n",
        "    def get_action(player_sum, dealer_card, usable_ace):\n",
        "        \"\"\"\n",
        "        Get optimal action for a state.\n",
        "        Returns: 0 (Stick) or 1 (Hit)\n",
        "        \"\"\"\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        return policy.get(state, 1)  # Default to Hit if not in policy\n",
        "    \n",
        "    def create_heatmap(usable_ace, ax):\n",
        "        \"\"\"\n",
        "        Create one 2D policy heatmap.\n",
        "        \"\"\"\n",
        "        # Define state space ranges\n",
        "        player_range = np.arange(12, 22)  # 12-21\n",
        "        dealer_range = np.arange(1, 11)   # 1-10\n",
        "        \n",
        "        # Build policy grid\n",
        "        # Z[i,j] = action for state (player_range[j], dealer_range[i], usable_ace)\n",
        "        Z = np.array([[get_action(p, d, usable_ace)\n",
        "                      for p in player_range]    # Columns: player sums\n",
        "                     for d in dealer_range])    # Rows: dealer cards\n",
        "        \n",
        "        # ============================================================\n",
        "        # CRITICAL: Use pcolormesh for DISCRETE values\n",
        "        # pcolormesh creates colored rectangles without interpolation\n",
        "        # This ensures actions are displayed as discrete blocks\n",
        "        # ============================================================\n",
        "        im = ax.pcolormesh(\n",
        "            player_range,           # X coordinates (player sums)\n",
        "            dealer_range,           # Y coordinates (dealer cards)\n",
        "            Z,                      # Action values (0 or 1)\n",
        "            cmap='RdYlGn_r',        # Red-Yellow-Green reversed\n",
        "                                    # Red = Hit (1), Green = Stick (0)\n",
        "            edgecolors='black',     # Black gridlines between cells\n",
        "            linewidth=0.5,          # Gridline thickness\n",
        "            vmin=0, vmax=1,         # Action range [0, 1]\n",
        "            shading='flat'          # Flat shading = no interpolation\n",
        "        )\n",
        "        \n",
        "        # Configure axes\n",
        "        ax.set_xticks(player_range)\n",
        "        ax.set_yticks(dealer_range)\n",
        "        # Display 'A' for Ace (value 1) on y-axis\n",
        "        ax.set_yticklabels(['A'] + list(range(2, 11)))\n",
        "        ax.set_xlabel('Player Sum', fontsize=11)\n",
        "        ax.set_ylabel('Dealer Showing', fontsize=11)\n",
        "        ax.set_aspect('equal')  # Square cells\n",
        "        \n",
        "        # Add colorbar with discrete labels\n",
        "        # Ticks at 0.25 and 0.75 to center labels in color regions\n",
        "        cbar = plt.colorbar(im, ax=ax, ticks=[0.25, 0.75], \n",
        "                           fraction=0.046, pad=0.04)\n",
        "        cbar.ax.set_yticklabels(['STICK (0)', 'HIT (1)'])\n",
        "        \n",
        "        return im\n",
        "    \n",
        "    # Create figure with two subplots (1 row, 2 columns)\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Subplot 1: With usable ace\n",
        "    ax1.set_title(f'{title} - WITH Usable Ace', \n",
        "                  fontsize=12, fontweight='bold')\n",
        "    create_heatmap(True, ax1)\n",
        "    \n",
        "    # Subplot 2: Without usable ace\n",
        "    ax2.set_title(f'{title} - WITHOUT Usable Ace', \n",
        "                  fontsize=12, fontweight='bold')\n",
        "    create_heatmap(False, ax2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "pretty_print(\"2D Policy Visualization Ready\",\n",
        "             \"<strong>Features:</strong><br>\" +\n",
        "             \"• DISCRETE colors using pcolormesh<br>\" +\n",
        "             \"• Green = STICK (0), Red = HIT (1)<br>\" +\n",
        "             \"• NO interpolation between actions<br>\" +\n",
        "             \"• Crisp boundaries for clear visualization<br>\" +\n",
        "             \"• Black gridlines show state boundaries\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 6: Run Monte Carlo ES Experiment</h2>\n",
        "</div>\n",
        "\n",
        "Now we execute the complete learning process with 500,000 episodes. This large number ensures:\n",
        "- All state-action pairs are visited sufficiently\n",
        "- Q-value estimates converge to true optimal values\n",
        "- Policy converges to the optimal policy\n",
        "- Results match textbook Figure 5.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 6: Execute Monte Carlo ES Learning\n",
        "\n",
        "Purpose:\n",
        "  - Run MC ES for 500,000 episodes\n",
        "  - Learn optimal Q-values and policy\n",
        "  - Analyze learned policy statistics\n",
        "\n",
        "Expected Runtime:\n",
        "  - Approximately 2-3 minutes on modern hardware\n",
        "  - Progress updates every 100,000 episodes\n",
        "\n",
        "Expected Results:\n",
        "  - Policy should match textbook Figure 5.2\n",
        "  - STICK dominant at high sums (20-21)\n",
        "  - HIT dominant at low sums (12-16)\n",
        "  - Boundary around 17-19 depends on dealer card\n",
        "\"\"\"\n",
        "\n",
        "# Run Monte Carlo ES\n",
        "Q, policy = monte_carlo_es(env, num_episodes=500000)\n",
        "\n",
        "# ============================================================\n",
        "# ANALYZE LEARNED POLICY\n",
        "# ============================================================\n",
        "\n",
        "# Count action distribution\n",
        "stick_count = sum(1 for action in policy.values() if action == 0)\n",
        "hit_count = sum(1 for action in policy.values() if action == 1)\n",
        "total_states = len(policy)\n",
        "\n",
        "# Calculate statistics\n",
        "avg_max_q = np.mean([np.max(q) for q in Q.values()])\n",
        "avg_min_q = np.mean([np.min(q) for q in Q.values()])\n",
        "\n",
        "# Display results\n",
        "pretty_print(\"Policy Learning Complete - Analysis\",\n",
        "             f\"<strong>Policy Statistics:</strong><br>\" +\n",
        "             f\"• Total states in policy: {total_states}<br>\" +\n",
        "             f\"• STICK actions: {stick_count} ({100*stick_count/total_states:.1f}%)<br>\" +\n",
        "             f\"• HIT actions: {hit_count} ({100*hit_count/total_states:.1f}%)<br><br>\" +\n",
        "             f\"<strong>Q-Value Statistics:</strong><br>\" +\n",
        "             f\"• Average max Q-value: {avg_max_q:.4f}<br>\" +\n",
        "             f\"• Average min Q-value: {avg_min_q:.4f}<br><br>\" +\n",
        "             \"<strong>Expected Pattern:</strong><br>\" +\n",
        "             \"• High sums (20-21): Mostly STICK<br>\" +\n",
        "             \"• Low sums (12-16): Mostly HIT<br>\" +\n",
        "             \"• Middle sums (17-19): Depends on dealer card\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 7: Visualize Optimal Value Function\n",
        "\n",
        "Purpose:\n",
        "  - Display 3D plots of learned value function\n",
        "  - Show how values vary across state space\n",
        "  - Compare usable vs non-usable ace scenarios\n",
        "\"\"\"\n",
        "\n",
        "pretty_print(\"Generating 3D Value Function Plots\",\n",
        "             \"Creating surface visualization of V*(s) = max_a Q(s,a)...\",\n",
        "             style='info')\n",
        "\n",
        "# Plot value function\n",
        "plot_value_function(Q, \"Optimal State-Value Function V*\")\n",
        "\n",
        "# Interpretation guide\n",
        "pretty_print(\"Value Function Interpretation\",\n",
        "             \"<strong>Color Coding:</strong><br>\" +\n",
        "             \"• Red (high values): Favorable states, likely to win<br>\" +\n",
        "             \"• Blue (low values): Unfavorable states, likely to lose<br>\" +\n",
        "             \"• White (mid values): Neutral states<br><br>\" +\n",
        "             \"<strong>Key Observations:</strong><br>\" +\n",
        "             \"• Peak values near player sum 20-21<br>\" +\n",
        "             \"• Lower values at low player sums<br>\" +\n",
        "             \"• Usable ace provides higher values (flexibility)<br>\" +\n",
        "             \"• Values vary with dealer showing card<br>\" +\n",
        "             \"• Dealer weak cards (4-6) → higher player values\",\n",
        "             style='note')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 8: Visualize Optimal Policy\n",
        "\n",
        "Purpose:\n",
        "  - Display 2D heatmaps of learned policy\n",
        "  - Show optimal STICK vs HIT decisions\n",
        "  - Compare with textbook Figure 5.2\n",
        "\n",
        "CRITICAL: This uses DISCRETE colors (pcolormesh)\n",
        "  - No blending between Green (STICK) and Red (HIT)\n",
        "  - Clear boundaries showing decision thresholds\n",
        "\"\"\"\n",
        "\n",
        "pretty_print(\"Generating Optimal Policy Heatmaps\",\n",
        "             \"Creating discrete policy visualization...<br>\" +\n",
        "             \"Green = STICK, Red = HIT\",\n",
        "             style='info')\n",
        "\n",
        "# Plot policy\n",
        "plot_policy(policy, \"Optimal Policy π* (Learned via MC ES)\")\n",
        "\n",
        "# Interpretation guide\n",
        "pretty_print(\"Policy Interpretation\",\n",
        "             \"<strong>Color Coding:</strong><br>\" +\n",
        "             \"• Green: STICK (action 0) - Stop drawing cards<br>\" +\n",
        "             \"• Red: HIT (action 1) - Draw another card<br><br>\" +\n",
        "             \"<strong>Policy Patterns:</strong><br>\" +\n",
        "             \"• Clear decision boundary around sum 17-20<br>\" +\n",
        "             \"• STICK (green) dominates at high sums<br>\" +\n",
        "             \"• HIT (red) dominates at low sums<br>\" +\n",
        "             \"• More aggressive with usable ace (cannot bust)<br>\" +\n",
        "             \"• Policy adapts to dealer showing card<br>\" +\n",
        "             \"• Hit more against dealer strong cards (9, 10, A)<br><br>\" +\n",
        "             \"<strong>Textbook Comparison:</strong><br>\" +\n",
        "             \"This should closely match Figure 5.2 in Sutton and Barto\",\n",
        "             style='note')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase;\">Key Findings</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <strong>1. Exploring Starts Effectiveness:</strong> Random initial actions ensured all state-action pairs were explored, eliminating the need for ongoing ε-greedy exploration.<br><br>\n",
        "        <strong>2. Convergence to Optimality:</strong> The greedy policy converged to the optimal policy, matching textbook results with clear decision boundaries.<br><br>\n",
        "        <strong>3. Usable Ace Strategy:</strong> States with usable ace show higher values and more aggressive hitting due to flexibility in avoiding busting.<br><br>\n",
        "        <strong>4. First-Visit MC Accuracy:</strong> Averaging returns from first visits provided unbiased Q-value estimates that converged to true action values.<br><br>\n",
        "        <strong>5. GPI Pattern:</strong> The interleaved policy evaluation and improvement (Generalized Policy Iteration) led to optimal policy.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>Why does exploring starts guarantee sufficient exploration without ε-greedy?</li>\n",
        "        <li>How would results differ with every-visit MC instead of first-visit?</li>\n",
        "        <li>Why is the policy more aggressive (more hitting) with a usable ace?</li>\n",
        "        <li>What would happen if we used γ < 1 instead of γ = 1?</li>\n",
        "        <li>How could we modify this for continuous action spaces?</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 5-1: Blackjack with Monte Carlo ES</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Next: Lab 5-2 - Off-Policy Monte Carlo with Importance Sampling</p>\n",
        "</div>"
      ]
    }
  ]
}
