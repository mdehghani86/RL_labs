{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_05_1_Blackjack_Monte_Carlo_FIXED.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 5-1: Blackjack with Monte Carlo Methods\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapter 5 | Intermediate Level | 75 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Monte Carlo methods learn directly from episodes of experience without requiring a model of the environment's dynamics.\n",
        "        First introduced for RL by <a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_method\" style=\"color: #17a2b8;\">Stanislaw Ulam</a> \n",
        "        during the Manhattan Project, these methods are particularly effective for episodic tasks. This lab implements the\n",
        "        <strong>First-Visit Monte Carlo</strong> algorithm on the classic Blackjack problem from\n",
        "        <a href=\"http://incompleteideas.net/book/the-book-2nd.html\" style=\"color: #17a2b8;\">Sutton & Barto (2018)</a>, Example 5.1.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Understand Monte Carlo prediction methods</li>\n",
        "        <li>Implement First-Visit MC algorithm</li>\n",
        "        <li>Learn from sampled episodes of experience</li>\n",
        "        <li>Estimate action-value functions Q(s,a)</li>\n",
        "        <li>Visualize value functions and policies</li>\n",
        "        <li>Work with OpenAI Gym environments</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Blackjack Rules</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Goal</code> → Get sum close to 21 without exceeding</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Actions</code> → Hit (draw card) or Stick (stop)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">States</code> → (player_sum, dealer_card, usable_ace)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Rewards</code> → +1 (win), 0 (draw), -1 (lose)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Ace</code> → Can be 1 or 11 (usable if 11)</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px; margin-top: 30px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h2 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 20px; margin: 0; font-weight: 300;\">\n",
        "            Section 1: Environment Setup and Dependencies\n",
        "        </h2>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">Section 1</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        Importing libraries and configuring the computational environment\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\\n",
        "import gymnasium as gym\\n",
        "import numpy as np\\n",
        "from collections import defaultdict\\n",
        "from mpl_toolkits.mplot3d import Axes3D\\n",
        "import matplotlib.pyplot as plt\\n",
        "from matplotlib import cm\\n",
        "import warnings\\n",
        "warnings.filterwarnings('ignore')\\n",
        "\\n",
        "plt.rcParams['figure.dpi'] = 100\\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\\n",
        "plt.rcParams['font.size'] = 10\\n",
        "\\n",
        "print('Libraries imported successfully')\\n",
        "print(f'Gymnasium version: {gym.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px; margin-top: 30px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h2 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 20px; margin: 0; font-weight: 300;\">\n",
        "            Section 2: Creating the Blackjack Environment\n",
        "        </h2>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">Section 2</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        Initializing the OpenAI Gymnasium Blackjack-v1 environment\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('Blackjack-v1')\\n",
        "\\n",
        "print(f'Environment: Blackjack-v1')\\n",
        "print(f'Action space: {env.action_space}')\\n",
        "print(f'Number of actions: {env.action_space.n}')\\n",
        "print('Actions: 0 = Stick (stop), 1 = Hit (draw card)')\\n",
        "\\n",
        "sample_state, _ = env.reset()\\n",
        "print(f'\\\\nSample initial state: {sample_state}')\\n",
        "print(f'  Player sum: {sample_state[0]}')\\n",
        "print(f'  Dealer showing: {sample_state[1]}')\\n",
        "print(f'  Usable ace: {sample_state[2]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px; margin-top: 30px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h2 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 20px; margin: 0; font-weight: 300;\">\n",
        "            Section 3: Monte Carlo ES Algorithm Overview\n",
        "        </h2>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">Section 3</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        Algorithm for Finding Optimal Policy\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0;\">Algorithm Overview</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Monte Carlo ES uses Exploring Starts to ensure all state-action pairs are visited.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <img src=\"https://github.com/mdehghani86/RL_labs/blob/master/Lab%2005/MCM_ES.jpg?raw=true\" \n",
        "         alt=\"Monte Carlo ES Pseudocode\" \n",
        "         style=\"width: 70%; max-width: 800px; border: 2px solid #17a2b8; border-radius: 8px;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px; margin-top: 30px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h2 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 20px; margin: 0; font-weight: 300;\">\n",
        "            Section 4: Stochastic Policy for Exploration\n",
        "        </h2>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">Section 4</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        Defining an arbitrary exploration policy\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_episode_arbitrary_policy(env):\\n",
        "    episode = []\\n",
        "    state, _ = env.reset()\\n",
        "    \\n",
        "    while True:\\n",
        "        if state[0] > 18:\\n",
        "            action_probs = [0.8, 0.2]\\n",
        "        else:\\n",
        "            action_probs = [0.2, 0.8]\\n",
        "        \\n",
        "        action = np.random.choice([0, 1], p=action_probs)\\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\\n",
        "        done = terminated or truncated\\n",
        "        \\n",
        "        episode.append((state, action, reward))\\n",
        "        state = next_state\\n",
        "        \\n",
        "        if done:\\n",
        "            break\\n",
        "    \\n",
        "    return episode\\n",
        "\\n",
        "sample_episode = play_episode_arbitrary_policy(env)\\n",
        "print(f'Episode length: {len(sample_episode)} steps')\\n",
        "print(f'Final reward: {sample_episode[-1][2]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px; margin-top: 30px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h2 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 20px; margin: 0; font-weight: 300;\">\n",
        "            Section 5: First-Visit Monte Carlo Learning\n",
        "        </h2>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">Section 5</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        Implementing Q-value updates and prediction loop\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_Q(episode, Q, returns_sum, N, gamma=1.0):\\n",
        "    visited = set()\\n",
        "    \\n",
        "    for t, (state, action, reward) in enumerate(episode):\\n",
        "        sa_pair = (state, action)\\n",
        "        \\n",
        "        if sa_pair not in visited:\\n",
        "            visited.add(sa_pair)\\n",
        "            G = sum((gamma ** k) * r for k, (_, _, r) in enumerate(episode[t:]))\\n",
        "            returns_sum[state][action] += G\\n",
        "            N[state][action] += 1.0\\n",
        "            Q[state][action] = returns_sum[state][action] / N[state][action]\\n",
        "\\n",
        "def mc_predict(env, num_episodes, gamma=1.0):\\n",
        "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\\n",
        "    N = defaultdict(lambda: np.zeros(env.action_space.n))\\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\\n",
        "    \\n",
        "    print(f'Starting MC prediction with {num_episodes:,} episodes...')\\n",
        "    \\n",
        "    for i_episode in range(1, num_episodes + 1):\\n",
        "        episode = play_episode_arbitrary_policy(env)\\n",
        "        update_Q(episode, Q, returns_sum, N, gamma)\\n",
        "        \\n",
        "        if i_episode % 50000 == 0:\\n",
        "            print(f'Episode {i_episode:,}/{num_episodes:,}')\\n",
        "    \\n",
        "    print('MC prediction complete')\\n",
        "    return Q\\n",
        "\\n",
        "print('MC functions ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px; margin-top: 30px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h2 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 20px; margin: 0; font-weight: 300;\">\n",
        "            Section 6: Visualization Functions\n",
        "        </h2>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">Section 6</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        Creating 3D value plots and 2D policy heatmaps\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_blackjack_values(V):\\n",
        "    def get_Z(player_sum, dealer_card, usable_ace):\\n",
        "        state = (player_sum, dealer_card, usable_ace)\\n",
        "        return V.get(state, 0)\\n",
        "    \\n",
        "    def create_surface(usable_ace, ax):\\n",
        "        player_range = np.arange(12, 22)\\n",
        "        dealer_range = np.arange(1, 11)\\n",
        "        X, Y = np.meshgrid(player_range, dealer_range)\\n",
        "        Z = np.array([[get_Z(x, y, usable_ace) for x in player_range] for y in dealer_range])\\n",
        "        surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True, vmin=-1, vmax=1, alpha=0.8)\\n",
        "        ax.set_xlabel('Player Sum')\\n",
        "        ax.set_ylabel('Dealer Showing')\\n",
        "        ax.set_zlabel('Value')\\n",
        "        ax.set_zlim(-1, 1)\\n",
        "        ax.view_init(elev=25, azim=-130)\\n",
        "        return surf\\n",
        "    \\n",
        "    fig = plt.figure(figsize=(14, 11))\\n",
        "    ax1 = fig.add_subplot(211, projection='3d')\\n",
        "    ax1.set_title('State Values WITH Usable Ace', fontsize=13, fontweight='bold')\\n",
        "    surf1 = create_surface(True, ax1)\\n",
        "    fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=10)\\n",
        "    ax2 = fig.add_subplot(212, projection='3d')\\n",
        "    ax2.set_title('State Values WITHOUT Usable Ace', fontsize=13, fontweight='bold')\\n",
        "    surf2 = create_surface(False, ax2)\\n",
        "    fig.colorbar(surf2, ax=ax2, shrink=0.5, aspect=10)\\n",
        "    plt.tight_layout()\\n",
        "    plt.show()\\n",
        "\\n",
        "def plot_policy(policy):\\n",
        "    def get_action(player_sum, dealer_card, usable_ace):\\n",
        "        state = (player_sum, dealer_card, usable_ace)\\n",
        "        return policy.get(state, 1)\\n",
        "    \\n",
        "    def create_heatmap(usable_ace, ax):\\n",
        "        player_range = range(12, 22)\\n",
        "        dealer_range = range(1, 11)\\n",
        "        Z = np.array([[get_action(player, dealer, usable_ace) for player in player_range] for dealer in dealer_range])\\n",
        "        im = ax.imshow(Z, cmap='RdYlGn_r', aspect='auto', vmin=0, vmax=1, extent=[11.5, 21.5, 0.5, 10.5], origin='lower', interpolation='nearest')\\n",
        "        ax.set_xticks(range(12, 22))\\n",
        "        ax.set_yticks(range(1, 11))\\n",
        "        ax.set_yticklabels(['A'] + list(range(2, 11)))\\n",
        "        ax.set_xlabel('Player Sum')\\n",
        "        ax.set_ylabel('Dealer Showing')\\n",
        "        ax.grid(True, color='black', linewidth=0.5, alpha=0.3)\\n",
        "        cbar = plt.colorbar(im, ax=ax, ticks=[0, 1], fraction=0.046, pad=0.04)\\n",
        "        cbar.ax.set_yticklabels(['STICK', 'HIT'])\\n",
        "        return im\\n",
        "    \\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\\n",
        "    ax1.set_title('Policy WITH Usable Ace', fontsize=12, fontweight='bold')\\n",
        "    create_heatmap(True, ax1)\\n",
        "    ax2.set_title('Policy WITHOUT Usable Ace', fontsize=12, fontweight='bold')\\n",
        "    create_heatmap(False, ax2)\\n",
        "    plt.tight_layout()\\n",
        "    plt.show()\\n",
        "\\n",
        "print('Visualization functions ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px; margin-top: 30px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h2 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 20px; margin: 0; font-weight: 300;\">\n",
        "            Section 7: Running Monte Carlo Experiments\n",
        "        </h2>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">Section 7</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        Learning Q-values and extracting optimal policy\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_EPISODES = 500000\\n",
        "Q = mc_predict(env, NUM_EPISODES)\\n",
        "\\n",
        "V_arbitrary = {}\\n",
        "for state, action_values in Q.items():\\n",
        "    if state[0] > 18:\\n",
        "        V_arbitrary[state] = 0.8 * action_values[0] + 0.2 * action_values[1]\\n",
        "    else:\\n",
        "        V_arbitrary[state] = 0.2 * action_values[0] + 0.8 * action_values[1]\\n",
        "\\n",
        "optimal_policy = {}\\n",
        "for state, action_values in Q.items():\\n",
        "    optimal_policy[state] = np.argmax(action_values)\\n",
        "\\n",
        "states_count = len(Q)\\n",
        "stick_count = sum(1 for a in optimal_policy.values() if a == 0)\\n",
        "hit_count = sum(1 for a in optimal_policy.values() if a == 1)\\n",
        "\\n",
        "print(f'States explored: {states_count}')\\n",
        "print(f'STICK states: {stick_count} ({100*stick_count/states_count:.1f}%)')\\n",
        "print(f'HIT states: {hit_count} ({100*hit_count/states_count:.1f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_blackjack_values(V_arbitrary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_policy(optimal_policy)"
      ]
    }
  ]
}
