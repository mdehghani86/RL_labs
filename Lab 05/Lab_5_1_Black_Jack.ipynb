{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_05_1_Blackjack_Monte_Carlo_FIXED.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 5-1: Blackjack with Monte Carlo Methods\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapter 5 | Intermediate Level | 75 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Monte Carlo methods learn directly from episodes of experience without requiring a model of the environment. \n",
        "        First introduced for RL by Stanislaw Ulam during the Manhattan Project, these methods are particularly effective \n",
        "        for episodic tasks. This lab implements the <strong>First-Visit Monte Carlo</strong> algorithm on the classic \n",
        "        Blackjack problem from Sutton & Barto (2018), Example 5.1. We explore how Monte Carlo methods estimate value \n",
        "        functions through repeated sampling and averaging of returns.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Understand Monte Carlo prediction methods</li>\n",
        "        <li>Implement First-Visit MC algorithm</li>\n",
        "        <li>Learn from sampled episodes of experience</li>\n",
        "        <li>Estimate action-value functions Q(s,a)</li>\n",
        "        <li>Visualize value functions and policies</li>\n",
        "        <li>Work with OpenAI Gymnasium environments</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Blackjack Rules</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Goal</code> → Get sum close to 21 without exceeding</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Actions</code> → Hit (draw card) or Stick (stop)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">States</code> → (player_sum, dealer_card, usable_ace)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Rewards</code> → +1 (win), 0 (draw), -1 (lose)</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Ace</code> → Can be 1 or 11 (usable if 11)</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Environment Setup and Dependencies\n",
        "\n",
        "We begin by importing the necessary libraries for our Monte Carlo implementation. The key libraries are:\n",
        "- **Gymnasium**: Provides the Blackjack-v1 environment (successor to OpenAI Gym)\n",
        "- **NumPy**: For numerical computations and array operations\n",
        "- **Matplotlib**: For creating visualizations of value functions and policies\n",
        "- **Collections**: For efficient data structures like defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(f\"Gymnasium version: {gym.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Creating the Blackjack Environment\n",
        "\n",
        "The Blackjack environment simulates the card game with simplified rules. The state space consists of three components:\n",
        "1. **Player sum** (12-21): Current sum of player cards\n",
        "2. **Dealer card** (1-10): The dealer's visible card (1 = Ace, 10 = face cards)\n",
        "3. **Usable ace** (True/False): Whether player has an ace counted as 11\n",
        "\n",
        "The action space has two actions: Stick (0) to stop taking cards, or Hit (1) to draw another card. Rewards are given only at episode termination: +1 for winning, 0 for drawing, and -1 for losing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "env = gym.make('Blackjack-v1')\n",
        "\n",
        "print(f\"Environment: Blackjack-v1\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Number of actions: {env.action_space.n}\")\n",
        "print(\"Actions: 0 = Stick, 1 = Hit\")\n",
        "\n",
        "sample_state, _ = env.reset()\n",
        "print(f\"\\nSample initial state: {sample_state}\")\n",
        "print(f\"  Player sum: {sample_state[0]}\")\n",
        "print(f\"  Dealer showing: {sample_state[1]}\")\n",
        "print(f\"  Usable ace: {sample_state[2]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Monte Carlo ES Algorithm Overview\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0;\">Monte Carlo with Exploring Starts</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Monte Carlo ES uses <strong>Exploring Starts</strong> to ensure comprehensive exploration of the state-action space. \n",
        "        Each episode begins with a random state-action pair, guaranteeing that all possibilities are visited. After the \n",
        "        initial random selection, the agent follows its current policy for the remainder of the episode. This approach \n",
        "        solves the exploration problem while still converging to the optimal policy through iterative improvement.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <img src=\"https://github.com/mdehghani86/RL_labs/blob/master/Lab%2005/MCM_ES.jpg?raw=true\" \n",
        "         alt=\"Monte Carlo ES Pseudocode\" \n",
        "         style=\"width: 70%; max-width: 800px; border: 2px solid #17a2b8; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
        "    <p style=\"color: #666; font-size: 12px; margin-top: 10px; font-style: italic;\">Figure: Monte Carlo ES Algorithm from Sutton & Barto</p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: #e8f5e9; padding: 12px 15px; border-left: 3px solid #4caf50; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #2e7d32; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Algorithm Steps</h4>\n",
        "    <ol style=\"color: #555; line-height: 1.6; margin: 0; padding-left: 20px; font-size: 12px;\">\n",
        "        <li><strong>Exploring Start:</strong> Choose random (S₀, A₀) pair</li>\n",
        "        <li><strong>Generate Episode:</strong> Follow current policy π from S₁ onward</li>\n",
        "        <li><strong>Calculate Returns:</strong> Compute G for each visited (s,a)</li>\n",
        "        <li><strong>Update Q-values:</strong> Average all returns for each (s,a) pair</li>\n",
        "        <li><strong>Policy Improvement:</strong> Make policy greedy: π(s) ← argmax Q(s,a)</li>\n",
        "    </ol>\n",
        "</td>\n",
        "<td style=\"background: #fff3e0; padding: 12px 15px; border-left: 3px solid #ff9800; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #e65100; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Why Exploring Starts?</h4>\n",
        "    <p style=\"color: #555; font-size: 12px; line-height: 1.6; margin: 0 0 8px 0;\">\n",
        "        Without exploring starts, a deterministic policy might never visit certain state-action pairs, \n",
        "        preventing optimal value estimation. Random initialization ensures every (s,a) pair has non-zero \n",
        "        probability of being explored.\n",
        "    </p>\n",
        "    <p style=\"color: #555; font-size: 12px; line-height: 1.6; margin: 0;\">\n",
        "        <strong>Key Guarantee:</strong> All state-action pairs are visited infinitely often as episodes → ∞\n",
        "    </p>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Stochastic Policy for Exploration\n",
        "\n",
        "In this implementation, we use an **arbitrary stochastic policy** for generating learning episodes. This policy serves as our exploration mechanism during the learning phase. The policy is threshold-based:\n",
        "- When player sum > 18: Prefer to stick (80% probability) to avoid busting\n",
        "- When player sum ≤ 18: Prefer to hit (80% probability) to get closer to 21\n",
        "\n",
        "This is NOT the optimal policy we're trying to find. Rather, it's a reasonable exploration strategy that ensures we visit diverse states and actions. From the Q-values learned using this arbitrary policy, we will later extract the optimal greedy policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def play_episode_arbitrary_policy(env):\n",
        "    episode = []\n",
        "    state, _ = env.reset()\n",
        "    \n",
        "    while True:\n",
        "        if state[0] > 18:\n",
        "            action_probs = [0.8, 0.2]\n",
        "        else:\n",
        "            action_probs = [0.2, 0.8]\n",
        "        \n",
        "        action = np.random.choice([0, 1], p=action_probs)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    return episode\n",
        "\n",
        "sample_episode = play_episode_arbitrary_policy(env)\n",
        "print(f\"Sample episode length: {len(sample_episode)} steps\")\n",
        "print(f\"Final reward: {sample_episode[-1][2]}\")\n",
        "print(f\"First 3 steps:\")\n",
        "for i, (state, action, reward) in enumerate(sample_episode[:3]):\n",
        "    action_name = \"Stick\" if action == 0 else \"Hit\"\n",
        "    print(f\"  {i+1}. State={state}, Action={action_name}, Reward={reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: First-Visit Monte Carlo Q-Value Updates\n",
        "\n",
        "The core of Monte Carlo learning is the update of Q-values based on observed returns. We implement the **First-Visit MC** approach:\n",
        "\n",
        "**First-Visit Rule:** For each (state, action) pair, only the FIRST occurrence in an episode is used for updates. Subsequent visits to the same pair are ignored.\n",
        "\n",
        "**Return Calculation:** From time t when (s,a) is first visited:\n",
        "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$$\n",
        "\n",
        "**Q-value Update:** The action-value is the average of all observed returns:\n",
        "$$Q(s,a) = \\frac{1}{N(s,a)} \\sum_{\\text{episodes}} G_t$$\n",
        "\n",
        "Where N(s,a) is the number of times we've made a first-visit to (s,a) across all episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def update_Q(episode, Q, returns_sum, N, gamma=1.0):\n",
        "    visited = set()\n",
        "    \n",
        "    for t, (state, action, reward) in enumerate(episode):\n",
        "        sa_pair = (state, action)\n",
        "        \n",
        "        if sa_pair not in visited:\n",
        "            visited.add(sa_pair)\n",
        "            \n",
        "            G = sum((gamma ** k) * r for k, (_, _, r) in enumerate(episode[t:]))\n",
        "            \n",
        "            returns_sum[state][action] += G\n",
        "            N[state][action] += 1.0\n",
        "            Q[state][action] = returns_sum[state][action] / N[state][action]\n",
        "\n",
        "print(\"Q-value update function ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def mc_predict(env, num_episodes, gamma=1.0):\n",
        "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    print(f\"Starting MC prediction with {num_episodes:,} episodes...\\n\")\n",
        "    \n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        episode = play_episode_arbitrary_policy(env)\n",
        "        update_Q(episode, Q, returns_sum, N, gamma)\n",
        "        \n",
        "        if i_episode % 50000 == 0:\n",
        "            print(f\"Episode {i_episode:,}/{num_episodes:,}\")\n",
        "    \n",
        "    print(\"\\nMonte Carlo prediction complete\")\n",
        "    return Q\n",
        "\n",
        "print(\"MC prediction function ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Visualization Functions\n",
        "\n",
        "We create two types of visualizations to understand the learned value function and policy:\n",
        "\n",
        "**3D Surface Plots:** Display state values V(s) as a function of player sum and dealer showing card. The height and color of the surface represent the expected value of being in that state. We create separate plots for states with and without a usable ace, as the ace significantly affects strategy.\n",
        "\n",
        "**2D Policy Heatmaps:** Show the optimal action (Stick or Hit) for each state using color coding. Green indicates Stick (action 0) and Red indicates Hit (action 1). These heatmaps provide an intuitive view of the decision boundaries learned by the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_blackjack_values(V):\n",
        "    def get_Z(player_sum, dealer_card, usable_ace):\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        return V.get(state, 0)\n",
        "    \n",
        "    def create_surface(usable_ace, ax):\n",
        "        player_range = np.arange(12, 22)\n",
        "        dealer_range = np.arange(1, 11)\n",
        "        X, Y = np.meshgrid(player_range, dealer_range)\n",
        "        \n",
        "        Z = np.array([[get_Z(x, y, usable_ace) \n",
        "                      for x in player_range] \n",
        "                     for y in dealer_range])\n",
        "        \n",
        "        surf = ax.plot_surface(X, Y, Z, \n",
        "                               cmap=cm.coolwarm,\n",
        "                               linewidth=0,\n",
        "                               antialiased=True,\n",
        "                               vmin=-1, vmax=1,\n",
        "                               alpha=0.8)\n",
        "        \n",
        "        ax.set_xlabel('Player Sum', fontsize=11)\n",
        "        ax.set_ylabel('Dealer Showing', fontsize=11)\n",
        "        ax.set_zlabel('Value', fontsize=11)\n",
        "        ax.set_zlim(-1, 1)\n",
        "        ax.view_init(elev=25, azim=-130)\n",
        "        return surf\n",
        "    \n",
        "    fig = plt.figure(figsize=(14, 11))\n",
        "    \n",
        "    ax1 = fig.add_subplot(211, projection='3d')\n",
        "    ax1.set_title('State Values WITH Usable Ace', fontsize=13, fontweight='bold', pad=15)\n",
        "    surf1 = create_surface(True, ax1)\n",
        "    fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=10)\n",
        "    \n",
        "    ax2 = fig.add_subplot(212, projection='3d')\n",
        "    ax2.set_title('State Values WITHOUT Usable Ace', fontsize=13, fontweight='bold', pad=15)\n",
        "    surf2 = create_surface(False, ax2)\n",
        "    fig.colorbar(surf2, ax=ax2, shrink=0.5, aspect=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_policy(policy):\n",
        "    def get_action(player_sum, dealer_card, usable_ace):\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        return policy.get(state, 1)\n",
        "    \n",
        "    def create_heatmap(usable_ace, ax):\n",
        "        player_range = range(12, 22)\n",
        "        dealer_range = range(1, 11)\n",
        "        \n",
        "        Z = np.array([[get_action(player, dealer, usable_ace)\n",
        "                      for player in player_range]\n",
        "                     for dealer in dealer_range])\n",
        "        \n",
        "        im = ax.imshow(Z, \n",
        "                       cmap='RdYlGn_r',\n",
        "                       aspect='auto',\n",
        "                       vmin=0, vmax=1,\n",
        "                       extent=[11.5, 21.5, 0.5, 10.5],\n",
        "                       origin='lower',\n",
        "                       interpolation='nearest')\n",
        "        \n",
        "        ax.set_xticks(range(12, 22))\n",
        "        ax.set_yticks(range(1, 11))\n",
        "        ax.set_yticklabels(['A'] + list(range(2, 11)))\n",
        "        ax.set_xlabel('Player Sum', fontsize=11)\n",
        "        ax.set_ylabel('Dealer Showing', fontsize=11)\n",
        "        ax.grid(True, color='black', linewidth=0.5, alpha=0.3)\n",
        "        ax.set_axisbelow(False)\n",
        "        \n",
        "        cbar = plt.colorbar(im, ax=ax, ticks=[0, 1], fraction=0.046, pad=0.04)\n",
        "        cbar.ax.set_yticklabels(['STICK', 'HIT'])\n",
        "        return im\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    ax1.set_title('Policy WITH Usable Ace', fontsize=12, fontweight='bold')\n",
        "    create_heatmap(True, ax1)\n",
        "    \n",
        "    ax2.set_title('Policy WITHOUT Usable Ace', fontsize=12, fontweight='bold')\n",
        "    create_heatmap(False, ax2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Visualization functions ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Running Monte Carlo Experiments\n",
        "\n",
        "Now we execute the complete Monte Carlo learning process. This section demonstrates the distinction between two key concepts:\n",
        "\n",
        "**Arbitrary Exploration Policy:** The stochastic threshold-based policy we defined earlier is used to GENERATE episodes and collect experience. This policy explores the environment but is not necessarily optimal. It serves as our data collection mechanism.\n",
        "\n",
        "**Optimal Policy Extraction:** After learning Q-values from the arbitrary policy's experiences, we extract the optimal policy by selecting the action with highest Q-value in each state: π*(s) = argmax_a Q(s,a). This greedy policy represents what we've learned about the best way to play Blackjack.\n",
        "\n",
        "The learning process flows as: Exploration Policy → Generate Episodes → Learn Q-values → Extract Optimal Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "NUM_EPISODES = 500000\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"LEARNING PHASE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Episodes: {NUM_EPISODES:,}\")\n",
        "print(\"Using arbitrary stochastic policy for exploration\\n\")\n",
        "\n",
        "Q = mc_predict(env, NUM_EPISODES)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"POLICY EXTRACTION PHASE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "V_arbitrary = {}\n",
        "for state, action_values in Q.items():\n",
        "    if state[0] > 18:\n",
        "        V_arbitrary[state] = 0.8 * action_values[0] + 0.2 * action_values[1]\n",
        "    else:\n",
        "        V_arbitrary[state] = 0.2 * action_values[0] + 0.8 * action_values[1]\n",
        "\n",
        "optimal_policy = {}\n",
        "for state, action_values in Q.items():\n",
        "    optimal_policy[state] = np.argmax(action_values)\n",
        "\n",
        "print(\"Optimal policy extracted via greedy selection\\n\")\n",
        "\n",
        "states_count = len(Q)\n",
        "stick_count = sum(1 for a in optimal_policy.values() if a == 0)\n",
        "hit_count = sum(1 for a in optimal_policy.values() if a == 1)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"States explored: {states_count}\")\n",
        "print(f\"\\nOptimal Policy:\")\n",
        "print(f\"  STICK: {stick_count} states ({100*stick_count/states_count:.1f}%)\")\n",
        "print(f\"  HIT:   {hit_count} states ({100*hit_count/states_count:.1f}%)\")\n",
        "print(f\"\\nAverage state value: {np.mean(list(V_arbitrary.values())):.4f}\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Generating 3D value function plots...\\n\")\n",
        "plot_blackjack_values(V_arbitrary)\n",
        "\n",
        "print(\"Value Function Interpretation:\")\n",
        "print(\"  - Red (high): Favorable states\")\n",
        "print(\"  - Blue (low): Unfavorable states\")\n",
        "print(\"  - Peak near sum 20-21: Best positions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Generating optimal policy heatmaps...\\n\")\n",
        "plot_policy(optimal_policy)\n",
        "\n",
        "print(\"Optimal Policy Interpretation:\")\n",
        "print(\"  - Green = STICK\")\n",
        "print(\"  - Red = HIT\")\n",
        "print(\"  - Clear threshold around sum 17-20\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase;\">Key Findings</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>1. Policy Learning:</strong> Used arbitrary exploration policy to generate episodes, then extracted optimal policy from learned Q-values through greedy selection.</p>\n",
        "        <p><strong>2. Exploration vs Exploitation:</strong> Arbitrary policy provides exploration during learning, optimal policy is purely exploitative at decision time.</p>\n",
        "        <p><strong>3. Usable Ace Impact:</strong> Optimal strategy differs significantly with usable ace due to flexibility in avoiding bust.</p>\n",
        "        <p><strong>4. Decision Boundaries:</strong> Clear threshold emerges around sum 17-20 for stick/hit decision, adapting to dealer card.</p>\n",
        "        <p><strong>5. Monte Carlo Strength:</strong> Model-free learning directly from experience converges to near-optimal behavior without environment dynamics.</p>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>Why do we need an exploration policy if we are trying to find the optimal policy?</li>\n",
        "        <li>What would happen if we used a purely greedy policy from the start?</li>\n",
        "        <li>How does First-Visit MC differ from Every-Visit MC in terms of bias and variance?</li>\n",
        "        <li>Why is Monte Carlo particularly suitable for Blackjack compared to Dynamic Programming?</li>\n",
        "        <li>How could we implement epsilon-greedy exploration instead of arbitrary policy?</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 5-1: Blackjack with Monte Carlo Methods</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Next: Lab 5-2 - Monte Carlo Control</p>\n",
        "</div>"
      ]
    }
  ]
}
