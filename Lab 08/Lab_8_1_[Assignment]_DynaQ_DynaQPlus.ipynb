{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_08_1_Dyna_Q_and_Dyna_Q_Plus.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 8-1: Dyna-Q and Dyna-Q+ Algorithms\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">Â© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton and Barto Chapter 8 | 90 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        This lab implements <strong>Dyna-Q</strong> and <strong>Dyna-Q+</strong> algorithms for integrating planning, acting, and learning.\n",
        "        Dyna-Q combines direct reinforcement learning with planning using a learned model. The key innovation is \n",
        "        <strong>simulated experience</strong>: the agent uses its model to generate imaginary transitions for additional learning.\n",
        "        Dyna-Q+ extends this with an <strong>exploration bonus</strong> for long-unvisited state-action pairs, enabling better\n",
        "        adaptation to changing environments.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Implement the Dyna-Q algorithm</li>\n",
        "        <li>Understand model-based planning</li>\n",
        "        <li>Implement Dyna-Q+ with exploration bonus</li>\n",
        "        <li>Compare performance in changing environments</li>\n",
        "        <li>Analyze the exploration-exploitation tradeoff</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Algorithm Components</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Direct RL</code> â†’ Q-learning from real experience</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Model Learning</code> â†’ Store (s,a,r,s') transitions</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Planning</code> â†’ Q-learning from simulated experience</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Exploration Bonus</code> â†’ ÎºâˆšÏ„ for unvisited pairs (Q+)</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 1: Environment Setup and Dependencies</h2>\n",
        "</div>\n",
        "\n",
        "We'll be using the **Shortcut Maze Environment** where the agent must navigate from start (S) to goal (G). After 3000 timesteps, a shortcut opens up, testing the agent's ability to discover and exploit new opportunities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 1: Import Libraries and Clone Repository\n",
        "\n",
        "Purpose:\n",
        "  - Clone the maze environment repository\n",
        "  - Import all required libraries for Dyna algorithms\n",
        "  - Set up RL-Glue framework for agent-environment interaction\n",
        "\n",
        "Key Libraries:\n",
        "  - numpy: Numerical operations and array handling\n",
        "  - matplotlib: Visualization of results and state visitations\n",
        "  - rlglue: Framework for RL experiments\n",
        "  - jdc: Jupyter cell magic for class definitions\n",
        "\n",
        "Environment Details:\n",
        "  - Shortcut Maze: 6x9 grid world\n",
        "  - Actions: Up, Down, Left, Right (deterministic)\n",
        "  - Reward: +1 at goal, 0 elsewhere\n",
        "  - Discount: Î³ = 0.95\n",
        "\"\"\"\n",
        "\n",
        "# Clone the repository with maze environment\n",
        "!git clone https://github.com/mdehghani86/MazeExampleRep.git\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Install required packages\n",
        "!pip install jdc rlglue\n",
        "import jdc\n",
        "\n",
        "# Import RL-Glue components\n",
        "from MazeExampleRep.rl_glue import RLGlue\n",
        "from MazeExampleRep.agent import BaseAgent\n",
        "from MazeExampleRep.maze_env import ShortcutMazeEnvironment\n",
        "\n",
        "# Create results directory\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "# Configure matplotlib for better quality figures\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "plt.rcParams.update({'figure.figsize': [8, 5]})\n",
        "\n",
        "print(\"âœ“ Environment setup complete\")\n",
        "print(\"âœ“ Shortcut Maze environment loaded\")\n",
        "print(\"âœ“ Ready to implement Dyna-Q algorithms\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 2: Dyna-Q Algorithm Implementation</h2>\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <img src=\"MazeExampleRep/images/DynaQ.png\" alt=\"Dyna-Q Pseudocode\" style=\"width: 70%; max-width: 700px; border: 2px solid #17a2b8; border-radius: 8px;\">\n",
        "    <p style=\"color: #666; font-size: 12px; margin-top: 10px; font-style: italic;\">Dyna-Q Algorithm Pseudocode</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #e8f5e9; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #4caf50;\">\n",
        "    <h3 style=\"color: #2e7d32; font-size: 14px; margin: 0 0 8px 0;\">Algorithm Steps</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        <strong>1. Action Selection:</strong> Îµ-greedy policy for exploration<br>\n",
        "        <strong>2. Direct RL:</strong> Q-learning update from real experience<br>\n",
        "        <strong>3. Model Learning:</strong> Store observed transitions<br>\n",
        "        <strong>4. Planning:</strong> n simulated Q-learning updates from model\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 2: DynaQAgent Class Initialization\n",
        "\n",
        "Purpose:\n",
        "  - Initialize the Dyna-Q agent with all necessary components\n",
        "  - Set up Q-values, model, and random number generators\n",
        "\n",
        "Key Components:\n",
        "  - q_values: Action-value estimates Q(s,a)\n",
        "  - model: Dictionary storing transitions {s: {a: (s', r)}}\n",
        "  - planning_steps: Number of simulated updates per real step\n",
        "  - Two RNGs: One for action selection, one for planning\n",
        "\"\"\"\n",
        "\n",
        "class DynaQAgent(BaseAgent):\n",
        "\n",
        "    def agent_init(self, agent_info):\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
        "        \n",
        "        # Extract agent parameters\n",
        "        try:\n",
        "            self.num_states = agent_info[\"num_states\"]\n",
        "            self.num_actions = agent_info[\"num_actions\"]\n",
        "        except:\n",
        "            print(\"ERROR: num_states and num_actions required in agent_info\")\n",
        "            \n",
        "        self.gamma = agent_info.get(\"discount\", 0.95)\n",
        "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
        "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
        "        self.planning_steps = agent_info.get(\"planning_steps\", 10)\n",
        "\n",
        "        # Initialize random number generators\n",
        "        self.rand_generator = np.random.RandomState(agent_info.get('random_seed', 42))\n",
        "        self.planning_rand_generator = np.random.RandomState(agent_info.get('planning_random_seed', 42))\n",
        "\n",
        "        # Initialize Q-values and model\n",
        "        self.q_values = np.zeros((self.num_states, self.num_actions))\n",
        "        self.actions = list(range(self.num_actions))\n",
        "        self.past_action = -1\n",
        "        self.past_state = -1\n",
        "        \n",
        "        # Model: dictionary of dictionaries\n",
        "        # model[s][a] = (next_state, reward)\n",
        "        self.model = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">ðŸ”§ Hands-On Exercise 1: Model Update</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Implement the model update step. The model stores observed transitions for later planning.\n",
        "        <br><br>\n",
        "        <strong>Hint:</strong> For deterministic environments, simply store the observed (s', r) for each (s, a) pair.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to DynaQAgent\n",
        "\n",
        "# [HANDS-ON EXERCISE 1]\n",
        "\n",
        "def update_model(self, past_state, past_action, state, reward):\n",
        "    \"\"\"\n",
        "    Updates the model with observed transition.\n",
        "    \n",
        "    Args:\n",
        "        past_state (int): Previous state s\n",
        "        past_action (int): Action taken a\n",
        "        state (int): Resulting state s'\n",
        "        reward (float): Observed reward r\n",
        "    \n",
        "    Hint: Create nested dictionary structure if state not yet in model\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ### (1-4 lines)\n",
        "    # Check if past_state exists in model\n",
        "    # If not, create empty dictionary for it\n",
        "    # Store the transition: model[s][a] = (s', r)\n",
        "    \n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">ðŸ”§ Hands-On Exercise 2: Planning Step</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Implement the planning step - the heart of Dyna-Q. Use the model to generate simulated experience and update Q-values.\n",
        "        <br><br>\n",
        "        <strong>Key Points:</strong><br>\n",
        "        â€¢ Randomly sample from previously observed (s,a) pairs<br>\n",
        "        â€¢ Use the model to get (s', r)<br>\n",
        "        â€¢ Apply Q-learning update<br>\n",
        "        â€¢ Terminal states are stored as -1\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to DynaQAgent\n",
        "\n",
        "# [HANDS-ON EXERCISE 2]\n",
        "\n",
        "def planning_step(self):\n",
        "    \"\"\"\n",
        "    Performs planning using the model (indirect RL).\n",
        "    Samples random state-action pairs and updates Q-values.\n",
        "    \n",
        "    CRITICAL: Use self.planning_rand_generator for randomness\n",
        "    \"\"\"\n",
        "    \n",
        "    # ============================================================\n",
        "    # PLANNING LOOP: Repeat for self.planning_steps iterations\n",
        "    # ============================================================\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    for step in range(self.planning_steps):\n",
        "        # Step 1: Randomly select a state from the model\n",
        "        # Hint: Get list of states using list(self.model.keys())\n",
        "        \n",
        "        \n",
        "        # Step 2: Randomly select an action for that state\n",
        "        # Hint: Get available actions using list(self.model[state].keys())\n",
        "        \n",
        "        \n",
        "        # Step 3: Query the model for next state and reward\n",
        "        # Hint: next_state, reward = self.model[state][action]\n",
        "        \n",
        "        \n",
        "        # Step 4: Perform Q-learning update\n",
        "        # Different update for terminal (next_state == -1) vs non-terminal\n",
        "        # Terminal: Q(s,a) += Î±[r - Q(s,a)]\n",
        "        # Non-terminal: Q(s,a) += Î±[r + Î³*max(Q(s',Â·)) - Q(s,a)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    ### END YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to DynaQAgent\n",
        "\n",
        "# Helper functions for action selection\n",
        "\n",
        "def argmax(self, q_values):\n",
        "    \"\"\"Argmax with random tie-breaking.\"\"\"\n",
        "    top = float(\"-inf\")\n",
        "    ties = []\n",
        "\n",
        "    for i in range(len(q_values)):\n",
        "        if q_values[i] > top:\n",
        "            top = q_values[i]\n",
        "            ties = []\n",
        "        if q_values[i] == top:\n",
        "            ties.append(i)\n",
        "\n",
        "    return self.rand_generator.choice(ties)\n",
        "\n",
        "def choose_action_egreedy(self, state):\n",
        "    \"\"\"Îµ-greedy action selection.\"\"\"\n",
        "    if self.rand_generator.rand() < self.epsilon:\n",
        "        action = self.rand_generator.choice(self.actions)\n",
        "    else:\n",
        "        values = self.q_values[state]\n",
        "        action = self.argmax(values)\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">ðŸ”§ Hands-On Exercise 3: Agent Step Methods</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Complete the agent's step methods to integrate all Dyna-Q components.\n",
        "        <br><br>\n",
        "        <strong>Order of operations:</strong><br>\n",
        "        1. Direct RL update (Q-learning)<br>\n",
        "        2. Update model<br>\n",
        "        3. Planning steps<br>\n",
        "        4. Select next action\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to DynaQAgent\n",
        "\n",
        "# [HANDS-ON EXERCISE 3]\n",
        "\n",
        "def agent_start(self, state):\n",
        "    \"\"\"First action selection in episode.\"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ### (2 lines)\n",
        "    # Select action using Îµ-greedy\n",
        "    # Store state and action for later use\n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "    \n",
        "    return self.past_action\n",
        "\n",
        "def agent_step(self, reward, state):\n",
        "    \"\"\"Main learning step combining all Dyna-Q components.\"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Step 1: Direct RL - Q-learning update\n",
        "    # Q(s,a) += Î±[r + Î³*max(Q(s',Â·)) - Q(s,a)]\n",
        "    \n",
        "    \n",
        "    # Step 2: Update the model\n",
        "    \n",
        "    \n",
        "    # Step 3: Planning\n",
        "    \n",
        "    \n",
        "    # Step 4: Select next action\n",
        "    \n",
        "    \n",
        "    # Step 5: Store current state and action\n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "    \n",
        "    return self.past_action\n",
        "\n",
        "def agent_end(self, reward):\n",
        "    \"\"\"Handle terminal state.\"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Step 1: Q-learning update for terminal state\n",
        "    # Q(s,a) += Î±[r - Q(s,a)]\n",
        "    \n",
        "    \n",
        "    # Step 2: Update model (use -1 for terminal state)\n",
        "    \n",
        "    \n",
        "    # Step 3: Final planning step\n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 3: Running Dyna-Q Experiments</h2>\n",
        "</div>\n",
        "\n",
        "Let's test the Dyna-Q agent with different numbers of planning steps to see how planning improves learning speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 3: Experiment Runner Functions\n",
        "\n",
        "Purpose:\n",
        "  - Define functions to run experiments and visualize results\n",
        "  - Compare different planning step values\n",
        "\n",
        "Metrics:\n",
        "  - Steps per episode (lower is better)\n",
        "  - Cumulative reward over time\n",
        "  - State visitation heatmaps\n",
        "\"\"\"\n",
        "\n",
        "def run_experiment(env, agent, env_parameters, agent_parameters, exp_parameters):\n",
        "    \"\"\"Run experiment with multiple planning step values.\"\"\"\n",
        "    \n",
        "    # Extract parameters\n",
        "    num_runs = exp_parameters['num_runs']\n",
        "    num_episodes = exp_parameters['num_episodes']\n",
        "    planning_steps_all = agent_parameters['planning_steps']\n",
        "\n",
        "    env_info = env_parameters                     \n",
        "    agent_info = {\n",
        "        \"num_states\": agent_parameters[\"num_states\"],\n",
        "        \"num_actions\": agent_parameters[\"num_actions\"],\n",
        "        \"epsilon\": agent_parameters[\"epsilon\"], \n",
        "        \"discount\": env_parameters[\"discount\"],\n",
        "        \"step_size\": agent_parameters[\"step_size\"]\n",
        "    }\n",
        "\n",
        "    all_averages = np.zeros((len(planning_steps_all), num_runs, num_episodes))\n",
        "    log_data = {'planning_steps_all': planning_steps_all}\n",
        "\n",
        "    for idx, planning_steps in enumerate(planning_steps_all):\n",
        "        print(f'Planning steps: {planning_steps}')\n",
        "        agent_info[\"planning_steps\"] = planning_steps  \n",
        "\n",
        "        for i in tqdm(range(num_runs)):\n",
        "            agent_info['random_seed'] = i\n",
        "            agent_info['planning_random_seed'] = i\n",
        "\n",
        "            rl_glue = RLGlue(env, agent)\n",
        "            rl_glue.rl_init(agent_info, env_info)\n",
        "\n",
        "            for j in range(num_episodes):\n",
        "                rl_glue.rl_start()\n",
        "                is_terminal = False\n",
        "                num_steps = 0\n",
        "                \n",
        "                while not is_terminal:\n",
        "                    reward, _, action, is_terminal = rl_glue.rl_step()\n",
        "                    num_steps += 1\n",
        "\n",
        "                all_averages[idx][i][j] = num_steps\n",
        "\n",
        "    log_data['all_averages'] = all_averages\n",
        "    np.save(\"results/Dyna-Q_planning_steps\", log_data)\n",
        "\n",
        "def plot_steps_per_episode(file_path):\n",
        "    \"\"\"Plot learning curves for different planning steps.\"\"\"\n",
        "    \n",
        "    data = np.load(file_path, allow_pickle=True).item()\n",
        "    all_averages = data['all_averages']\n",
        "    planning_steps_all = data['planning_steps_all']\n",
        "\n",
        "    for i, planning_steps in enumerate(planning_steps_all):\n",
        "        plt.plot(np.mean(all_averages[i], axis=0), \n",
        "                label=f'Planning steps = {planning_steps}')\n",
        "\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Steps\\nper\\nepisode', rotation=0, labelpad=40)\n",
        "    plt.axhline(y=16, linestyle='--', color='grey', alpha=0.4, \n",
        "               label='Optimal')\n",
        "    plt.title('Dyna-Q Learning Performance')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 4: Run Dyna-Q Experiment\n",
        "\n",
        "Purpose:\n",
        "  - Test Dyna-Q with 0, 5, and 50 planning steps\n",
        "  - Compare learning efficiency\n",
        "\n",
        "Expected Results:\n",
        "  - More planning steps â†’ Faster learning\n",
        "  - n=0 is standard Q-learning (no planning)\n",
        "  - n=50 should converge quickly\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================\n",
        "# EXPERIMENT CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "# Experiment parameters\n",
        "experiment_parameters = {\n",
        "    \"num_runs\": 30,        # Average over 30 runs\n",
        "    \"num_episodes\": 40,    # Episodes per run\n",
        "}\n",
        "\n",
        "# Environment parameters\n",
        "environment_parameters = { \n",
        "    \"discount\": 0.95,\n",
        "}\n",
        "\n",
        "# Agent parameters\n",
        "agent_parameters = {  \n",
        "    \"num_states\": 54,       # 6x9 grid\n",
        "    \"num_actions\": 4,       # Up, Down, Left, Right\n",
        "    \"epsilon\": 0.1,         # Exploration rate\n",
        "    \"step_size\": 0.125,     # Learning rate\n",
        "    \"planning_steps\": [0, 5, 50]  # Compare different values\n",
        "}\n",
        "\n",
        "print(\"Running Dyna-Q experiments...\")\n",
        "print(\"This will take 2-3 minutes\\n\")\n",
        "\n",
        "# Run experiment\n",
        "current_env = ShortcutMazeEnvironment\n",
        "current_agent = DynaQAgent\n",
        "\n",
        "run_experiment(current_env, current_agent, environment_parameters, \n",
        "              agent_parameters, experiment_parameters)\n",
        "\n",
        "# Plot results\n",
        "plot_steps_per_episode('results/Dyna-Q_planning_steps.npy')\n",
        "\n",
        "# Save results\n",
        "shutil.make_archive('results', 'zip', 'results');\n",
        "\n",
        "print(\"\\nâœ“ Experiment complete!\")\n",
        "print(\"Notice how more planning steps lead to faster learning.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 4: Dyna-Q+ with Exploration Bonus</h2>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #e8f5e9; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #4caf50;\">\n",
        "    <h3 style=\"color: #2e7d32; font-size: 14px; margin: 0 0 8px 0;\">Dyna-Q+ Enhancements</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        <strong>1. Exploration Bonus:</strong> Add ÎºâˆšÏ„(s,a) to rewards during planning<br>\n",
        "        <strong>2. Complete Model:</strong> Initialize unvisited actions with self-loops<br>\n",
        "        <strong>3. Time Tracking:</strong> Count steps since last visit for each (s,a)<br><br>\n",
        "        This encourages revisiting long-unvisited state-action pairs, helping discover environment changes.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 5: DynaQPlusAgent Initialization\n",
        "\n",
        "Purpose:\n",
        "  - Initialize Dyna-Q+ with exploration bonus components\n",
        "  - Add time-since-visit tracking (tau)\n",
        "  - Add exploration bonus parameter (kappa)\n",
        "\n",
        "New Components:\n",
        "  - tau[s,a]: Steps since last visit\n",
        "  - kappa: Scaling factor for exploration bonus\n",
        "\"\"\"\n",
        "\n",
        "class DynaQPlusAgent(BaseAgent):\n",
        "    \n",
        "    def agent_init(self, agent_info):\n",
        "        \"\"\"Initialize Dyna-Q+ agent.\"\"\"\n",
        "        \n",
        "        # Same initialization as Dyna-Q\n",
        "        try:\n",
        "            self.num_states = agent_info[\"num_states\"]\n",
        "            self.num_actions = agent_info[\"num_actions\"]\n",
        "        except:\n",
        "            print(\"ERROR: num_states and num_actions required\")\n",
        "            \n",
        "        self.gamma = agent_info.get(\"discount\", 0.95)\n",
        "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
        "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
        "        self.planning_steps = agent_info.get(\"planning_steps\", 10)\n",
        "        \n",
        "        # ============================================================\n",
        "        # NEW FOR DYNA-Q+: Exploration bonus parameter\n",
        "        # ============================================================\n",
        "        self.kappa = agent_info.get(\"kappa\", 0.001)\n",
        "\n",
        "        # Random number generators\n",
        "        self.rand_generator = np.random.RandomState(agent_info.get('random_seed', 42))\n",
        "        self.planning_rand_generator = np.random.RandomState(agent_info.get('planning_random_seed', 42))\n",
        "\n",
        "        # Initialize Q-values and time tracking\n",
        "        self.q_values = np.zeros((self.num_states, self.num_actions))\n",
        "        \n",
        "        # ============================================================\n",
        "        # NEW FOR DYNA-Q+: Track time since last visit\n",
        "        # ============================================================\n",
        "        self.tau = np.zeros((self.num_states, self.num_actions))\n",
        "        \n",
        "        self.actions = list(range(self.num_actions))\n",
        "        self.past_action = -1\n",
        "        self.past_state = -1\n",
        "        self.model = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">ðŸ”§ Hands-On Exercise 4: Dyna-Q+ Model Update</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Modify model update for Dyna-Q+: when visiting a state for the first time,\n",
        "        add ALL actions to the model (unvisited actions loop back with reward 0).\n",
        "        <br><br>\n",
        "        <strong>Hint:</strong> This ensures all actions can be selected during planning.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to DynaQPlusAgent\n",
        "\n",
        "# [HANDS-ON EXERCISE 4]\n",
        "\n",
        "def update_model(self, past_state, past_action, state, reward):\n",
        "    \"\"\"\n",
        "    Update model with complete action set for new states.\n",
        "    \n",
        "    CRITICAL DIFFERENCE from Dyna-Q:\n",
        "    - When adding a new state, initialize ALL actions\n",
        "    - Unvisited actions: self-loop with reward 0\n",
        "    \"\"\"\n",
        "    \n",
        "    if past_state not in self.model:\n",
        "        self.model[past_state] = {past_action: (state, reward)}\n",
        "        \n",
        "        ### START YOUR CODE HERE ### (3 lines)\n",
        "        # Add all other actions as self-loops with reward 0\n",
        "        # Hint: Loop through self.actions\n",
        "        # For actions != past_action: model[past_state][action] = (past_state, 0)\n",
        "        \n",
        "        \n",
        "        \n",
        "        ### END YOUR CODE HERE ###\n",
        "    else:\n",
        "        self.model[past_state][past_action] = (state, reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">ðŸ”§ Hands-On Exercise 5: Planning with Exploration Bonus</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Implement planning with exploration bonus: reward + ÎºâˆšÏ„(s,a).\n",
        "        <br><br>\n",
        "        <strong>Key insight:</strong> Actions not visited recently get bonus reward during planning,\n",
        "        encouraging exploration of potentially changed areas.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to DynaQPlusAgent\n",
        "\n",
        "# [HANDS-ON EXERCISE 5]\n",
        "\n",
        "def planning_step(self):\n",
        "    \"\"\"\n",
        "    Planning with exploration bonus.\n",
        "    Adds ÎºâˆšÏ„(s,a) to reward during simulated updates.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    for step in range(self.planning_steps):\n",
        "        # Step 1: Sample random state\n",
        "        \n",
        "        \n",
        "        # Step 2: Sample random action for that state\n",
        "        \n",
        "        \n",
        "        # Step 3: Get model prediction\n",
        "        \n",
        "        \n",
        "        # Step 4: Add exploration bonus\n",
        "        # bonus_reward = reward + self.kappa * np.sqrt(self.tau[state, action])\n",
        "        \n",
        "        \n",
        "        # Step 5: Q-learning update with bonus reward\n",
        "        # Remember to handle terminal states (next_state == -1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    ### END YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">ðŸ”§ Hands-On Exercise 6: Complete Dyna-Q+ Agent</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Complete the agent methods with time tracking.\n",
        "        <br><br>\n",
        "        <strong>Critical:</strong> Update tau (time since visit) correctly:<br>\n",
        "        â€¢ Increment all tau values each step<br>\n",
        "        â€¢ Reset tau[s,a] = 0 when (s,a) is visited\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to DynaQPlusAgent\n",
        "\n",
        "# Helper functions (same as Dyna-Q)\n",
        "def argmax(self, q_values):\n",
        "    \"\"\"Argmax with random tie-breaking.\"\"\"\n",
        "    top = float(\"-inf\")\n",
        "    ties = []\n",
        "    for i in range(len(q_values)):\n",
        "        if q_values[i] > top:\n",
        "            top = q_values[i]\n",
        "            ties = []\n",
        "        if q_values[i] == top:\n",
        "            ties.append(i)\n",
        "    return self.rand_generator.choice(ties)\n",
        "\n",
        "def choose_action_egreedy(self, state):\n",
        "    \"\"\"Îµ-greedy action selection.\"\"\"\n",
        "    if self.rand_generator.rand() < self.epsilon:\n",
        "        action = self.rand_generator.choice(self.actions)\n",
        "    else:\n",
        "        values = self.q_values[state]\n",
        "        action = self.argmax(values)\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to DynaQPlusAgent\n",
        "\n",
        "# [HANDS-ON EXERCISE 6]\n",
        "\n",
        "def agent_start(self, state):\n",
        "    \"\"\"First action - no tau update yet.\"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ### (2 lines)\n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "    \n",
        "    return self.past_action\n",
        "\n",
        "def agent_step(self, reward, state):\n",
        "    \"\"\"Main step with time tracking.\"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Step 1: Update tau (time since visit)\n",
        "    # Increment all tau values\n",
        "    \n",
        "    \n",
        "    # Reset tau for visited (s,a) pair\n",
        "    \n",
        "    \n",
        "    # Step 2: Direct RL update\n",
        "    \n",
        "    \n",
        "    # Step 3: Update model\n",
        "    \n",
        "    \n",
        "    # Step 4: Planning\n",
        "    \n",
        "    \n",
        "    # Step 5: Select next action\n",
        "    \n",
        "    \n",
        "    # Step 6: Store state and action\n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "    \n",
        "    return self.past_action\n",
        "\n",
        "def agent_end(self, reward):\n",
        "    \"\"\"Terminal state handling.\"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Update tau\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Direct RL update\n",
        "    \n",
        "    \n",
        "    # Update model\n",
        "    \n",
        "    \n",
        "    # Planning\n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 5: Testing in Changing Environments</h2>\n",
        "</div>\n",
        "\n",
        "Now we'll test both algorithms in an environment where a shortcut opens after 3000 steps.\n",
        "Dyna-Q+ should discover and exploit the new path, while Dyna-Q may get stuck with its old model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 6: Run Comparison Experiment\n",
        "\n",
        "Purpose:\n",
        "  - Compare Dyna-Q vs Dyna-Q+ in changing environment\n",
        "  - Visualize cumulative reward and state visitations\n",
        "\n",
        "Expected Results:\n",
        "  - Dyna-Q+: Discovers shortcut, increased reward rate\n",
        "  - Dyna-Q: Stuck with old path\n",
        "\"\"\"\n",
        "\n",
        "# [Additional experiment runner functions would go here]\n",
        "# [Similar structure to previous experiments but tracking state visitations]\n",
        "\n",
        "print(\"Running comparison experiment...\")\n",
        "print(\"Environment changes at step 3000 (shortcut opens)\")\n",
        "print(\"\\nThis will take 3-4 minutes...\\n\")\n",
        "\n",
        "# Run experiments and plot results\n",
        "# [Code to run and visualize experiments]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase;\">Key Findings</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <strong>1. Planning Efficiency:</strong> More planning steps dramatically reduce learning time<br><br>\n",
        "        <strong>2. Model-Based Benefits:</strong> Dyna-Q learns from both real and simulated experience<br><br>\n",
        "        <strong>3. Exploration Bonus:</strong> Dyna-Q+ adapts to environment changes through targeted exploration<br><br>\n",
        "        <strong>4. Trade-offs:</strong> Planning requires computation but provides sample efficiency<br><br>\n",
        "        <strong>5. Adaptation:</strong> ÎºâˆšÏ„ bonus helps discover and exploit new opportunities\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>Why does Dyna-Q fail to find the shortcut while Dyna-Q+ succeeds?</li>\n",
        "        <li>How would performance change with different Îº values?</li>\n",
        "        <li>What happens if planning steps are increased to 100 or 1000?</li>\n",
        "        <li>How could prioritized sweeping improve planning efficiency?</li>\n",
        "        <li>What are the computational trade-offs of model-based methods?</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 8-1: Dyna-Q and Dyna-Q+ Algorithms</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Next: Lab 8-2 - Prioritized Sweeping</p>\n",
        "</div>"
      ]
    }
  ]
}
