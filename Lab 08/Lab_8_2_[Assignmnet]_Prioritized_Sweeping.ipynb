{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_08_2_Prioritized_Sweeping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 8-2: Dyna-Q with Prioritized Sweeping\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton and Barto Chapter 8.4 | 60 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        This lab implements <strong>Prioritized Sweeping</strong>, an enhancement to Dyna-Q that focuses computational\n",
        "        effort on state-action pairs where updates would be most beneficial. Instead of randomly selecting states\n",
        "        for planning, prioritized sweeping maintains a <strong>priority queue</strong> ordered by the magnitude of\n",
        "        potential value changes. This leads to more efficient learning, especially in large state spaces where\n",
        "        random sampling would be wasteful.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Understand prioritized sweeping algorithm</li>\n",
        "        <li>Implement priority queue for planning</li>\n",
        "        <li>Track state predecessors efficiently</li>\n",
        "        <li>Compare with standard Dyna-Q</li>\n",
        "        <li>Analyze computational efficiency gains</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Key Components</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Priority Queue</code> → Updates ordered by |ΔQ|</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Predecessors</code> → Track s→s' relationships</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Threshold θ</code> → Minimum priority for updates</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Backward Focus</code> → Propagate value changes</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 1: Environment Setup</h2>\n",
        "</div>\n",
        "\n",
        "We'll use the same Shortcut Maze environment from Lab 8-1. The prioritized sweeping algorithm will demonstrate\n",
        "improved efficiency in finding optimal paths compared to random planning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 1: Import Libraries and Setup Environment\n",
        "\n",
        "Purpose:\n",
        "  - Clone maze environment repository\n",
        "  - Import required libraries including PriorityQueue\n",
        "  - Setup visualization parameters\n",
        "\n",
        "Key Libraries:\n",
        "  - queue.PriorityQueue: Maintains planning queue ordered by priority\n",
        "  - numpy: Array operations and numerical computation\n",
        "  - matplotlib: Performance visualization\n",
        "  - rlglue: Agent-environment interaction framework\n",
        "\n",
        "Environment:\n",
        "  - 6x9 grid maze with walls\n",
        "  - Deterministic transitions\n",
        "  - +1 reward at goal, 0 elsewhere\n",
        "\"\"\"\n",
        "\n",
        "# Clone the repository with maze environment\n",
        "!git clone https://github.com/mdehghani86/MazeExampleRep.git\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, shutil\n",
        "from tqdm import tqdm\n",
        "from queue import PriorityQueue  # Critical for prioritized sweeping\n",
        "\n",
        "# Install and import RL-Glue\n",
        "!pip install jdc rlglue\n",
        "import jdc\n",
        "\n",
        "from MazeExampleRep.rl_glue import RLGlue\n",
        "from MazeExampleRep.agent import BaseAgent\n",
        "from MazeExampleRep.maze_env import ShortcutMazeEnvironment\n",
        "\n",
        "# Create results directory\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "plt.rcParams.update({'figure.figsize': [8, 5]})\n",
        "\n",
        "print(\"✓ Environment setup complete\")\n",
        "print(\"✓ Priority queue library loaded\")\n",
        "print(\"✓ Ready for prioritized sweeping implementation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 2: Prioritized Sweeping Algorithm</h2>\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <img src=\"MazeExampleRep/images/prioritized_sweeping.png\" alt=\"Prioritized Sweeping Pseudocode\" \n",
        "         style=\"width: 70%; max-width: 700px; border: 2px solid #17a2b8; border-radius: 8px;\">\n",
        "    <p style=\"color: #666; font-size: 12px; margin-top: 10px; font-style: italic;\">Prioritized Sweeping Algorithm from Sutton & Barto</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #e8f5e9; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #4caf50;\">\n",
        "    <h3 style=\"color: #2e7d32; font-size: 14px; margin: 0 0 8px 0;\">Algorithm Enhancements</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        <strong>1. Priority Queue:</strong> States prioritized by |r + γ·max Q(s',·) - Q(s,a)|<br>\n",
        "        <strong>2. Predecessor Tracking:</strong> Maintain reverse model for backward propagation<br>\n",
        "        <strong>3. Threshold θ:</strong> Only queue updates with priority > θ<br>\n",
        "        <strong>4. Focused Updates:</strong> Process highest-priority states first<br>\n",
        "        <strong>5. Cascade Effect:</strong> Updates propagate to predecessor states\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 2: PriorityAgent Class Initialization\n",
        "\n",
        "Purpose:\n",
        "  - Initialize agent with prioritized sweeping components\n",
        "  - Setup priority queue and predecessor tracking\n",
        "  - Configure threshold parameter θ\n",
        "\n",
        "Key Data Structures:\n",
        "  - queue: PriorityQueue for planning order\n",
        "  - predecessors: Dict mapping states to their predecessors\n",
        "  - theta: Threshold for minimum priority\n",
        "  - model: Standard Dyna-Q model {s: {a: (s', r)}}\n",
        "\n",
        "CRITICAL: Priority queue uses negative priorities (min-heap)\n",
        "\"\"\"\n",
        "\n",
        "class PriorityAgent(BaseAgent):\n",
        "\n",
        "    def agent_init(self, agent_info):\n",
        "        \"\"\"Initialize prioritized sweeping agent.\"\"\"\n",
        "        \n",
        "        # Extract standard parameters\n",
        "        try:\n",
        "            self.num_states = agent_info[\"num_states\"]\n",
        "            self.num_actions = agent_info[\"num_actions\"]\n",
        "        except:\n",
        "            print(\"ERROR: num_states and num_actions required\")\n",
        "            \n",
        "        self.gamma = agent_info.get(\"discount\", 0.95)\n",
        "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
        "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
        "        self.planning_steps = agent_info.get(\"planning_steps\", 10)\n",
        "\n",
        "        # Random number generators\n",
        "        self.rand_generator = np.random.RandomState(agent_info.get('random_seed', 50))\n",
        "        self.planning_rand_generator = np.random.RandomState(\n",
        "            agent_info.get('planning_random_seed', 50))\n",
        "\n",
        "        # Standard Dyna-Q components\n",
        "        self.q_values = np.zeros((self.num_states, self.num_actions))\n",
        "        self.actions = list(range(self.num_actions))\n",
        "        self.past_action = -1\n",
        "        self.past_state = -1\n",
        "        self.model = {}\n",
        "        \n",
        "        # ============================================================\n",
        "        # PRIORITIZED SWEEPING COMPONENTS\n",
        "        # ============================================================\n",
        "        self.theta = agent_info.get(\"theta\", 0.05)  # Priority threshold\n",
        "        self.queue = PriorityQueue()                # Priority queue for planning\n",
        "        self.predecessors = {}                      # s' -> [(s, a), ...]\n",
        "        \n",
        "        print(f\"Initialized with θ = {self.theta}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">🔧 Hands-On Exercise 1: Core Components</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Implement the three core components of prioritized sweeping:\n",
        "        model update, predecessor tracking, and priority queue management.\n",
        "        <br><br>\n",
        "        <strong>Key insight:</strong> These components work together to focus planning on important updates.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to PriorityAgent\n",
        "\n",
        "# [HANDS-ON EXERCISE 1A: Model Update]\n",
        "\n",
        "def update_model(self, past_state, past_action, state, reward):\n",
        "    \"\"\"\n",
        "    Update the model (same as Dyna-Q).\n",
        "    \n",
        "    Args:\n",
        "        past_state (int): Previous state s\n",
        "        past_action (int): Action taken a\n",
        "        state (int): Resulting state s'\n",
        "        reward (float): Observed reward r\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ### (1-4 lines)\n",
        "    # Hint: Create nested dict if state not in model\n",
        "    # Store transition: model[s][a] = (s', r)\n",
        "    \n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to PriorityAgent\n",
        "\n",
        "# [HANDS-ON EXERCISE 1B: Predecessor Tracking]\n",
        "\n",
        "def update_predecessors(self, past_state, past_action, state):\n",
        "    \"\"\"\n",
        "    Track which state-action pairs lead to each state.\n",
        "    \n",
        "    Args:\n",
        "        past_state (int): Previous state s\n",
        "        past_action (int): Action taken a  \n",
        "        state (int): Resulting state s'\n",
        "        \n",
        "    Hint: Store (s,a) pairs that lead to s' for backward propagation\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ### (1-4 lines)\n",
        "    # Create list for state if not exists\n",
        "    # Append (past_state, past_action) if not already there\n",
        "    \n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to PriorityAgent\n",
        "\n",
        "# [HANDS-ON EXERCISE 1C: Priority Queue Update]\n",
        "\n",
        "def update_queue(self, past_state, past_action, state, reward):\n",
        "    \"\"\"\n",
        "    Add state-action pair to queue with appropriate priority.\n",
        "    \n",
        "    Priority = |r + γ·max Q(s',·) - Q(s,a)|\n",
        "    Note: Use negative priority for min-heap behavior\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ### (2-4 lines)\n",
        "    # Step 1: Calculate TD error magnitude\n",
        "    # priority = |reward + gamma * max(Q[state]) - Q[past_state][past_action]|\n",
        "    \n",
        "    \n",
        "    # Step 2: If priority > theta, add to queue with NEGATIVE priority\n",
        "    # self.queue.put((-priority, (past_state, past_action)))\n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">🔧 Hands-On Exercise 2: Planning Step</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Implement the prioritized planning step - the core innovation.\n",
        "        Process highest-priority updates and propagate changes to predecessors.\n",
        "        <br><br>\n",
        "        <strong>Critical:</strong> Updates cascade backward through predecessor states.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to PriorityAgent\n",
        "\n",
        "# [HANDS-ON EXERCISE 2: Prioritized Planning]\n",
        "\n",
        "def planning_step(self):\n",
        "    \"\"\"\n",
        "    Perform prioritized sweeping planning steps.\n",
        "    Process queue in priority order and propagate to predecessors.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    \n",
        "    # Planning loop - continue for planning_steps or until queue empty\n",
        "    for _ in range(self.planning_steps):\n",
        "        if self.queue.empty():\n",
        "            break\n",
        "            \n",
        "        # Step 1: Get highest priority state-action from queue\n",
        "        # priority, (state, action) = self.queue.get()\n",
        "        \n",
        "        \n",
        "        # Step 2: Get model prediction\n",
        "        # next_state, reward = self.model[state][action]\n",
        "        \n",
        "        \n",
        "        # Step 3: Q-learning update\n",
        "        # Handle terminal (next_state == -1) and non-terminal cases\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        # Step 4: Update all predecessors of current state\n",
        "        # For each (s, a) that leads to current state:\n",
        "        #   Calculate priority for predecessor\n",
        "        #   Add to queue if priority > theta\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    ### END YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to PriorityAgent\n",
        "\n",
        "# Helper functions for action selection\n",
        "\n",
        "def argmax(self, q_values):\n",
        "    \"\"\"Argmax with random tie-breaking.\"\"\"\n",
        "    top = float(\"-inf\")\n",
        "    ties = []\n",
        "    for i in range(len(q_values)):\n",
        "        if q_values[i] > top:\n",
        "            top = q_values[i]\n",
        "            ties = []\n",
        "        if q_values[i] == top:\n",
        "            ties.append(i)\n",
        "    return self.rand_generator.choice(ties)\n",
        "\n",
        "def choose_action_egreedy(self, state):\n",
        "    \"\"\"ε-greedy action selection.\"\"\"\n",
        "    if self.rand_generator.rand() < self.epsilon:\n",
        "        action = self.rand_generator.choice(self.actions)\n",
        "    else:\n",
        "        values = self.q_values[state]\n",
        "        action = self.argmax(values)\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">🔧 Hands-On Exercise 3: Agent Integration</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Complete the agent methods to integrate all components.\n",
        "        Remember to update model, queue, and predecessors in the correct order.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%add_to PriorityAgent\n",
        "\n",
        "# [HANDS-ON EXERCISE 3: Agent Methods]\n",
        "\n",
        "def agent_start(self, state):\n",
        "    \"\"\"First action selection.\"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ### (2 lines)\n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "    \n",
        "    return self.past_action\n",
        "\n",
        "def agent_step(self, reward, state):\n",
        "    \"\"\"Main learning step with prioritized sweeping.\"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    \n",
        "    # Step 1: Direct RL update\n",
        "    \n",
        "    \n",
        "    # Step 2: Update model\n",
        "    \n",
        "    \n",
        "    # Step 3: Update priority queue\n",
        "    \n",
        "    \n",
        "    # Step 4: Update predecessors\n",
        "    \n",
        "    \n",
        "    # Step 5: Planning\n",
        "    \n",
        "    \n",
        "    # Step 6: Select next action\n",
        "    \n",
        "    \n",
        "    # Step 7: Store state and action\n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###\n",
        "    \n",
        "    return self.past_action\n",
        "\n",
        "def agent_end(self, reward):\n",
        "    \"\"\"Terminal state handling.\"\"\"\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    \n",
        "    # Similar to agent_step but with terminal state (-1)\n",
        "    # Remember: No predecessors for terminal state\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 3: Running Experiments</h2>\n",
        "</div>\n",
        "\n",
        "Test the prioritized sweeping agent and compare its performance with standard Dyna-Q."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 3: Experiment Runner\n",
        "\n",
        "Purpose:\n",
        "  - Run prioritized sweeping experiments\n",
        "  - Measure steps per episode\n",
        "  - Compare learning efficiency\n",
        "\n",
        "Metrics:\n",
        "  - Steps to goal (lower is better)\n",
        "  - Learning speed\n",
        "  - Planning efficiency\n",
        "\"\"\"\n",
        "\n",
        "def run_experiment(env, agent, env_parameters, agent_parameters, exp_parameters):\n",
        "    \"\"\"Run experiment with prioritized sweeping.\"\"\"\n",
        "    \n",
        "    # Extract parameters\n",
        "    num_runs = exp_parameters['num_runs']\n",
        "    num_episodes = exp_parameters['num_episodes']\n",
        "    planning_steps_all = agent_parameters['planning_steps']\n",
        "\n",
        "    env_info = env_parameters                     \n",
        "    agent_info = {\n",
        "        \"num_states\": agent_parameters[\"num_states\"],\n",
        "        \"num_actions\": agent_parameters[\"num_actions\"],\n",
        "        \"epsilon\": agent_parameters[\"epsilon\"],\n",
        "        \"theta\": agent_parameters[\"theta\"],\n",
        "        \"discount\": env_parameters[\"discount\"],\n",
        "        \"step_size\": agent_parameters[\"step_size\"]\n",
        "    }\n",
        "\n",
        "    all_averages = np.zeros((len(planning_steps_all), num_runs, num_episodes))\n",
        "    log_data = {'planning_steps_all': planning_steps_all}\n",
        "\n",
        "    for idx, planning_steps in enumerate(planning_steps_all):\n",
        "        print(f'Planning steps: {planning_steps}')\n",
        "        agent_info[\"planning_steps\"] = planning_steps  \n",
        "\n",
        "        for i in tqdm(range(num_runs)):\n",
        "            agent_info['random_seed'] = i\n",
        "            agent_info['planning_random_seed'] = i\n",
        "\n",
        "            rl_glue = RLGlue(env, agent)\n",
        "            rl_glue.rl_init(agent_info, env_info)\n",
        "\n",
        "            for j in range(num_episodes):\n",
        "                rl_glue.rl_start()\n",
        "                is_terminal = False\n",
        "                num_steps = 0\n",
        "                \n",
        "                while not is_terminal:\n",
        "                    reward, _, action, is_terminal = rl_glue.rl_step()\n",
        "                    num_steps += 1\n",
        "\n",
        "                all_averages[idx][i][j] = num_steps\n",
        "\n",
        "    log_data['all_averages'] = all_averages\n",
        "    np.save(\"results/Priority-Sweeping_steps\", log_data)\n",
        "    \n",
        "\n",
        "def plot_steps_per_episode(file_path):\n",
        "    \"\"\"Plot learning curves.\"\"\"\n",
        "    \n",
        "    data = np.load(file_path, allow_pickle=True).item()\n",
        "    all_averages = data['all_averages']\n",
        "    planning_steps_all = data['planning_steps_all']\n",
        "\n",
        "    for i, planning_steps in enumerate(planning_steps_all):\n",
        "        plt.plot(np.mean(all_averages[i], axis=0), \n",
        "                label=f'Planning steps = {planning_steps}')\n",
        "\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Steps\\nper\\nepisode', rotation=0, labelpad=40)\n",
        "    plt.axhline(y=16, linestyle='--', color='grey', alpha=0.4,\n",
        "               label='Optimal')\n",
        "    plt.title('Prioritized Sweeping Performance')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 4: Run Prioritized Sweeping Experiment\n",
        "\n",
        "Purpose:\n",
        "  - Execute experiment with optimized parameters\n",
        "  - Visualize learning performance\n",
        "  - Save results for analysis\n",
        "\n",
        "Parameters:\n",
        "  - θ = 0.2: Priority threshold\n",
        "  - Planning steps = 25\n",
        "  - ε = 0.1: Exploration rate\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================\n",
        "# EXPERIMENT CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "# Experiment parameters\n",
        "experiment_parameters = {\n",
        "    \"num_runs\": 20,        # Number of independent runs\n",
        "    \"num_episodes\": 30,    # Episodes per run\n",
        "}\n",
        "\n",
        "# Environment parameters\n",
        "environment_parameters = { \n",
        "    \"discount\": 0.95,\n",
        "}\n",
        "\n",
        "# Agent parameters\n",
        "agent_parameters = {  \n",
        "    \"num_states\": 54,      # 6x9 grid\n",
        "    \"num_actions\": 4,      # 4 directions\n",
        "    \"epsilon\": 0.1, \n",
        "    \"step_size\": 0.125,\n",
        "    \"theta\": 0.2,          # Priority threshold\n",
        "    \"planning_steps\": [25] # Focus computational effort\n",
        "}\n",
        "\n",
        "print(\"Running prioritized sweeping experiment...\")\n",
        "print(f\"Configuration: θ={agent_parameters['theta']}, \"\n",
        "      f\"planning={agent_parameters['planning_steps'][0]}\\n\")\n",
        "\n",
        "# Run experiment\n",
        "current_env = ShortcutMazeEnvironment\n",
        "current_agent = PriorityAgent\n",
        "\n",
        "run_experiment(current_env, current_agent, environment_parameters, \n",
        "              agent_parameters, experiment_parameters)\n",
        "\n",
        "# Plot results\n",
        "plot_steps_per_episode('results/Priority-Sweeping_steps.npy')\n",
        "\n",
        "# Save results\n",
        "shutil.make_archive('results', 'zip', 'results')\n",
        "\n",
        "print(\"\\n✓ Experiment complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 4: Analysis and Observations</h2>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f4f8; padding: 20px; margin: 20px 0; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 15px 0;\">📝 Record Your Observations</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0 0 15px 0; font-size: 13px;\">\n",
        "        Based on your experimental results, record your observations about prioritized sweeping performance.\n",
        "        Consider the following aspects in your analysis:\n",
        "    </p>\n",
        "    \n",
        "    <div style=\"background: white; padding: 15px; border-radius: 5px; margin-top: 10px;\">\n",
        "        <p style=\"color: #17a2b8; font-weight: bold; margin: 0 0 10px 0;\">1. Learning Speed Comparison</p>\n",
        "        <div style=\"background: #f8f9fa; padding: 10px; border-left: 2px solid #17a2b8; margin: 10px 0;\">\n",
        "            <em style=\"color: #666; font-size: 12px;\">Compare with standard Dyna-Q from Lab 8-1. How much faster does prioritized sweeping converge?</em>\n",
        "            <div style=\"margin-top: 10px; padding: 10px; border: 1px dashed #ccc; min-height: 60px;\">\n",
        "                <!-- Student observation here -->\n",
        "            </div>\n",
        "        </div>\n",
        "        \n",
        "        <p style=\"color: #17a2b8; font-weight: bold; margin: 20px 0 10px 0;\">2. Computational Efficiency</p>\n",
        "        <div style=\"background: #f8f9fa; padding: 10px; border-left: 2px solid #17a2b8; margin: 10px 0;\">\n",
        "            <em style=\"color: #666; font-size: 12px;\">How does prioritized sweeping achieve better results with the same number of planning steps?</em>\n",
        "            <div style=\"margin-top: 10px; padding: 10px; border: 1px dashed #ccc; min-height: 60px;\">\n",
        "                <!-- Student observation here -->\n",
        "            </div>\n",
        "        </div>\n",
        "        \n",
        "        <p style=\"color: #17a2b8; font-weight: bold; margin: 20px 0 10px 0;\">3. Key Algorithm Insights</p>\n",
        "        <div style=\"background: #f8f9fa; padding: 10px; border-left: 2px solid #17a2b8; margin: 10px 0;\">\n",
        "            <em style=\"color: #666; font-size: 12px;\">What makes prioritized sweeping more effective? Consider the role of the priority queue and backward focusing.</em>\n",
        "            <div style=\"margin-top: 10px; padding: 10px; border: 1px dashed #ccc; min-height: 60px;\">\n",
        "                <!-- Student observation here -->\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase;\">Key Takeaways</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <strong>1. Focused Computation:</strong> Priority queue ensures updates happen where most needed<br><br>\n",
        "        <strong>2. Backward Propagation:</strong> Value changes cascade through predecessor states<br><br>\n",
        "        <strong>3. Threshold θ:</strong> Balances computation vs. accuracy<br><br>\n",
        "        <strong>4. Efficiency Gain:</strong> Same planning budget achieves faster convergence<br><br>\n",
        "        <strong>5. Scalability:</strong> More important in larger state spaces\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">Questions for Further Exploration</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>How would different θ values affect performance?</li>\n",
        "        <li>What happens in stochastic environments?</li>\n",
        "        <li>How does performance scale with maze size?</li>\n",
        "        <li>Could we combine with Dyna-Q+ exploration bonus?</li>\n",
        "        <li>What are the memory requirements vs. standard Dyna-Q?</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 8-2: Prioritized Sweeping</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Module 8 Complete - Planning and Learning with Tabular Methods</p>\n",
        "</div>"
      ]
    }
  ]
}
