{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Lab_06b_Windy_Gridworld_SARSA_QLearning_TEACHING_VERSION.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 6b: SARSA vs Q-Learning in Windy Gridworld\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapter 6 | Intermediate Level | 75 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        The Windy Gridworld problem, introduced in \n",
        "        <a href=\"http://incompleteideas.net/book/the-book-2nd.html\" style=\"color: #17a2b8;\">Sutton & Barto Example 6.5</a>,\n",
        "        demonstrates the difference between on-policy (SARSA) and off-policy (Q-learning) TD control methods.\n",
        "        In this environment, crosswind affects the agent's movement, creating a stochastic transition model.\n",
        "        We'll implement both algorithms and compare their learning characteristics, exploring how\n",
        "        the on-policy vs off-policy distinction affects convergence and performance.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Implement SARSA (on-policy TD control)</li>\n",
        "        <li>Implement Q-learning (off-policy TD control)</li>\n",
        "        <li>Understand the impact of wind on state transitions</li>\n",
        "        <li>Compare on-policy vs off-policy learning</li>\n",
        "        <li>Analyze convergence rates and final policies</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Key Concepts</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">SARSA</code> → On-policy TD control</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Q-learning</code> → Off-policy TD control</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">ε-greedy</code> → Action selection strategy</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Wind strength</code> → [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Grid size</code> → 7×10 gridworld</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; margin-top: 20px; border-left: 3px solid #17a2b8;\">\n",
        "    <h2 style=\"color: #17a2b8; font-size: 16px; margin: 0 0 8px 0; font-weight: 600;\">Section 1: Environment Setup</h2>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        We begin by setting up the Windy Gridworld environment, a classic benchmark problem for comparing TD control algorithms.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "### The Windy Gridworld Problem\n",
        "\n",
        "**Environment characteristics:**\n",
        "- **Grid**: 7 rows × 10 columns (70 total states)\n",
        "- **Start**: Position (3, 0) - row 3, column 0\n",
        "- **Goal**: Position (3, 7) - row 3, column 7\n",
        "- **Wind**: Upward push in middle columns\n",
        "- **Actions**: 4 standard moves (UP=0, RIGHT=1, DOWN=2, LEFT=3)\n",
        "- **Reward**: -1 per step (encourages finding shortest path)\n",
        "\n",
        "**Wind Pattern (by column):**\n",
        "```\n",
        "Column:  0  1  2  3  4  5  6  7  8  9\n",
        "Wind:    0  0  0  1  1  1  2  2  1  0\n",
        "```\n",
        "\n",
        "**Key Challenge:** The agent must learn to compensate for wind while finding the optimal path to the goal.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 1: Install Dependencies and Import Libraries\n",
        "Purpose: Set up the computational environment for RL experiments\n",
        "\n",
        "TEACHING NOTE:\n",
        "- We use gymnasium (modern fork of gym) for better compatibility\n",
        "- Fallback to gym 0.26.2 if gymnasium unavailable\n",
        "- Action constants (UP, RIGHT, DOWN, LEFT) improve code readability\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Install gymnasium (modern, maintained version of OpenAI Gym)\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gymnasium\"])\n",
        "    import gymnasium as gym\n",
        "    print(\"✓ Using gymnasium (recommended)\")\n",
        "except:\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gym==0.26.2\"])\n",
        "        import gym\n",
        "        print(\"✓ Using gym 0.26.2 (fallback)\")\n",
        "    except:\n",
        "        print(\"⚠ Using existing gym/gymnasium installation\")\n",
        "        import gym\n",
        "\n",
        "# Core scientific computing libraries\n",
        "import numpy as np                          # Numerical operations\n",
        "from collections import namedtuple, defaultdict  # Data structures\n",
        "import matplotlib.pyplot as plt             # Plotting\n",
        "import pandas as pd                         # Data analysis (for smoothing)\n",
        "import itertools                            # Iteration tools\n",
        "from IPython.display import display, HTML, clear_output  # Jupyter display\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure matplotlib for publication-quality figures\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Define action constants for improved code readability\n",
        "# These map to integers 0-3 but make the code self-documenting\n",
        "UP = 0      # Move up (decrease row)\n",
        "RIGHT = 1   # Move right (increase column)\n",
        "DOWN = 2    # Move down (increase row)\n",
        "LEFT = 3    # Move left (decrease column)\n",
        "\n",
        "def pretty_print(title, content, style='info'):\n",
        "    \"\"\"\n",
        "    Display formatted output boxes for better visual organization\n",
        "    \n",
        "    TEACHING NOTE:\n",
        "    This function creates HTML-styled output boxes in Jupyter notebooks.\n",
        "    Different styles (info, success, warning, result, note) use different colors\n",
        "    to help students distinguish different types of information.\n",
        "    \n",
        "    Args:\n",
        "        title: Header text for the box\n",
        "        content: Main content (can include HTML tags like <br>, <strong>)\n",
        "        style: Color theme - 'info', 'success', 'warning', 'result', 'note'\n",
        "    \"\"\"\n",
        "    themes = {\n",
        "        'info': {'primary': '#17a2b8', 'secondary': '#0e5a63', 'background': '#f8f9fa'},\n",
        "        'success': {'primary': '#28a745', 'secondary': '#155724', 'background': '#f8fff9'},\n",
        "        'warning': {'primary': '#ffc107', 'secondary': '#e0a800', 'background': '#fffdf5'},\n",
        "        'result': {'primary': '#6f42c1', 'secondary': '#4e2c8e', 'background': '#faf5ff'},\n",
        "        'note': {'primary': '#20c997', 'secondary': '#0d7a5f', 'background': '#f0fdf9'}\n",
        "    }\n",
        "    theme = themes.get(style, themes['info'])\n",
        "    \n",
        "    html = f'''\n",
        "    <div style=\"border-radius: 5px; margin: 10px 0; max-width: 800px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
        "        <div style=\"background: linear-gradient(90deg, {theme['primary']} 0%, {theme['secondary']} 100%); \n",
        "                    padding: 10px 15px; border-radius: 5px 5px 0 0;\">\n",
        "            <strong style=\"color: white; font-size: 14px;\">{title}</strong>\n",
        "        </div>\n",
        "        <div style=\"background: {theme['background']}; padding: 10px 15px; \n",
        "                    border-radius: 0 0 5px 5px; border-left: 3px solid {theme['primary']};\">\n",
        "            <div style=\"color: rgba(0,0,0,0.8); font-size: 12px; line-height: 1.5;\">{content}</div>\n",
        "        </div>\n",
        "    </div>\n",
        "    '''\n",
        "    display(HTML(html))\n",
        "\n",
        "# Test the setup\n",
        "try:\n",
        "    test_env = gym.make('CartPole-v1') if hasattr(gym, 'make') else None\n",
        "    pretty_print(\"✓ Environment Ready\", \n",
        "                 \"Successfully loaded all dependencies<br>\" +\n",
        "                 \"• NumPy, Matplotlib, Pandas loaded<br>\" +\n",
        "                 \"• Gym/Gymnasium available<br>\" +\n",
        "                 \"• pretty_print utility defined<br>\" +\n",
        "                 \"• Action constants defined (UP=0, RIGHT=1, DOWN=2, LEFT=3)\", \n",
        "                 style='success')\n",
        "except Exception as e:\n",
        "    pretty_print(\"⚠ Partial Setup\", \n",
        "                 f\"Some dependencies loaded, but encountered: {str(e)}<br>\" +\n",
        "                 \"You may need to install packages manually\", \n",
        "                 style='warning')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Windy Gridworld Environment\n",
        "\n",
        "Before implementing our algorithms, let's understand the environment structure:\n",
        "\n",
        "**State Space:**\n",
        "- Each cell in the 7×10 grid is a state\n",
        "- States are represented as (row, column) tuples\n",
        "- Total of 70 possible states\n",
        "\n",
        "**Action Space:**\n",
        "- 4 actions: UP, RIGHT, DOWN, LEFT\n",
        "- Agent intends to move in chosen direction\n",
        "- Wind may push agent upward (perpendicular to intended movement)\n",
        "\n",
        "**Transition Dynamics:**\n",
        "```\n",
        "Next State = Intended Movement + Wind Effect\n",
        "```\n",
        "Example: If agent is in column 6 (wind strength = 2) and chooses RIGHT:\n",
        "- Intended: Move right (row stays same, column +1)\n",
        "- Wind effect: Pushed up 2 rows (row -2)\n",
        "- Result: Final position is 2 rows up and 1 column right\n",
        "\n",
        "**Reward Structure:**\n",
        "- -1 for every step (living cost)\n",
        "- Episode ends when goal is reached\n",
        "- This encourages finding the shortest path\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 2: Define Windy Gridworld Environment\n",
        "Purpose: Create custom environment with wind dynamics\n",
        "\n",
        "TEACHING NOTE:\n",
        "This is a custom implementation that works with any gym/gymnasium version.\n",
        "Key learning points:\n",
        "1. Environment must implement: reset(), step(), render()\n",
        "2. step() returns: (next_state, reward, done, info)\n",
        "3. Wind is applied AFTER the agent's intended action\n",
        "4. States are indexed as single integers but internally tracked as (row, col)\n",
        "\"\"\"\n",
        "\n",
        "class WindyGridworldEnv:\n",
        "    \"\"\"\n",
        "    Windy Gridworld Environment (Sutton & Barto, Example 6.5)\n",
        "    \n",
        "    A 7x10 gridworld where wind pushes the agent upward in certain columns.\n",
        "    The agent must learn to compensate for wind while reaching the goal.\n",
        "    \n",
        "    State Space: 70 states (7 rows × 10 columns)\n",
        "    Action Space: 4 actions (UP, RIGHT, DOWN, LEFT)\n",
        "    Start State: (3, 0)\n",
        "    Goal State: (3, 7)\n",
        "    Wind Pattern: [0, 0, 0, 1, 1, 1, 2, 2, 1, 0] by column\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the environment\"\"\"\n",
        "        self.shape = (7, 10)  # (rows, columns)\n",
        "        self.start_state = (3, 0)\n",
        "        self.goal_state = (3, 7)\n",
        "        \n",
        "        # Wind strength for each column (0-9)\n",
        "        # Wind pushes agent UPWARD (decreases row number)\n",
        "        # Column:  0  1  2  3  4  5  6  7  8  9\n",
        "        # Wind:    0  0  0  1  1  1  2  2  1  0\n",
        "        self.winds = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
        "        \n",
        "        # Create action_space and observation_space objects\n",
        "        # (compatible with gym API)\n",
        "        self.action_space = type('obj', (object,), {'n': 4})()\n",
        "        self.observation_space = type('obj', (object,), {\n",
        "            'n': self.shape[0] * self.shape[1]\n",
        "        })()\n",
        "        \n",
        "        self.state = None\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset environment to starting state\n",
        "        \n",
        "        Returns:\n",
        "            state_index: Integer index of start state\n",
        "        \"\"\"\n",
        "        self.state = self.start_state\n",
        "        return self._state_to_index(self.state)\n",
        "    \n",
        "    def _state_to_index(self, state):\n",
        "        \"\"\"\n",
        "        Convert (row, col) tuple to single integer index\n",
        "        \n",
        "        Formula: index = row × num_columns + col\n",
        "        Example: (3, 0) → 3 × 10 + 0 = 30\n",
        "        \"\"\"\n",
        "        return state[0] * self.shape[1] + state[1]\n",
        "    \n",
        "    def _index_to_state(self, index):\n",
        "        \"\"\"\n",
        "        Convert single integer index to (row, col) tuple\n",
        "        \n",
        "        Formula: row = index // num_columns, col = index % num_columns\n",
        "        Example: 30 → (30 // 10, 30 % 10) = (3, 0)\n",
        "        \"\"\"\n",
        "        return (index // self.shape[1], index % self.shape[1])\n",
        "    \n",
        "    def _limit_coordinates(self, coord):\n",
        "        \"\"\"\n",
        "        Keep agent within grid boundaries (clip to valid range)\n",
        "        \n",
        "        TEACHING NOTE:\n",
        "        This prevents the agent from moving off the grid.\n",
        "        If action would take agent outside grid, they stay at the edge.\n",
        "        \"\"\"\n",
        "        row, col = coord\n",
        "        row = max(0, min(row, self.shape[0] - 1))  # Clip row to [0, 6]\n",
        "        col = max(0, min(col, self.shape[1] - 1))  # Clip col to [0, 9]\n",
        "        return (row, col)\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute action and return result\n",
        "        \n",
        "        TEACHING NOTE - Step-by-step process:\n",
        "        1. Apply agent's intended action (move in chosen direction)\n",
        "        2. Apply wind effect (pushes upward based on current column)\n",
        "        3. Clip to grid boundaries\n",
        "        4. Check if goal reached\n",
        "        5. Return (next_state, reward, done, info)\n",
        "        \n",
        "        Args:\n",
        "            action: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
        "        \n",
        "        Returns:\n",
        "            next_state: Integer state index\n",
        "            reward: -1.0 (cost per step)\n",
        "            done: True if goal reached, False otherwise\n",
        "            info: Empty dict (required by gym API)\n",
        "        \"\"\"\n",
        "        row, col = self.state\n",
        "        \n",
        "        # Step 1: Apply agent's intended action\n",
        "        if action == UP:\n",
        "            row -= 1  # Move up (decrease row)\n",
        "        elif action == RIGHT:\n",
        "            col += 1  # Move right (increase column)\n",
        "        elif action == DOWN:\n",
        "            row += 1  # Move down (increase row)\n",
        "        elif action == LEFT:\n",
        "            col -= 1  # Move left (decrease column)\n",
        "        \n",
        "        # Step 2: Apply wind effect (based on CURRENT column, before action)\n",
        "        # Wind pushes UPWARD, so we SUBTRACT from row\n",
        "        wind_strength = self.winds[self.state[1]]\n",
        "        row -= wind_strength\n",
        "        \n",
        "        # Step 3: Keep agent within grid boundaries\n",
        "        self.state = self._limit_coordinates((row, col))\n",
        "        \n",
        "        # Step 4: Check if goal reached\n",
        "        done = (self.state == self.goal_state)\n",
        "        \n",
        "        # Step 5: Return results\n",
        "        reward = -1.0  # Cost per step (encourages shortest path)\n",
        "        return self._state_to_index(self.state), reward, done, {}\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"\n",
        "        Display current state of environment\n",
        "        \n",
        "        Symbols:\n",
        "        X = current agent position\n",
        "        G = goal\n",
        "        S = start\n",
        "        . = empty cell\n",
        "        \"\"\"\n",
        "        print(\"\\nCurrent Grid State:\")\n",
        "        for row in range(self.shape[0]):\n",
        "            line = \"\"\n",
        "            for col in range(self.shape[1]):\n",
        "                if (row, col) == self.state:\n",
        "                    line += \" X \"  # Agent\n",
        "                elif (row, col) == self.goal_state:\n",
        "                    line += \" G \"  # Goal\n",
        "                elif (row, col) == self.start_state:\n",
        "                    line += \" S \"  # Start\n",
        "                else:\n",
        "                    line += \" . \"  # Empty\n",
        "            print(line)\n",
        "        \n",
        "        # Display wind strength at bottom\n",
        "        print(\"\\nWind: \", end=\"\")\n",
        "        for w in self.winds:\n",
        "            print(f\" {w} \", end=\"\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "# Create environment instance\n",
        "env = WindyGridworldEnv()\n",
        "\n",
        "# Test basic functionality\n",
        "test_state = env.reset()\n",
        "test_next, test_reward, test_done, _ = env.step(RIGHT)\n",
        "\n",
        "pretty_print(\"✓ Windy Gridworld Created\",\n",
        "             \"Environment specifications:<br>\" +\n",
        "             \"• Grid size: 7×10 (70 states)<br>\" +\n",
        "             \"• Start: (3, 0) → state index 30<br>\" +\n",
        "             \"• Goal: (3, 7) → state index 37<br>\" +\n",
        "             \"• Wind: Varies by column (0-2 strength)<br>\" +\n",
        "             \"• Reward: -1 per step<br>\" +\n",
        "             f\"• Test step successful: next_state={test_next}, reward={test_reward}\",\n",
        "             style='success')\n",
        "\n",
        "# Display initial state\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; margin-top: 20px; border-left: 3px solid #17a2b8;\">\n",
        "    <h2 style=\"color: #17a2b8; font-size: 16px; margin: 0 0 8px 0; font-weight: 600;\">Section 2: Policy Implementation</h2>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Implementing the epsilon-greedy policy that balances exploration and exploitation for both algorithms.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "### Understanding Epsilon-Greedy Policy\n",
        "\n",
        "The ε-greedy policy is a simple but effective exploration strategy:\n",
        "\n",
        "**Policy Definition:**\n",
        "```\n",
        "π(a|s) = {\n",
        "    1 - ε + ε/|A|   if a = argmax Q(s,a)  (greedy action)\n",
        "    ε/|A|           otherwise              (exploratory actions)\n",
        "}\n",
        "```\n",
        "\n",
        "**In Plain English:**\n",
        "- With probability **1-ε**: Choose the best action (exploit)\n",
        "- With probability **ε**: Choose random action (explore)\n",
        "\n",
        "**Example with ε=0.1 and 4 actions:**\n",
        "- Best action gets: 1 - 0.1 + 0.1/4 = 0.925 (92.5%)\n",
        "- Each other action: 0.1/4 = 0.025 (2.5%)\n",
        "- Total: 0.925 + 3×0.025 = 1.0 ✓\n",
        "\n",
        "**Why This Works:**\n",
        "- Guarantees exploration of all state-action pairs\n",
        "- Maintains focus on promising actions\n",
        "- Balances exploration-exploitation tradeoff\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 3: Define Epsilon-Greedy Policy\n",
        "Purpose: Implement action selection strategy for exploration-exploitation balance\n",
        "\n",
        "TEACHING NOTE:\n",
        "This function returns a probability distribution over actions.\n",
        "The calling code then samples from this distribution to select an action.\n",
        "\"\"\"\n",
        "\n",
        "def epsilon_greedy_policy(Q, state, nA, epsilon):\n",
        "    \"\"\"\n",
        "    Create epsilon-greedy policy based on Q-values\n",
        "    \n",
        "    ALGORITHM:\n",
        "    1. Start with uniform exploration probability ε/|A| for all actions\n",
        "    2. Find the best action (highest Q-value)\n",
        "    3. Add remaining probability (1-ε) to best action\n",
        "    \n",
        "    Args:\n",
        "        Q: Action-value function (dict mapping state → action values)\n",
        "        state: Current state (integer index)\n",
        "        nA: Number of actions available\n",
        "        epsilon: Exploration probability (typically 0.01 to 0.1)\n",
        "    \n",
        "    Returns:\n",
        "        probs: Array of action probabilities [P(a₀), P(a₁), P(a₂), P(a₃)]\n",
        "    \n",
        "    Example:\n",
        "        Q[s] = [1.0, 2.5, 0.5, 1.8]  # Action 1 is best\n",
        "        With ε=0.1 and nA=4:\n",
        "        → probs = [0.025, 0.925, 0.025, 0.025]\n",
        "    \"\"\"\n",
        "    # Step 1: Initialize with uniform exploration probability\n",
        "    # Each action gets ε/nA probability (equal chance during exploration)\n",
        "    probs = np.ones(nA) * epsilon / nA\n",
        "    \n",
        "    # Step 2: Find best action\n",
        "    # argmax returns index of action with highest Q-value\n",
        "    best_action = np.argmax(Q[state])\n",
        "    \n",
        "    # Step 3: Add exploitation probability to best action\n",
        "    # Best action gets extra (1-ε) probability\n",
        "    probs[best_action] += 1.0 - epsilon\n",
        "    \n",
        "    # Verify: sum(probs) should equal 1.0\n",
        "    assert abs(np.sum(probs) - 1.0) < 1e-6, \"Probabilities must sum to 1\"\n",
        "    \n",
        "    return probs\n",
        "\n",
        "pretty_print(\"Policy Function Ready\",\n",
        "             \"Epsilon-greedy policy implemented<br>\" +\n",
        "             \"<strong>Key Points:</strong><br>\" +\n",
        "             \"• Balances exploration (trying new actions) with exploitation (using best known action)<br>\" +\n",
        "             \"• ε = 0.1 means 10% random exploration, 90% greedy selection<br>\" +\n",
        "             \"• Essential for both SARSA and Q-learning algorithms<br>\" +\n",
        "             \"• Returns probability distribution, not single action\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; margin-top: 20px; border-left: 3px solid #00acc1;\">\n",
        "    <h2 style=\"color: #00acc1; font-size: 16px; margin: 0 0 8px 0; font-weight: 600;\">Section 3: SARSA Implementation</h2>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Implementing SARSA (State-Action-Reward-State-Action), an on-policy TD control algorithm that learns the value of the policy being followed.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "### SARSA: State-Action-Reward-State-Action\n",
        "\n",
        "**Algorithm Name Origin:**\n",
        "The name SARSA comes from the tuple (S, A, R, S', A') used in each update:\n",
        "- **S**: Current state\n",
        "- **A**: Action taken\n",
        "- **R**: Reward received\n",
        "- **S'**: Next state\n",
        "- **A'**: Next action (chosen by ε-greedy policy)\n",
        "\n",
        "**Update Rule:**\n",
        "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$\n",
        "\n",
        "**Breakdown:**\n",
        "- $Q(S_t, A_t)$: Current estimate of action value\n",
        "- $\\alpha$: Learning rate (typically 0.1 - 0.5)\n",
        "- $R_{t+1}$: Immediate reward\n",
        "- $\\gamma$: Discount factor (typically 0.9 - 1.0)\n",
        "- $Q(S_{t+1}, A_{t+1})$: Estimated value of next state-action pair\n",
        "\n",
        "**Key Characteristic: ON-POLICY**\n",
        "- SARSA learns the value of the policy it's actually following (ε-greedy)\n",
        "- Uses the actual next action A' (chosen by ε-greedy) in the update\n",
        "- More conservative: accounts for exploration in value estimates\n",
        "- Learns: \"What's the value if I keep using ε-greedy?\"\n",
        "\n",
        "**Algorithm Steps:**\n",
        "```\n",
        "1. Initialize Q(s,a) arbitrarily\n",
        "2. For each episode:\n",
        "   a. S ← initial state\n",
        "   b. A ← action from ε-greedy(Q, S)\n",
        "   c. Repeat until episode ends:\n",
        "      i.   Take action A, observe R, S'\n",
        "      ii.  A' ← action from ε-greedy(Q, S')\n",
        "      iii. Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n",
        "      iv.  S ← S', A ← A'\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 4: Implement SARSA Algorithm\n",
        "Purpose: On-policy TD control for optimal epsilon-greedy policy\n",
        "\n",
        "TEACHING NOTE:\n",
        "SARSA is 'on-policy' because it learns about the policy it follows.\n",
        "The update uses Q(S',A') where A' is actually chosen by the policy.\n",
        "This makes SARSA more conservative - it accounts for exploration.\n",
        "\"\"\"\n",
        "\n",
        "# Define statistics tracking structure\n",
        "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
        "\n",
        "def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    SARSA algorithm: On-policy TD control\n",
        "    \n",
        "    This algorithm learns the value Q(s,a) for the ε-greedy policy.\n",
        "    It updates using the ACTUAL next action chosen by the policy.\n",
        "    \n",
        "    MATHEMATICAL FOUNDATION:\n",
        "    TD Error: δ = R + γQ(S',A') - Q(S,A)\n",
        "    Update: Q(S,A) ← Q(S,A) + α·δ\n",
        "    \n",
        "    Args:\n",
        "        env: OpenAI gym environment\n",
        "        num_episodes: Number of episodes to run\n",
        "        discount_factor (γ): Discount for future rewards [0,1]\n",
        "        alpha (α): Learning rate (step size) [0,1]\n",
        "        epsilon (ε): Exploration probability [0,1]\n",
        "    \n",
        "    Returns:\n",
        "        Q: Learned action-value function {state: [Q(s,a₀), Q(s,a₁), ...]}\n",
        "        stats: Episode statistics (lengths and rewards)\n",
        "    \"\"\"\n",
        "    # Initialize Q-table with zeros\n",
        "    # defaultdict automatically creates zero arrays for new states\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    # Track episode statistics for analysis\n",
        "    stats = EpisodeStats(\n",
        "        episode_lengths=np.zeros(num_episodes),\n",
        "        episode_rewards=np.zeros(num_episodes)\n",
        "    )\n",
        "    \n",
        "    pretty_print(\"Starting SARSA Training\",\n",
        "                 f\"Running {num_episodes} episodes<br>\" +\n",
        "                 f\"<strong>Parameters:</strong> α={alpha}, γ={discount_factor}, ε={epsilon}<br>\" +\n",
        "                 f\"<strong>Type:</strong> On-policy (learns ε-greedy policy)\",\n",
        "                 style='info')\n",
        "    \n",
        "    # Main training loop\n",
        "    for i_episode in range(num_episodes):\n",
        "        # Progress indicator (every 100 episodes)\n",
        "        if (i_episode + 1) % 100 == 0:\n",
        "            print(f\"\\rEpisode {i_episode + 1}/{num_episodes}\", end=\"\")\n",
        "            sys.stdout.flush()\n",
        "        \n",
        "        # STEP 1: Initialize episode - get starting state S\n",
        "        state = env.reset()\n",
        "        \n",
        "        # STEP 2: Choose first action A from S using ε-greedy policy\n",
        "        action_probs = epsilon_greedy_policy(Q, state, env.action_space.n, epsilon)\n",
        "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "        \n",
        "        # STEP 3: Episode loop - repeat until goal reached\n",
        "        for t in itertools.count():\n",
        "            # STEP 3a: Take action A, observe reward R and next state S'\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # STEP 3b: Choose next action A' from S' using ε-greedy\n",
        "            # CRITICAL: This is what makes SARSA on-policy!\n",
        "            # We use the ACTUAL next action that will be taken\n",
        "            next_action_probs = epsilon_greedy_policy(Q, next_state, env.action_space.n, epsilon)\n",
        "            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)\n",
        "            \n",
        "            # Update statistics\n",
        "            stats.episode_rewards[i_episode] += reward\n",
        "            stats.episode_lengths[i_episode] = t\n",
        "            \n",
        "            # STEP 3c: SARSA UPDATE\n",
        "            # TD Target: R + γQ(S',A')\n",
        "            td_target = reward + discount_factor * Q[next_state][next_action]\n",
        "            \n",
        "            # TD Error: δ = Target - Current\n",
        "            td_error = td_target - Q[state][action]\n",
        "            \n",
        "            # Update: Q(S,A) ← Q(S,A) + α·δ\n",
        "            Q[state][action] += alpha * td_error\n",
        "            \n",
        "            # Check if episode finished\n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "            # STEP 3d: Move to next state-action pair\n",
        "            # S ← S', A ← A'\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    return Q, stats\n",
        "\n",
        "pretty_print(\"SARSA Implementation Complete\",\n",
        "             \"<strong>On-policy TD control algorithm ready</strong><br><br>\" +\n",
        "             \"<strong>Key Features:</strong><br>\" +\n",
        "             \"• Uses actual next action A' from ε-greedy policy<br>\" +\n",
        "             \"• Learns value of the policy being followed<br>\" +\n",
        "             \"• Conservative: accounts for exploration in updates<br>\" +\n",
        "             \"• Converges to optimal ε-greedy policy<br><br>\" +\n",
        "             \"<strong>When to use SARSA:</strong><br>\" +\n",
        "             \"• When safety during learning matters<br>\" +\n",
        "             \"• When you want policy that accounts for exploration<br>\" +\n",
        "             \"• In environments with dangerous states\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; margin-top: 20px; border-left: 3px solid #00acc1;\">\n",
        "    <h2 style=\"color: #00acc1; font-size: 16px; margin: 0 0 8px 0; font-weight: 600;\">Section 4: Q-Learning Implementation</h2>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Implementing Q-learning, an off-policy TD control algorithm that learns the optimal policy while following an exploratory policy.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "### Q-Learning: Learning the Optimal Policy\n",
        "\n",
        "**Update Rule:**\n",
        "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$\n",
        "\n",
        "**Key Difference from SARSA:**\n",
        "- SARSA uses: $Q(S_{t+1}, A_{t+1})$ → actual next action\n",
        "- Q-learning uses: $\\max_a Q(S_{t+1}, a)$ → best possible action\n",
        "\n",
        "**Key Characteristic: OFF-POLICY**\n",
        "- Q-learning learns the value of the OPTIMAL policy\n",
        "- But follows a different policy (ε-greedy) for exploration\n",
        "- Uses max Q-value (best action) in update, not actual action taken\n",
        "- More aggressive: assumes greedy behavior in value estimates\n",
        "- Learns: \"What's the value if I always take the best action?\"\n",
        "\n",
        "**Algorithm Steps:**\n",
        "```\n",
        "1. Initialize Q(s,a) arbitrarily\n",
        "2. For each episode:\n",
        "   a. S ← initial state\n",
        "   b. Repeat until episode ends:\n",
        "      i.   A ← action from ε-greedy(Q, S)\n",
        "      ii.  Take action A, observe R, S'\n",
        "      iii. Q(S,A) ← Q(S,A) + α[R + γ·max_a Q(S',a) - Q(S,A)]\n",
        "      iv.  S ← S'\n",
        "```\n",
        "\n",
        "**SARSA vs Q-Learning Summary:**\n",
        "\n",
        "| Aspect | SARSA | Q-Learning |\n",
        "|--------|-------|------------|\n",
        "| Type | On-policy | Off-policy |\n",
        "| Learns | Value of ε-greedy policy | Value of optimal policy |\n",
        "| Update uses | Q(S',A') - actual next action | max Q(S',a) - best action |\n",
        "| Behavior | Conservative, safe | Aggressive, optimal |\n",
        "| Convergence | To optimal ε-greedy | To optimal greedy |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 5: Implement Q-Learning Algorithm\n",
        "Purpose: Off-policy TD control for learning optimal policy\n",
        "\n",
        "TEACHING NOTE:\n",
        "Q-learning is 'off-policy' because it learns about one policy (optimal greedy)\n",
        "while following a different policy (ε-greedy for exploration).\n",
        "The update uses max Q(S',a) instead of the actual next action.\n",
        "\"\"\"\n",
        "\n",
        "def q_learning(env, num_episodes, discount_factor=0.9, alpha=0.5, epsilon=0.05):\n",
        "    \"\"\"\n",
        "    Q-Learning algorithm: Off-policy TD control\n",
        "    \n",
        "    This algorithm learns the optimal Q*(s,a) regardless of the policy followed.\n",
        "    It updates using the BEST possible next action (greedy), not the actual one.\n",
        "    \n",
        "    MATHEMATICAL FOUNDATION:\n",
        "    TD Error: δ = R + γ·max_a Q(S',a) - Q(S,A)\n",
        "    Update: Q(S,A) ← Q(S,A) + α·δ\n",
        "    \n",
        "    COMPARISON TO SARSA:\n",
        "    SARSA:      Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]  ← uses A'\n",
        "    Q-Learning: Q(S,A) ← Q(S,A) + α[R + γ max Q(S',a) - Q(S,A)]  ← uses max\n",
        "    \n",
        "    Args:\n",
        "        env: OpenAI gym environment\n",
        "        num_episodes: Number of episodes to run\n",
        "        discount_factor (γ): Discount for future rewards [0,1]\n",
        "        alpha (α): Learning rate (step size) [0,1]\n",
        "        epsilon (ε): Exploration probability [0,1]\n",
        "    \n",
        "    Returns:\n",
        "        Q: Learned action-value function {state: [Q(s,a₀), Q(s,a₁), ...]}\n",
        "        stats: Episode statistics (lengths and rewards)\n",
        "    \"\"\"\n",
        "    # Initialize Q-table with zeros\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    # Track episode statistics\n",
        "    stats = EpisodeStats(\n",
        "        episode_lengths=np.zeros(num_episodes),\n",
        "        episode_rewards=np.zeros(num_episodes)\n",
        "    )\n",
        "    \n",
        "    pretty_print(\"Starting Q-Learning Training\",\n",
        "                 f\"Running {num_episodes} episodes<br>\" +\n",
        "                 f\"<strong>Parameters:</strong> α={alpha}, γ={discount_factor}, ε={epsilon}<br>\" +\n",
        "                 f\"<strong>Type:</strong> Off-policy (learns optimal policy)\",\n",
        "                 style='info')\n",
        "    \n",
        "    # Main training loop\n",
        "    for i_episode in range(num_episodes):\n",
        "        # Progress indicator\n",
        "        if (i_episode + 1) % 100 == 0:\n",
        "            print(f\"\\rEpisode {i_episode + 1}/{num_episodes}\", end=\"\")\n",
        "            sys.stdout.flush()\n",
        "        \n",
        "        # STEP 1: Initialize episode - get starting state S\n",
        "        state = env.reset()\n",
        "        \n",
        "        # STEP 2: Episode loop - repeat until goal reached\n",
        "        for t in range(10000):  # Max steps per episode (safety limit)\n",
        "            # STEP 2a: Choose action A from S using ε-greedy policy\n",
        "            # Note: Action selection is STILL ε-greedy (for exploration)\n",
        "            action_probs = epsilon_greedy_policy(Q, state, env.action_space.n, epsilon)\n",
        "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "            \n",
        "            # STEP 2b: Take action A, observe reward R and next state S'\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # STEP 2c: Q-LEARNING UPDATE\n",
        "            # CRITICAL DIFFERENCE FROM SARSA:\n",
        "            # We use max_a Q(S',a) instead of Q(S',A')\n",
        "            # This means we update as if we'll take the BEST action next,\n",
        "            # even though we might actually explore\n",
        "            \n",
        "            # TD Target: R + γ·max_a Q(S',a)\n",
        "            td_target = reward + discount_factor * np.max(Q[next_state])\n",
        "            \n",
        "            # TD Error: δ = Target - Current\n",
        "            td_error = td_target - Q[state][action]\n",
        "            \n",
        "            # Update: Q(S,A) ← Q(S,A) + α·δ\n",
        "            Q[state][action] += alpha * td_error\n",
        "            \n",
        "            # Update statistics\n",
        "            stats.episode_rewards[i_episode] += reward\n",
        "            stats.episode_lengths[i_episode] = t\n",
        "            \n",
        "            # Check if episode finished\n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "            # STEP 2d: Move to next state\n",
        "            # S ← S' (No need to track action since we use max)\n",
        "            state = next_state\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    return Q, stats\n",
        "\n",
        "pretty_print(\"Q-Learning Implementation Complete\",\n",
        "             \"<strong>Off-policy TD control algorithm ready</strong><br><br>\" +\n",
        "             \"<strong>Key Features:</strong><br>\" +\n",
        "             \"• Uses maximum Q-value (best possible action)<br>\" +\n",
        "             \"• Learns optimal policy regardless of exploration<br>\" +\n",
        "             \"• Aggressive: assumes greedy behavior in updates<br>\" +\n",
        "             \"• Converges to optimal greedy policy<br><br>\" +\n",
        "             \"<strong>When to use Q-Learning:</strong><br>\" +\n",
        "             \"• When you want the absolute best policy<br>\" +\n",
        "             \"• When safety during learning is less critical<br>\" +\n",
        "             \"• When faster convergence to optimum is desired\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; margin-top: 20px; border-left: 3px solid #17a2b8;\">\n",
        "    <h2 style=\"color: #17a2b8; font-size: 16px; margin: 0 0 8px 0; font-weight: 600;\">Section 5: Running Experiments</h2>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Training both algorithms on the Windy Gridworld to compare their learning characteristics and performance.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "### Experimental Design\n",
        "\n",
        "We'll train both SARSA and Q-learning on the same environment with slightly different hyperparameters to highlight their differences:\n",
        "\n",
        "**Parameter Choices:**\n",
        "\n",
        "| Parameter | SARSA | Q-Learning | Rationale |\n",
        "|-----------|-------|------------|----------|\n",
        "| Episodes | 300 | 300 | Same training duration |\n",
        "| α (alpha) | 0.5 | 0.5 | Same learning rate |\n",
        "| γ (gamma) | 1.0 | 0.9 | SARSA: no discount; Q-Learning: slight discount |\n",
        "| ε (epsilon) | 0.1 | 0.05 | SARSA: more exploration; Q-Learning: less needed |\n",
        "\n",
        "**Why These Parameters?**\n",
        "- SARSA uses γ=1.0 (no discount) and higher ε=0.1 because it learns the ε-greedy policy\n",
        "- Q-Learning uses γ=0.9 and lower ε=0.05 because it learns the optimal policy\n",
        "- Lower ε for Q-Learning reduces exploration once good policy found\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 6: Define Experiment Parameters\n",
        "Purpose: Set hyperparameters for both algorithms\n",
        "\n",
        "TEACHING NOTE:\n",
        "These parameters are carefully chosen based on:\n",
        "1. Standard values from Sutton & Barto textbook\n",
        "2. Empirical performance on Windy Gridworld\n",
        "3. Highlighting differences between algorithms\n",
        "\"\"\"\n",
        "\n",
        "# Common parameter\n",
        "NUM_EPISODES = 300\n",
        "\n",
        "# SARSA parameters\n",
        "# Higher epsilon because SARSA learns the ε-greedy policy\n",
        "# γ=1.0 appropriate when episodes terminate (no infinite horizon)\n",
        "SARSA_PARAMS = {\n",
        "    'discount_factor': 1.0,   # No discounting (episodic task)\n",
        "    'alpha': 0.5,             # Moderate learning rate\n",
        "    'epsilon': 0.1            # 10% exploration\n",
        "}\n",
        "\n",
        "# Q-Learning parameters\n",
        "# Lower epsilon because Q-learning learns optimal policy\n",
        "# γ=0.9 to slightly favor immediate rewards\n",
        "QLEARNING_PARAMS = {\n",
        "    'discount_factor': 0.9,   # Slight preference for immediate rewards\n",
        "    'alpha': 0.5,             # Same learning rate as SARSA\n",
        "    'epsilon': 0.05           # 5% exploration (less needed)\n",
        "}\n",
        "\n",
        "pretty_print(\"Experiment Parameters Set\",\n",
        "             f\"<strong>Training Episodes:</strong> {NUM_EPISODES}<br><br>\" +\n",
        "             f\"<strong>SARSA (On-Policy):</strong><br>\" +\n",
        "             f\"• Learning rate α={SARSA_PARAMS['alpha']}<br>\" +\n",
        "             f\"• Discount factor γ={SARSA_PARAMS['discount_factor']}<br>\" +\n",
        "             f\"• Exploration rate ε={SARSA_PARAMS['epsilon']} (10%)<br><br>\" +\n",
        "             f\"<strong>Q-Learning (Off-Policy):</strong><br>\" +\n",
        "             f\"• Learning rate α={QLEARNING_PARAMS['alpha']}<br>\" +\n",
        "             f\"• Discount factor γ={QLEARNING_PARAMS['discount_factor']}<br>\" +\n",
        "             f\"• Exploration rate ε={QLEARNING_PARAMS['epsilon']} (5%)<br><br>\" +\n",
        "             f\"<strong>Note:</strong> Different ε values highlight algorithm behaviors\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 7: Train Both Algorithms\n",
        "Purpose: Run SARSA and Q-learning experiments and collect results\n",
        "\n",
        "TEACHING NOTE:\n",
        "Watch the episode lengths decrease over time.\n",
        "Both algorithms should learn to reach the goal in ~15 steps\n",
        "(optimal path without wind is 7 steps, wind adds complexity)\n",
        "\"\"\"\n",
        "\n",
        "# Train SARSA (On-Policy)\n",
        "pretty_print(\"Training SARSA\", \n",
        "             \"Running on-policy TD control...<br>\" +\n",
        "             \"This will take about 10-30 seconds\", \n",
        "             style='info')\n",
        "Q_sarsa, stats_sarsa = sarsa(env, NUM_EPISODES, **SARSA_PARAMS)\n",
        "\n",
        "# Train Q-Learning (Off-Policy)\n",
        "pretty_print(\"Training Q-Learning\", \n",
        "             \"Running off-policy TD control...<br>\" +\n",
        "             \"This will take about 10-30 seconds\", \n",
        "             style='info')\n",
        "Q_qlearning, stats_qlearning = q_learning(env, NUM_EPISODES, **QLEARNING_PARAMS)\n",
        "\n",
        "# Display results\n",
        "pretty_print(\"Training Complete - Initial Analysis\",\n",
        "             f\"<strong>SARSA Results:</strong><br>\" +\n",
        "             f\"• Final episode reward: {stats_sarsa.episode_rewards[-1]:.0f}<br>\" +\n",
        "             f\"• Final episode length: {stats_sarsa.episode_lengths[-1]:.0f} steps<br>\" +\n",
        "             f\"• Average of last 20 episodes: {np.mean(stats_sarsa.episode_lengths[-20:]):.1f} steps<br><br>\" +\n",
        "             f\"<strong>Q-Learning Results:</strong><br>\" +\n",
        "             f\"• Final episode reward: {stats_qlearning.episode_rewards[-1]:.0f}<br>\" +\n",
        "             f\"• Final episode length: {stats_qlearning.episode_lengths[-1]:.0f} steps<br>\" +\n",
        "             f\"• Average of last 20 episodes: {np.mean(stats_qlearning.episode_lengths[-20:]):.1f} steps<br><br>\" +\n",
        "             f\"<strong>Interpretation:</strong><br>\" +\n",
        "             f\"• Lower episode length = better (faster path to goal)<br>\" +\n",
        "             f\"• Reward = -1 × steps (more negative = longer path)<br>\" +\n",
        "             f\"• Optimal path is approximately 15 steps accounting for wind\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; margin-top: 20px; border-left: 3px solid #17a2b8;\">\n",
        "    <h2 style=\"color: #17a2b8; font-size: 16px; margin: 0 0 8px 0; font-weight: 600;\">Section 6: Visualization and Analysis</h2>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Creating comprehensive visualizations to compare the performance and learning characteristics of SARSA and Q-learning.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "### What to Look For in the Plots\n",
        "\n",
        "The comparison plots will show four key metrics:\n",
        "\n",
        "**1. Episode Length Over Time:**\n",
        "- Shows how quickly each algorithm finds shorter paths\n",
        "- Q-learning typically shows faster initial improvement\n",
        "- SARSA may show more gradual, stable improvement\n",
        "\n",
        "**2. Episode Reward Over Time:**\n",
        "- Reward = -1 × steps, so higher (less negative) is better\n",
        "- Smoothed curves show overall learning trend\n",
        "- Raw data shows exploration variance\n",
        "\n",
        "**3. Learning Speed (Episodes vs Time Steps):**\n",
        "- Shows how many steps needed to complete N episodes\n",
        "- Steeper slope = learning faster\n",
        "- Q-learning often reaches asymptote sooner\n",
        "\n",
        "**4. Performance Summary:**\n",
        "- Statistical comparison of final performance\n",
        "- Average of last 20 episodes (converged behavior)\n",
        "- Best single episode achieved\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 8: Create Comprehensive Comparison Plots\n",
        "Purpose: Visualize and compare SARSA vs Q-learning performance\n",
        "\n",
        "TEACHING NOTE:\n",
        "These plots help students understand:\n",
        "1. How learning progresses over time\n",
        "2. Variance in performance (raw vs smoothed)\n",
        "3. Convergence characteristics\n",
        "4. Practical differences between algorithms\n",
        "\"\"\"\n",
        "\n",
        "def plot_algorithm_comparison(stats_sarsa, stats_qlearning, smoothing_window=10):\n",
        "    \"\"\"\n",
        "    Create comprehensive comparison plots\n",
        "    \n",
        "    VISUALIZATION STRATEGY:\n",
        "    - Raw data (transparent) shows actual performance and variance\n",
        "    - Smoothed data (solid) shows overall learning trend\n",
        "    - Both are important for understanding algorithm behavior\n",
        "    \n",
        "    Args:\n",
        "        stats_sarsa: SARSA episode statistics\n",
        "        stats_qlearning: Q-learning episode statistics\n",
        "        smoothing_window: Window size for moving average (default 10)\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # === PLOT 1: Episode Length Comparison ===\n",
        "    # Lower is better (fewer steps to goal)\n",
        "    axes[0, 0].plot(stats_sarsa.episode_lengths, alpha=0.3, color='blue', label='SARSA (raw)')\n",
        "    axes[0, 0].plot(pd.Series(stats_sarsa.episode_lengths).rolling(smoothing_window, min_periods=1).mean(),\n",
        "                   color='blue', linewidth=2, label='SARSA (smoothed)')\n",
        "    axes[0, 0].plot(stats_qlearning.episode_lengths, alpha=0.3, color='red', label='Q-Learning (raw)')\n",
        "    axes[0, 0].plot(pd.Series(stats_qlearning.episode_lengths).rolling(smoothing_window, min_periods=1).mean(),\n",
        "                   color='red', linewidth=2, label='Q-Learning (smoothed)')\n",
        "    axes[0, 0].set_xlabel('Episode')\n",
        "    axes[0, 0].set_ylabel('Episode Length (steps)')\n",
        "    axes[0, 0].set_title('Episode Length Over Time\\n(Lower is Better)', fontweight='bold')\n",
        "    axes[0, 0].legend(loc='upper right')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # === PLOT 2: Episode Reward Comparison ===\n",
        "    # Higher (less negative) is better\n",
        "    rewards_smoothed_sarsa = pd.Series(stats_sarsa.episode_rewards).rolling(smoothing_window, min_periods=1).mean()\n",
        "    rewards_smoothed_qlearning = pd.Series(stats_qlearning.episode_rewards).rolling(smoothing_window, min_periods=1).mean()\n",
        "    \n",
        "    axes[0, 1].plot(stats_sarsa.episode_rewards, alpha=0.3, color='blue', label='SARSA (raw)')\n",
        "    axes[0, 1].plot(rewards_smoothed_sarsa, color='blue', linewidth=2, label='SARSA (smoothed)')\n",
        "    axes[0, 1].plot(stats_qlearning.episode_rewards, alpha=0.3, color='red', label='Q-Learning (raw)')\n",
        "    axes[0, 1].plot(rewards_smoothed_qlearning, color='red', linewidth=2, label='Q-Learning (smoothed)')\n",
        "    axes[0, 1].set_xlabel('Episode')\n",
        "    axes[0, 1].set_ylabel('Episode Reward')\n",
        "    axes[0, 1].set_title(f'Episode Reward (Smoothed over {smoothing_window} episodes)\\n(Higher is Better)', fontweight='bold')\n",
        "    axes[0, 1].legend(loc='lower right')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # === PLOT 3: Learning Speed Comparison ===\n",
        "    # Shows total steps needed vs episodes completed\n",
        "    axes[1, 0].plot(np.cumsum(stats_sarsa.episode_lengths), np.arange(len(stats_sarsa.episode_lengths)),\n",
        "                   color='blue', linewidth=2, label='SARSA')\n",
        "    axes[1, 0].plot(np.cumsum(stats_qlearning.episode_lengths), np.arange(len(stats_qlearning.episode_lengths)),\n",
        "                   color='red', linewidth=2, label='Q-Learning')\n",
        "    axes[1, 0].set_xlabel('Total Time Steps')\n",
        "    axes[1, 0].set_ylabel('Episodes Completed')\n",
        "    axes[1, 0].set_title('Learning Speed Comparison\\n(Steeper = Faster Learning)', fontweight='bold')\n",
        "    axes[1, 0].legend(loc='lower right')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # === PLOT 4: Performance Summary ===\n",
        "    axes[1, 1].axis('off')\n",
        "    \n",
        "    # Calculate statistics\n",
        "    sarsa_final_avg = np.mean(stats_sarsa.episode_lengths[-20:])\n",
        "    qlearning_final_avg = np.mean(stats_qlearning.episode_lengths[-20:])\n",
        "    sarsa_best = np.min(stats_sarsa.episode_lengths)\n",
        "    qlearning_best = np.min(stats_qlearning.episode_lengths)\n",
        "    \n",
        "    summary_text = f\"\"\"\n",
        "    ALGORITHM COMPARISON SUMMARY\n",
        "    \n",
        "    SARSA (On-Policy):\n",
        "    • Average steps (last 20): {sarsa_final_avg:.1f}\n",
        "    • Best episode: {sarsa_best:.0f} steps\n",
        "    • Convergence: Smoother, more conservative\n",
        "    • Learns: ε-greedy policy value\n",
        "    \n",
        "    Q-Learning (Off-Policy):\n",
        "    • Average steps (last 20): {qlearning_final_avg:.1f}\n",
        "    • Best episode: {qlearning_best:.0f} steps\n",
        "    • Convergence: Faster to optimal\n",
        "    • Learns: Optimal policy value\n",
        "    \n",
        "    KEY INSIGHTS:\n",
        "    • Q-learning typically finds shorter paths\n",
        "    • SARSA safer (accounts for exploration)\n",
        "    • Both successfully solve the problem\n",
        "    • Wind compensation learned by both\n",
        "    \"\"\"\n",
        "    \n",
        "    axes[1, 1].text(0.1, 0.5, summary_text, fontsize=11, verticalalignment='center',\n",
        "                   family='monospace', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
        "    \n",
        "    plt.suptitle('SARSA vs Q-Learning on Windy Gridworld', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate comprehensive comparison plots\n",
        "plot_algorithm_comparison(stats_sarsa, stats_qlearning, smoothing_window=10)\n",
        "\n",
        "pretty_print(\"Analysis Complete\",\n",
        "             \"<strong>Key Observations:</strong><br>\" +\n",
        "             \"• Q-learning converges faster to optimal policy<br>\" +\n",
        "             \"• SARSA shows more conservative, safer behavior<br>\" +\n",
        "             \"• Both successfully solve the windy gridworld<br>\" +\n",
        "             \"• Raw data shows exploration variance<br>\" +\n",
        "             \"• Smoothed curves show learning trends<br><br>\" +\n",
        "             \"<strong>Teaching Points:</strong><br>\" +\n",
        "             \"• On-policy (SARSA) learns actual followed policy<br>\" +\n",
        "             \"• Off-policy (Q-Learning) learns optimal policy<br>\" +\n",
        "             \"• Both approaches have practical applications\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Results\n",
        "\n",
        "**What You Should See:**\n",
        "\n",
        "1. **Initial Episodes (1-50):** Both algorithms explore randomly, long episodes\n",
        "2. **Learning Phase (50-150):** Episode lengths decrease as good paths discovered\n",
        "3. **Convergence (150-300):** Performance stabilizes near optimal\n",
        "\n",
        "**Typical Performance:**\n",
        "- Optimal path (no wind): 7 steps\n",
        "- Optimal path (with wind): ~15 steps\n",
        "- Q-learning final: ~15-17 steps\n",
        "- SARSA final: ~16-18 steps\n",
        "\n",
        "**Why Q-Learning is Often Better Here:**\n",
        "- Learns optimal policy (greedy)\n",
        "- Windy Gridworld has no dangerous states\n",
        "- Can afford to be aggressive\n",
        "- Lower exploration rate (ε=0.05) helps convergence\n",
        "\n",
        "**When SARSA Would Be Better:**\n",
        "- Environments with \"cliffs\" or dangerous states\n",
        "- When safety during learning matters\n",
        "- When you want policy that accounts for exploration\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 9: Extended Experiment - Learning Rate Sensitivity\n",
        "Purpose: Demonstrate how learning rate affects convergence\n",
        "\n",
        "TEACHING NOTE:\n",
        "This experiment shows that hyperparameter tuning is important.\n",
        "Small α → slow but stable learning\n",
        "Large α → fast but potentially unstable learning\n",
        "\"\"\"\n",
        "\n",
        "# Test with very small alpha to show effect\n",
        "ALPHA_SMALL = 0.01\n",
        "EPISODES_EXTENDED = 1000\n",
        "\n",
        "pretty_print(\"Extended Experiment - Learning Rate Sensitivity\",\n",
        "             f\"Testing with α={ALPHA_SMALL} (much smaller than α=0.5)<br>\" +\n",
        "             f\"Running {EPISODES_EXTENDED} episodes to show convergence<br><br>\" +\n",
        "             \"<strong>Expected Results:</strong><br>\" +\n",
        "             \"• Slower convergence with small α<br>\" +\n",
        "             \"• More stable learning (less variance)<br>\" +\n",
        "             \"• Requires more episodes to reach good performance\",\n",
        "             style='info')\n",
        "\n",
        "# Train with small alpha\n",
        "Q_sarsa_small, stats_sarsa_small = sarsa(\n",
        "    env, EPISODES_EXTENDED, \n",
        "    discount_factor=SARSA_PARAMS['discount_factor'],\n",
        "    alpha=ALPHA_SMALL,\n",
        "    epsilon=SARSA_PARAMS['epsilon']\n",
        ")\n",
        "\n",
        "Q_qlearning_small, stats_qlearning_small = q_learning(\n",
        "    env, EPISODES_EXTENDED,\n",
        "    discount_factor=QLEARNING_PARAMS['discount_factor'],\n",
        "    alpha=ALPHA_SMALL,\n",
        "    epsilon=QLEARNING_PARAMS['epsilon']\n",
        ")\n",
        "\n",
        "# Create comparison plots\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# SARSA comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(pd.Series(stats_sarsa.episode_lengths).rolling(20, min_periods=1).mean(),\n",
        "         label=f'SARSA α={SARSA_PARAMS[\"alpha\"]}', color='blue', linewidth=2)\n",
        "plt.plot(pd.Series(stats_sarsa_small.episode_lengths).rolling(20, min_periods=1).mean(),\n",
        "         label=f'SARSA α={ALPHA_SMALL}', color='lightblue', linewidth=2)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Episode Length (smoothed)')\n",
        "plt.title('SARSA: Effect of Learning Rate\\n(Lower α = Slower but Steadier Learning)', fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Q-Learning comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(pd.Series(stats_qlearning.episode_lengths).rolling(20, min_periods=1).mean(),\n",
        "         label=f'Q-Learning α={QLEARNING_PARAMS[\"alpha\"]}', color='red', linewidth=2)\n",
        "plt.plot(pd.Series(stats_qlearning_small.episode_lengths).rolling(20, min_periods=1).mean(),\n",
        "         label=f'Q-Learning α={ALPHA_SMALL}', color='salmon', linewidth=2)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Episode Length (smoothed)')\n",
        "plt.title('Q-Learning: Effect of Learning Rate\\n(Lower α = Slower but Steadier Learning)', fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Learning Rate Impact on Convergence', fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "pretty_print(\"Learning Rate Analysis Complete\",\n",
        "             \"<strong>Key Findings:</strong><br><br>\" +\n",
        "             f\"<strong>Small α ({ALPHA_SMALL}):</strong><br>\" +\n",
        "             \"• Requires many more episodes to converge<br>\" +\n",
        "             \"• More stable learning curve<br>\" +\n",
        "             \"• Less sensitive to noisy rewards<br>\" +\n",
        "             \"• Better for non-stationary environments<br><br>\" +\n",
        "             f\"<strong>Large α ({SARSA_PARAMS['alpha']}):</strong><br>\" +\n",
        "             \"• Faster convergence<br>\" +\n",
        "             \"• More variance in learning curve<br>\" +\n",
        "             \"• Can overshoot optimal values<br>\" +\n",
        "             \"• Better for stationary environments<br><br>\" +\n",
        "             \"<strong>Practical Advice:</strong><br>\" +\n",
        "             \"• Start with α=0.1 or α=0.5<br>\" +\n",
        "             \"• Decrease α over time (learning rate schedule)<br>\" +\n",
        "             \"• Q-learning less sensitive to α than SARSA<br>\" +\n",
        "             \"• Monitor learning curves to tune α\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Key Findings Summary</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>1. On-Policy vs Off-Policy:</strong> SARSA learns the value of the policy it follows (ε-greedy), accounting for exploration in its updates. Q-learning learns the optimal greedy policy while using ε-greedy for exploration. This fundamental difference makes SARSA more conservative and Q-learning more aggressive.</p>\n",
        "        \n",
        "        <p><strong>2. Convergence Speed:</strong> Q-learning typically converges faster to optimal behavior because it directly learns the optimal policy. SARSA takes longer because it must also learn how exploration affects outcomes. In Windy Gridworld, Q-learning reaches good performance in ~150 episodes vs ~200 for SARSA.</p>\n",
        "        \n",
        "        <p><strong>3. Safety Considerations:</strong> SARSA naturally learns safer policies near dangerous areas because its updates account for exploratory actions that might lead to bad states. Q-learning assumes greedy behavior and may learn policies that are unsafe during exploration.</p>\n",
        "        \n",
        "        <p><strong>4. Wind Compensation:</strong> Both algorithms successfully learn to compensate for wind. Early episodes show agents being blown off course; later episodes show successful navigation through windy columns. The learning manifests as shorter, more direct paths.</p>\n",
        "        \n",
        "        <p><strong>5. Learning Rate Effects:</strong> Smaller α (0.01) provides stable but slow learning. Larger α (0.5) enables faster convergence but with more variance. Q-learning is generally less sensitive to α choice than SARSA because of its off-policy nature.</p>\n",
        "        \n",
        "        <p><strong>6. Practical Performance:</strong> Both algorithms achieve near-optimal performance (15-18 steps vs theoretical minimum of ~15 with wind). Q-learning averages 15-17 steps, SARSA 16-18 steps in final episodes.</p>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li><strong>Theoretical:</strong> Why does Q-learning use max Q(S',a) while SARSA uses Q(S',A')? What policy does each learn?</li>\n",
        "        <li><strong>Practical:</strong> In the Cliff Walking problem (Sutton & Barto Example 6.6), SARSA learns to stay away from the cliff edge while Q-learning walks near it. Why?</li>\n",
        "        <li><strong>Experimental:</strong> What would happen if we used ε=0 (purely greedy)? Would either algorithm still work?</li>\n",
        "        <li><strong>Design:</strong> How would you modify the wind pattern to make SARSA significantly outperform Q-learning?</li>\n",
        "        <li><strong>Exploration:</strong> Could we use different exploration strategies (softmax, UCB)? How would that change the comparison?</li>\n",
        "        <li><strong>Extension:</strong> How would you extend these algorithms to continuous state spaces? (Hint: Function approximation - next labs!)</li>\n",
        "        <li><strong>Convergence:</strong> Under what conditions is SARSA guaranteed to converge to optimal? What about Q-learning?</li>\n",
        "        <li><strong>Real-World:</strong> Give three real-world applications where you'd prefer SARSA over Q-learning, and vice versa.</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #e8f5e9; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #4caf50;\">\n",
        "    <h3 style=\"color: #4caf50; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Further Exploration</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>Try These Experiments:</strong></p>\n",
        "        <ul style=\"margin: 8px 0 0 0; padding-left: 20px;\">\n",
        "            <li>Modify the wind pattern (make it stochastic, stronger, or diagonal)</li>\n",
        "            <li>Add \"king's moves\" (8 directions instead of 4) to action space</li>\n",
        "            <li>Implement Expected SARSA (uses expected value instead of sampled A')</li>\n",
        "            <li>Try different discount factors (γ = 0.5, 0.99) and observe effects</li>\n",
        "            <li>Implement ε-decay (gradually reduce exploration over time)</li>\n",
        "            <li>Add a \"cliff\" with large negative reward and compare safety</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 6b: SARSA vs Q-Learning in Windy Gridworld</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Next: Lab 7 - Function Approximation with Neural Networks</p>\n",
        "</div>"
      ]
    }
  ]
}
