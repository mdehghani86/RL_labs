{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Lab_06b_Windy_Gridworld_SARSA_QLearning.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 6b: SARSA vs Q-Learning in Windy Gridworld\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapter 6 | Intermediate Level | 75 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        The Windy Gridworld problem, introduced in \n",
        "        <a href=\"http://incompleteideas.net/book/the-book-2nd.html\" style=\"color: #17a2b8;\">Sutton & Barto Example 6.5</a>,\n",
        "        demonstrates the difference between on-policy (SARSA) and off-policy (Q-learning) TD control methods.\n",
        "        In this environment, crosswind affects the agent's movement, creating a stochastic transition model.\n",
        "        We'll implement both algorithms and compare their learning characteristics, exploring how\n",
        "        the on-policy vs off-policy distinction affects convergence and performance.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Implement SARSA (on-policy TD control)</li>\n",
        "        <li>Implement Q-learning (off-policy TD control)</li>\n",
        "        <li>Understand the impact of wind on state transitions</li>\n",
        "        <li>Compare on-policy vs off-policy learning</li>\n",
        "        <li>Analyze convergence rates and final policies</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Key Concepts</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">SARSA</code> → On-policy TD control</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Q-learning</code> → Off-policy TD control</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">ε-greedy</code> → Action selection strategy</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Wind strength</code> → [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Grid size</code> → 7×10 gridworld</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; margin-top: 20px; border-left: 3px solid #17a2b8;\">\n",
        "    <h2 style=\"color: #17a2b8; font-size: 16px; margin: 0 0 8px 0; font-weight: 600;\">Section 1: Environment Setup</h2>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        We begin by setting up the Windy Gridworld environment, a classic benchmark problem for comparing TD control algorithms.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "### The Windy Gridworld Problem\n",
        "\n",
        "![Windy Gridworld](https://www.researchgate.net/profile/Markus-Dumke/publication/320890681/figure/fig1/AS:763210537922560@1558974980641/The-windy-gridworld-task-The-goal-is-to-move-from-the-start-state-S-to-the-goal-state-G.jpg)\n",
        "\n",
        "**Environment characteristics:**\n",
        "- **Grid**: 7 rows × 10 columns\n",
        "- **Start**: Position (3, 0) marked as 'S'\n",
        "- **Goal**: Position (3, 7) marked as 'G'\n",
        "- **Wind**: Upward push in middle columns (strength shown at bottom)\n",
        "- **Actions**: 4 standard moves (up, down, left, right)\n",
        "- **Reward**: -1 per step until goal reached"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 1: Install Dependencies and Import Libraries\n",
        "Purpose: Set up the environment with required packages and load utilities\n",
        "\"\"\"\n",
        "\n",
        "# Install specific gym version for compatibility\n",
        "!pip install gym==0.20 -q\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import sys\n",
        "from gym.envs.toy_text import discrete\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from collections import namedtuple, defaultdict\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load pretty print utility\n",
        "try:\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/mdehghani86/RL_labs/master/utility/rl_utility.py'\n",
        "    response = requests.get(url)\n",
        "    exec(response.text)\n",
        "    pretty_print(\"Environment Ready\", \n",
        "                 \"Successfully loaded dependencies and pretty_print utility<br>\" +\n",
        "                 \"Gym version 0.20 installed for WindyGridworld compatibility\", \n",
        "                 style='success')\n",
        "except Exception as e:\n",
        "    def pretty_print(title, content, style='info'):\n",
        "        \"\"\"Fallback pretty print function\"\"\"\n",
        "        themes = {\n",
        "            'info': {'primary': '#17a2b8', 'secondary': '#0e5a63', 'background': '#f8f9fa'},\n",
        "            'success': {'primary': '#28a745', 'secondary': '#155724', 'background': '#f8fff9'},\n",
        "            'warning': {'primary': '#ffc107', 'secondary': '#e0a800', 'background': '#fffdf5'},\n",
        "            'result': {'primary': '#6f42c1', 'secondary': '#4e2c8e', 'background': '#faf5ff'},\n",
        "            'note': {'primary': '#20c997', 'secondary': '#0d7a5f', 'background': '#f0fdf9'}\n",
        "        }\n",
        "        theme = themes.get(style, themes['info'])\n",
        "        html = f'''\n",
        "        <div style=\"border-radius: 5px; margin: 10px 0; width: 20cm; max-width: 20cm; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
        "            <div style=\"background: linear-gradient(90deg, {theme['primary']} 0%, {theme['secondary']} 100%); padding: 10px 15px; border-radius: 5px 5px 0 0;\">\n",
        "                <strong style=\"color: white; font-size: 14px;\">{title}</strong>\n",
        "            </div>\n",
        "            <div style=\"background: {theme['background']}; padding: 10px 15px; border-radius: 0 0 5px 5px; border-left: 3px solid {theme['primary']};\">\n",
        "                <div style=\"color: rgba(0,0,0,0.8); font-size: 12px; line-height: 1.5;\">{content}</div>\n",
        "            </div>\n",
        "        </div>\n",
        "        '''\n",
        "        display(HTML(html))\n",
        "    \n",
        "    pretty_print(\"Fallback Mode\", \n",
        "                 \"Using local pretty_print definition<br>\" +\n",
        "                 \"Dependencies loaded successfully\", \n",
        "                 style='warning')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 2: Define Windy Gridworld Environment\n",
        "Purpose: Create custom gym environment with wind dynamics\n",
        "\"\"\"\n",
        "\n",
        "# Define action constants for readability\n",
        "UP = 0\n",
        "RIGHT = 1\n",
        "DOWN = 2\n",
        "LEFT = 3\n",
        "\n",
        "class WindyGridworldEnv(discrete.DiscreteEnv):\n",
        "    \"\"\"\n",
        "    Windy Gridworld Environment\n",
        "    \n",
        "    7x10 grid with wind pushing agent upward in certain columns\n",
        "    Start: (3, 0), Goal: (3, 7)\n",
        "    Wind strength by column: [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
        "    \"\"\"\n",
        "    \n",
        "    metadata = {'render.modes': ['human', 'ansi']}\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.shape = (7, 10)  # Grid dimensions\n",
        "        nS = np.prod(self.shape)  # Total number of states\n",
        "        nA = 4  # Number of actions\n",
        "        \n",
        "        # Define wind strength for each column\n",
        "        # Column:     0  1  2  3  4  5  6  7  8  9\n",
        "        # Wind:       0  0  0  1  1  1  2  2  1  0\n",
        "        winds = np.zeros(self.shape)\n",
        "        winds[:, [3, 4, 5, 8]] = 1  # Wind strength 1\n",
        "        winds[:, [6, 7]] = 2  # Wind strength 2\n",
        "        \n",
        "        # Calculate transition probabilities\n",
        "        P = {}\n",
        "        for s in range(nS):\n",
        "            position = np.unravel_index(s, self.shape)\n",
        "            P[s] = {a: [] for a in range(nA)}\n",
        "            \n",
        "            # Define transitions for each action\n",
        "            P[s][UP] = self._calculate_transition_prob(position, [-1, 0], winds)\n",
        "            P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1], winds)\n",
        "            P[s][DOWN] = self._calculate_transition_prob(position, [1, 0], winds)\n",
        "            P[s][LEFT] = self._calculate_transition_prob(position, [0, -1], winds)\n",
        "        \n",
        "        # Initial state distribution (always start at (3, 0))\n",
        "        isd = np.zeros(nS)\n",
        "        isd[np.ravel_multi_index((3, 0), self.shape)] = 1.0\n",
        "        \n",
        "        super(WindyGridworldEnv, self).__init__(nS, nA, P, isd)\n",
        "    \n",
        "    def _calculate_transition_prob(self, current, delta, winds):\n",
        "        \"\"\"\n",
        "        Calculate next state considering action and wind effect\n",
        "        \n",
        "        Wind pushes agent upward (negative row direction)\n",
        "        \"\"\"\n",
        "        # Apply action and wind effect\n",
        "        new_position = np.array(current) + np.array(delta) + np.array([-1, 0]) * winds[tuple(current)]\n",
        "        new_position = self._limit_coordinates(new_position).astype(int)\n",
        "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
        "        \n",
        "        # Check if goal reached\n",
        "        is_done = tuple(new_position) == (3, 7)\n",
        "        \n",
        "        # Return transition: (probability, next_state, reward, done)\n",
        "        return [(1.0, new_state, -1.0, is_done)]\n",
        "    \n",
        "    def _limit_coordinates(self, coord):\n",
        "        \"\"\"\n",
        "        Keep agent within grid boundaries\n",
        "        \"\"\"\n",
        "        coord[0] = min(coord[0], self.shape[0] - 1)\n",
        "        coord[0] = max(coord[0], 0)\n",
        "        coord[1] = min(coord[1], self.shape[1] - 1)\n",
        "        coord[1] = max(coord[1], 0)\n",
        "        return coord\n",
        "    \n",
        "    def _render(self, mode='human', close=False):\n",
        "        \"\"\"\n",
        "        Render the current state of the environment\n",
        "        x = current position, T = goal, o = empty cell\n",
        "        \"\"\"\n",
        "        if close:\n",
        "            return\n",
        "        \n",
        "        outfile = sys.stdout\n",
        "        \n",
        "        for s in range(self.nS):\n",
        "            position = np.unravel_index(s, self.shape)\n",
        "            \n",
        "            if self.s == s:\n",
        "                output = \" x \"  # Current position\n",
        "            elif position == (3, 7):\n",
        "                output = \" T \"  # Goal\n",
        "            else:\n",
        "                output = \" o \"  # Empty cell\n",
        "            \n",
        "            if position[1] == 0:\n",
        "                output = output.lstrip()\n",
        "            if position[1] == self.shape[1] - 1:\n",
        "                output = output.rstrip()\n",
        "                output += \"\\n\"\n",
        "            \n",
        "            outfile.write(output)\n",
        "        outfile.write(\"\\n\")\n",
        "\n",
        "# Create environment instance\n",
        "env = WindyGridworldEnv()\n",
        "\n",
        "pretty_print(\"Windy Gridworld Created\",\n",
        "             \"Environment specifications:<br>\" +\n",
        "             \"• Grid size: 7×10<br>\" +\n",
        "             \"• Start position: (3, 0)<br>\" +\n",
        "             \"• Goal position: (3, 7)<br>\" +\n",
        "             \"• Wind columns: [3-5]=1, [6-7]=2, [8]=1<br>\" +\n",
        "             \"• Reward: -1 per step\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Policy Implementation\n",
        "\n",
        "### Epsilon-Greedy Policy\n",
        "\n",
        "Both SARSA and Q-learning use ε-greedy policy for action selection:\n",
        "- With probability ε: explore (random action)\n",
        "- With probability 1-ε: exploit (best action based on Q-values)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 3: Define Epsilon-Greedy Policy\n",
        "Purpose: Implement action selection strategy for exploration-exploitation balance\n",
        "\"\"\"\n",
        "\n",
        "def epsilon_greedy_policy(Q, state, nA, epsilon):\n",
        "    \"\"\"\n",
        "    Create epsilon-greedy policy based on Q-values\n",
        "    \n",
        "    Args:\n",
        "        Q: Action-value function dictionary\n",
        "        state: Current state\n",
        "        nA: Number of actions\n",
        "        epsilon: Exploration probability\n",
        "    \n",
        "    Returns:\n",
        "        probs: Action probabilities array\n",
        "    \"\"\"\n",
        "    # Initialize with epsilon/nA probability for each action (exploration)\n",
        "    probs = np.ones(nA) * epsilon / nA\n",
        "    \n",
        "    # Find best action based on Q-values\n",
        "    best_action = np.argmax(Q[state])\n",
        "    \n",
        "    # Add remaining probability mass to best action (exploitation)\n",
        "    probs[best_action] += 1.0 - epsilon\n",
        "    \n",
        "    return probs\n",
        "\n",
        "pretty_print(\"Policy Function Ready\",\n",
        "             \"Epsilon-greedy policy implemented<br>\" +\n",
        "             \"Balances exploration and exploitation based on ε parameter\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; margin-top: 20px; border-left: 3px solid #00acc1;\">\n",
        "    <h2 style=\"color: #00acc1; font-size: 16px; margin: 0 0 8px 0; font-weight: 600;\">Section 3: SARSA Implementation</h2>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Implementing SARSA (State-Action-Reward-State-Action), an on-policy TD control algorithm that learns the value of the policy being followed.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "### SARSA: On-Policy TD Control\n",
        "\n",
        "SARSA update rule:\n",
        "$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$\n",
        "\n",
        "Key characteristic: **On-policy** - learns about the policy being followed (ε-greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 4: Implement SARSA Algorithm\n",
        "Purpose: On-policy TD control for optimal epsilon-greedy policy\n",
        "\"\"\"\n",
        "\n",
        "# Define statistics tracking\n",
        "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
        "\n",
        "def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    SARSA algorithm: On-policy TD control\n",
        "    \n",
        "    Args:\n",
        "        env: OpenAI gym environment\n",
        "        num_episodes: Number of episodes to run\n",
        "        discount_factor: Gamma discount factor\n",
        "        alpha: TD learning rate\n",
        "        epsilon: Exploration probability\n",
        "    \n",
        "    Returns:\n",
        "        Q: Learned action-value function\n",
        "        stats: Episode statistics\n",
        "    \"\"\"\n",
        "    # Initialize Q-table with zeros\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    # Track statistics\n",
        "    stats = EpisodeStats(\n",
        "        episode_lengths=np.zeros(num_episodes),\n",
        "        episode_rewards=np.zeros(num_episodes)\n",
        "    )\n",
        "    \n",
        "    pretty_print(\"Starting SARSA Training\",\n",
        "                 f\"Episodes: {num_episodes}<br>\" +\n",
        "                 f\"α={alpha}, γ={discount_factor}, ε={epsilon}\",\n",
        "                 style='info')\n",
        "    \n",
        "    for i_episode in range(num_episodes):\n",
        "        # Progress indicator\n",
        "        if (i_episode + 1) % 100 == 0:\n",
        "            print(f\"\\rEpisode {i_episode + 1}/{num_episodes}\", end=\"\")\n",
        "            sys.stdout.flush()\n",
        "        \n",
        "        # Initialize S\n",
        "        state = env.reset()\n",
        "        \n",
        "        # Choose A from S using policy derived from Q (ε-greedy)\n",
        "        action_probs = epsilon_greedy_policy(Q, state, env.action_space.n, epsilon)\n",
        "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "        \n",
        "        # Episode loop\n",
        "        for t in itertools.count():\n",
        "            # Take action A, observe R, S'\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # Choose A' from S' using policy derived from Q (ε-greedy)\n",
        "            next_action_probs = epsilon_greedy_policy(Q, next_state, env.action_space.n, epsilon)\n",
        "            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)\n",
        "            \n",
        "            # Update statistics\n",
        "            stats.episode_rewards[i_episode] += reward\n",
        "            stats.episode_lengths[i_episode] = t\n",
        "            \n",
        "            # SARSA update: Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n",
        "            td_target = reward + discount_factor * Q[next_state][next_action]\n",
        "            td_error = td_target - Q[state][action]\n",
        "            Q[state][action] += alpha * td_error\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "            # S ← S', A ← A'\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    return Q, stats\n",
        "\n",
        "pretty_print(\"SARSA Implementation Complete\",\n",
        "             \"On-policy TD control algorithm ready<br>\" +\n",
        "             \"Updates use actual next action A' from ε-greedy policy\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Q-Learning Implementation\n",
        "\n",
        "### Q-Learning: Off-Policy TD Control\n",
        "\n",
        "Q-learning update rule:\n",
        "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$\n",
        "\n",
        "Key characteristic: **Off-policy** - learns optimal policy while following ε-greedy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 5: Implement Q-Learning Algorithm\n",
        "Purpose: Off-policy TD control for learning optimal policy\n",
        "\"\"\"\n",
        "\n",
        "def q_learning(env, num_episodes, discount_factor=0.9, alpha=0.5, epsilon=0.05):\n",
        "    \"\"\"\n",
        "    Q-Learning algorithm: Off-policy TD control\n",
        "    \n",
        "    Args:\n",
        "        env: OpenAI gym environment\n",
        "        num_episodes: Number of episodes to run\n",
        "        discount_factor: Gamma discount factor\n",
        "        alpha: Learning rate\n",
        "        epsilon: Exploration probability\n",
        "    \n",
        "    Returns:\n",
        "        Q: Learned action-value function\n",
        "        stats: Episode statistics\n",
        "    \"\"\"\n",
        "    # Initialize Q-table with zeros\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    \n",
        "    # Track statistics\n",
        "    stats = EpisodeStats(\n",
        "        episode_lengths=np.zeros(num_episodes),\n",
        "        episode_rewards=np.zeros(num_episodes)\n",
        "    )\n",
        "    \n",
        "    pretty_print(\"Starting Q-Learning Training\",\n",
        "                 f\"Episodes: {num_episodes}<br>\" +\n",
        "                 f\"α={alpha}, γ={discount_factor}, ε={epsilon}\",\n",
        "                 style='info')\n",
        "    \n",
        "    for i_episode in range(num_episodes):\n",
        "        # Progress indicator\n",
        "        if (i_episode + 1) % 100 == 0:\n",
        "            print(f\"\\rEpisode {i_episode + 1}/{num_episodes}\", end=\"\")\n",
        "            sys.stdout.flush()\n",
        "        \n",
        "        # Initialize S\n",
        "        state = env.reset()\n",
        "        \n",
        "        # Episode loop\n",
        "        for t in range(10000):  # Max steps per episode\n",
        "            # Choose A from S using policy derived from Q (ε-greedy)\n",
        "            action_probs = epsilon_greedy_policy(Q, state, env.action_space.n, epsilon)\n",
        "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "            \n",
        "            # Take action A, observe R, S'\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # Q-learning update: Q(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\n",
        "            # Key difference from SARSA: uses max Q(S',a) instead of Q(S',A')\n",
        "            td_target = reward + discount_factor * np.max(Q[next_state])\n",
        "            td_error = td_target - Q[state][action]\n",
        "            Q[state][action] += alpha * td_error\n",
        "            \n",
        "            # Update statistics\n",
        "            stats.episode_rewards[i_episode] += reward\n",
        "            stats.episode_lengths[i_episode] = t\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "            # S ← S'\n",
        "            state = next_state\n",
        "    \n",
        "    print(\"\\n\")\n",
        "    return Q, stats\n",
        "\n",
        "pretty_print(\"Q-Learning Implementation Complete\",\n",
        "             \"Off-policy TD control algorithm ready<br>\" +\n",
        "             \"Updates use maximum Q-value for next state (optimal action)\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Running Experiments\n",
        "\n",
        "Now we'll train both algorithms and compare their performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 6: Define Experiment Parameters\n",
        "Purpose: Set hyperparameters for both algorithms\n",
        "\"\"\"\n",
        "\n",
        "# Experiment parameters\n",
        "NUM_EPISODES = 300\n",
        "\n",
        "# SARSA parameters\n",
        "SARSA_PARAMS = {\n",
        "    'discount_factor': 1.0,\n",
        "    'alpha': 0.5,\n",
        "    'epsilon': 0.1\n",
        "}\n",
        "\n",
        "# Q-Learning parameters\n",
        "QLEARNING_PARAMS = {\n",
        "    'discount_factor': 0.9,\n",
        "    'alpha': 0.5,\n",
        "    'epsilon': 0.05\n",
        "}\n",
        "\n",
        "pretty_print(\"Experiment Parameters Set\",\n",
        "             f\"<strong>Common:</strong> {NUM_EPISODES} episodes<br><br>\" +\n",
        "             f\"<strong>SARSA:</strong><br>\" +\n",
        "             f\"• α={SARSA_PARAMS['alpha']}, γ={SARSA_PARAMS['discount_factor']}, ε={SARSA_PARAMS['epsilon']}<br><br>\" +\n",
        "             f\"<strong>Q-Learning:</strong><br>\" +\n",
        "             f\"• α={QLEARNING_PARAMS['alpha']}, γ={QLEARNING_PARAMS['discount_factor']}, ε={QLEARNING_PARAMS['epsilon']}\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 7: Train Both Algorithms\n",
        "Purpose: Run SARSA and Q-learning on the Windy Gridworld\n",
        "\"\"\"\n",
        "\n",
        "# Train SARSA\n",
        "pretty_print(\"Training SARSA\", \"Running on-policy TD control...\", style='info')\n",
        "Q_sarsa, stats_sarsa = sarsa(env, NUM_EPISODES, **SARSA_PARAMS)\n",
        "\n",
        "# Train Q-Learning\n",
        "pretty_print(\"Training Q-Learning\", \"Running off-policy TD control...\", style='info')\n",
        "Q_qlearning, stats_qlearning = q_learning(env, NUM_EPISODES, **QLEARNING_PARAMS)\n",
        "\n",
        "pretty_print(\"Training Complete\",\n",
        "             f\"<strong>SARSA Results:</strong><br>\" +\n",
        "             f\"• Final episode reward: {stats_sarsa.episode_rewards[-1]:.0f}<br>\" +\n",
        "             f\"• Final episode length: {stats_sarsa.episode_lengths[-1]:.0f}<br><br>\" +\n",
        "             f\"<strong>Q-Learning Results:</strong><br>\" +\n",
        "             f\"• Final episode reward: {stats_qlearning.episode_rewards[-1]:.0f}<br>\" +\n",
        "             f\"• Final episode length: {stats_qlearning.episode_lengths[-1]:.0f}\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Visualization and Analysis\n",
        "\n",
        "Let's visualize the learning curves and compare the performance of both algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 8: Create Comprehensive Comparison Plots\n",
        "Purpose: Visualize and compare SARSA vs Q-learning performance\n",
        "\"\"\"\n",
        "\n",
        "def plot_algorithm_comparison(stats_sarsa, stats_qlearning, smoothing_window=10):\n",
        "    \"\"\"\n",
        "    Create comparison plots for SARSA and Q-learning\n",
        "    \n",
        "    Args:\n",
        "        stats_sarsa: SARSA episode statistics\n",
        "        stats_qlearning: Q-learning episode statistics\n",
        "        smoothing_window: Window size for moving average\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # 1. Episode Length Comparison\n",
        "    axes[0, 0].plot(stats_sarsa.episode_lengths, alpha=0.3, color='blue', label='SARSA (raw)')\n",
        "    axes[0, 0].plot(pd.Series(stats_sarsa.episode_lengths).rolling(smoothing_window, min_periods=1).mean(),\n",
        "                   color='blue', linewidth=2, label='SARSA (smoothed)')\n",
        "    axes[0, 0].plot(stats_qlearning.episode_lengths, alpha=0.3, color='red', label='Q-Learning (raw)')\n",
        "    axes[0, 0].plot(pd.Series(stats_qlearning.episode_lengths).rolling(smoothing_window, min_periods=1).mean(),\n",
        "                   color='red', linewidth=2, label='Q-Learning (smoothed)')\n",
        "    axes[0, 0].set_xlabel('Episode')\n",
        "    axes[0, 0].set_ylabel('Episode Length')\n",
        "    axes[0, 0].set_title('Episode Length Over Time', fontweight='bold')\n",
        "    axes[0, 0].legend(loc='upper right')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Episode Reward Comparison\n",
        "    rewards_smoothed_sarsa = pd.Series(stats_sarsa.episode_rewards).rolling(smoothing_window, min_periods=1).mean()\n",
        "    rewards_smoothed_qlearning = pd.Series(stats_qlearning.episode_rewards).rolling(smoothing_window, min_periods=1).mean()\n",
        "    \n",
        "    axes[0, 1].plot(stats_sarsa.episode_rewards, alpha=0.3, color='blue', label='SARSA (raw)')\n",
        "    axes[0, 1].plot(rewards_smoothed_sarsa, color='blue', linewidth=2, label='SARSA (smoothed)')\n",
        "    axes[0, 1].plot(stats_qlearning.episode_rewards, alpha=0.3, color='red', label='Q-Learning (raw)')\n",
        "    axes[0, 1].plot(rewards_smoothed_qlearning, color='red', linewidth=2, label='Q-Learning (smoothed)')\n",
        "    axes[0, 1].set_xlabel('Episode')\n",
        "    axes[0, 1].set_ylabel('Episode Reward')\n",
        "    axes[0, 1].set_title(f'Episode Reward (Smoothed over {smoothing_window} episodes)', fontweight='bold')\n",
        "    axes[0, 1].legend(loc='lower right')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Cumulative Steps Comparison\n",
        "    axes[1, 0].plot(np.cumsum(stats_sarsa.episode_lengths), np.arange(len(stats_sarsa.episode_lengths)),\n",
        "                   color='blue', linewidth=2, label='SARSA')\n",
        "    axes[1, 0].plot(np.cumsum(stats_qlearning.episode_lengths), np.arange(len(stats_qlearning.episode_lengths)),\n",
        "                   color='red', linewidth=2, label='Q-Learning')\n",
        "    axes[1, 0].set_xlabel('Time Steps')\n",
        "    axes[1, 0].set_ylabel('Episodes')\n",
        "    axes[1, 0].set_title('Learning Speed Comparison', fontweight='bold')\n",
        "    axes[1, 0].legend(loc='lower right')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Performance Summary\n",
        "    axes[1, 1].axis('off')\n",
        "    \n",
        "    # Calculate statistics\n",
        "    sarsa_final_avg = np.mean(stats_sarsa.episode_lengths[-20:])\n",
        "    qlearning_final_avg = np.mean(stats_qlearning.episode_lengths[-20:])\n",
        "    sarsa_best = np.min(stats_sarsa.episode_lengths)\n",
        "    qlearning_best = np.min(stats_qlearning.episode_lengths)\n",
        "    \n",
        "    summary_text = f\"\"\"\n",
        "    Algorithm Comparison Summary:\n",
        "    \n",
        "    SARSA (On-Policy):\n",
        "    • Average steps (last 20 episodes): {sarsa_final_avg:.1f}\n",
        "    • Best episode: {sarsa_best:.0f} steps\n",
        "    • Convergence: Smoother, more conservative\n",
        "    • Behavior: Learns actual policy (ε-greedy)\n",
        "    \n",
        "    Q-Learning (Off-Policy):\n",
        "    • Average steps (last 20 episodes): {qlearning_final_avg:.1f}\n",
        "    • Best episode: {qlearning_best:.0f} steps\n",
        "    • Convergence: Faster to optimal\n",
        "    • Behavior: Learns optimal policy\n",
        "    \n",
        "    Key Insights:\n",
        "    • Q-learning typically finds shorter paths\n",
        "    • SARSA is safer near edges (accounts for exploration)\n",
        "    • Both converge to good policies\n",
        "    \"\"\"\n",
        "    \n",
        "    axes[1, 1].text(0.1, 0.5, summary_text, fontsize=11, verticalalignment='center',\n",
        "                   family='monospace', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
        "    \n",
        "    plt.suptitle('SARSA vs Q-Learning on Windy Gridworld', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate comparison plots\n",
        "plot_algorithm_comparison(stats_sarsa, stats_qlearning, smoothing_window=10)\n",
        "\n",
        "pretty_print(\"Analysis Complete\",\n",
        "             \"Visualizations show key differences:<br>\" +\n",
        "             \"• Q-learning converges faster to optimal policy<br>\" +\n",
        "             \"• SARSA shows more conservative, safer behavior<br>\" +\n",
        "             \"• Both successfully solve the windy gridworld problem\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 9: Test with Different Learning Rates\n",
        "Purpose: Compare algorithm sensitivity to learning rate\n",
        "\"\"\"\n",
        "\n",
        "# Test with very small alpha\n",
        "ALPHA_SMALL = 0.01\n",
        "EPISODES_EXTENDED = 1000\n",
        "\n",
        "pretty_print(\"Extended Experiment\",\n",
        "             f\"Testing with α={ALPHA_SMALL} for {EPISODES_EXTENDED} episodes<br>\" +\n",
        "             \"This demonstrates the effect of learning rate on convergence\",\n",
        "             style='info')\n",
        "\n",
        "# Train with small alpha\n",
        "Q_sarsa_small, stats_sarsa_small = sarsa(\n",
        "    env, EPISODES_EXTENDED, \n",
        "    discount_factor=SARSA_PARAMS['discount_factor'],\n",
        "    alpha=ALPHA_SMALL,\n",
        "    epsilon=SARSA_PARAMS['epsilon']\n",
        ")\n",
        "\n",
        "Q_qlearning_small, stats_qlearning_small = q_learning(\n",
        "    env, EPISODES_EXTENDED,\n",
        "    discount_factor=QLEARNING_PARAMS['discount_factor'],\n",
        "    alpha=ALPHA_SMALL,\n",
        "    epsilon=QLEARNING_PARAMS['epsilon']\n",
        ")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(pd.Series(stats_sarsa.episode_lengths).rolling(20, min_periods=1).mean(),\n",
        "         label=f'SARSA (α={SARSA_PARAMS[\"alpha\"]})', color='blue', linewidth=2)\n",
        "plt.plot(pd.Series(stats_sarsa_small.episode_lengths).rolling(20, min_periods=1).mean(),\n",
        "         label=f'SARSA (α={ALPHA_SMALL})', color='lightblue', linewidth=2)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Episode Length (smoothed)')\n",
        "plt.title('SARSA: Effect of Learning Rate', fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(pd.Series(stats_qlearning.episode_lengths).rolling(20, min_periods=1).mean(),\n",
        "         label=f'Q-Learning (α={QLEARNING_PARAMS[\"alpha\"]})', color='red', linewidth=2)\n",
        "plt.plot(pd.Series(stats_qlearning_small.episode_lengths).rolling(20, min_periods=1).mean(),\n",
        "         label=f'Q-Learning (α={ALPHA_SMALL})', color='salmon', linewidth=2)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Episode Length (smoothed)')\n",
        "plt.title('Q-Learning: Effect of Learning Rate', fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Learning Rate Impact on Convergence', fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "pretty_print(\"Learning Rate Analysis\",\n",
        "             \"<strong>Observations:</strong><br>\" +\n",
        "             f\"• Small α ({ALPHA_SMALL}) requires more episodes to converge<br>\" +\n",
        "             f\"• Large α ({SARSA_PARAMS['alpha']}) converges faster but may be less stable<br>\" +\n",
        "             \"• Q-learning is generally less sensitive to α than SARSA\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Key Findings</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>1. On-Policy vs Off-Policy:</strong> SARSA learns the value of the policy it follows (ε-greedy), while Q-learning learns the optimal policy regardless of exploration.</p>\n",
        "        <p><strong>2. Convergence Speed:</strong> Q-learning typically converges faster to the optimal policy, especially with lower epsilon values.</p>\n",
        "        <p><strong>3. Safety Near Edges:</strong> SARSA tends to learn safer paths away from grid edges due to accounting for exploratory actions in its updates.</p>\n",
        "        <p><strong>4. Wind Effect:</strong> Both algorithms successfully learn to compensate for wind, but Q-learning finds more direct paths.</p>\n",
        "        <p><strong>5. Learning Rate Sensitivity:</strong> Both algorithms are sensitive to α, with smaller values providing more stable but slower convergence.</p>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>Why does Q-learning tend to find shorter paths than SARSA in this environment?</li>\n",
        "        <li>How would changing the wind pattern affect the relative performance of the two algorithms?</li>\n",
        "        <li>What would happen if we used a decaying epsilon schedule instead of a fixed epsilon?</li>\n",
        "        <li>In what scenarios would SARSA's conservative behavior be preferable to Q-learning's optimality?</li>\n",
        "        <li>How would adding stochastic wind (varying strength) affect the comparison?</li>\n",
        "        <li>Could we combine both algorithms to get benefits of both on-policy safety and off-policy optimality?</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 6b: SARSA vs Q-Learning in Windy Gridworld</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Next: Lab 7 - Function Approximation</p>\n",
        "</div>"
      ]
    }
  ]
}
