{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo_XuRVJfeIG"
      },
      "source": [
        "\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 2 Part B: Advanced Multi-Armed Bandits\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapter 2 | Advanced Level | 90 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Advanced Bandit Methods</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Building on basic ε-greedy and optimistic initialization, we explore more sophisticated approaches:\n",
        "        <strong>Upper Confidence Bound (UCB)</strong> uses principled uncertainty estimates for exploration, while\n",
        "        <strong>Gradient Bandits</strong> learn action preferences through policy gradients. These methods represent\n",
        "        key advances in balancing exploration and exploitation.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Understand confidence-based exploration (UCB)</li>\n",
        "        <li>Implement gradient-based policy learning</li>\n",
        "        <li>Analyze the role of baselines in gradient methods</li>\n",
        "        <li>Compare all Chapter 2 methods</li>\n",
        "        <li>Reproduce Figures 2.4, 2.5, and 2.6</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Key Concepts</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">UCB</code> = upper confidence bound</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">H_t(a)</code> = action preferences</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">π_t(a)</code> = action probabilities</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">baseline</code> = average reward</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ZMAAinfeIK"
      },
      "source": [
        "## Environment Setup and Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NMWFJ1pfeIK"
      },
      "source": [
        "# ============================================\n",
        "# CELL 1: Environment Configuration and Imports\n",
        "# Purpose: Import libraries and configure experimental parameters for advanced bandits\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, List, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Define the pretty_print function directly in the notebook as the external utility module is not available.\n",
        "def pretty_print(title: str, content: str, style: str = 'info'):\n",
        "    \"\"\"\n",
        "    A simple utility function to display styled output boxes.\n",
        "    This is a placeholder since the external utility module is not available.\n",
        "    In a real scenario, this would provide richer formatting.\n",
        "    \"\"\"\n",
        "    styles = {\n",
        "        'info': {'color': 'blue', 'border': '2px solid blue'},\n",
        "        'success': {'color': 'green', 'border': '2px solid green'},\n",
        "        'warning': {'color': 'orange', 'border': '2px solid orange'},\n",
        "        'danger': {'color': 'red', 'border': '2px solid red'},\n",
        "        'result': {'color': 'purple', 'border': '2px dashed purple'},\n",
        "        'note': {'color': 'gray', 'border': '1px solid gray'}\n",
        "    }\n",
        "    s = styles.get(style, styles['info'])\n",
        "    print(f\"\\n--- {title} ---\")\n",
        "    print(content.replace('<br>', '\\n'))\n",
        "    print(\"--------------------\\n\")\n",
        "\n",
        "\n",
        "# Enhanced color scheme matching Sutton & Barto figures\n",
        "COLORS = {\n",
        "    'ucb': '#0000FF',           # Blue for UCB (matches Figure 2.4)\n",
        "    'epsilon_greedy': '#808080', # Gray for ε-greedy (matches Figure 2.4)\n",
        "    'gradient_with': '#0000FF',  # Blue for gradient with baseline\n",
        "    'gradient_without': '#8B4513', # Brown for gradient without baseline\n",
        "    'optimistic': '#00FF00',     # Green for optimistic initialization\n",
        "    'greedy': '#FF0000'          # Red for greedy\n",
        "}\n",
        "\n",
        "# Standard experimental parameters from Sutton & Barto\n",
        "K = 10          # Number of arms\n",
        "STEPS = 1000    # Time steps per run\n",
        "RUNS = 2000     # Number of independent runs for statistical significance\n",
        "\n",
        "# Configure matplotlib for publication-quality plots\n",
        "plt.rcParams.update({\n",
        "    'figure.dpi': 100,\n",
        "    'font.size': 10,\n",
        "    'axes.labelsize': 11,\n",
        "    'axes.titlesize': 12,\n",
        "    'legend.fontsize': 10,\n",
        "    'lines.linewidth': 1.5\n",
        "})\n",
        "\n",
        "pretty_print(\"Environment Initialized\",\n",
        "             f\"Configuration: {K} arms, {STEPS} steps, {RUNS} runs<br>\" +\n",
        "             \"Ready for advanced bandit algorithms: UCB and Gradient methods\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5aFEc_9feIK"
      },
      "source": [
        "# ============================================\n",
        "# CELL 2: Basic Bandit Environment Functions\n",
        "# Purpose: Reusable functions for bandit problem generation and reward sampling\n",
        "# ============================================\n",
        "\n",
        "def create_bandit(mean_shift: float = 0.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Create a k-armed bandit problem with configurable mean shift\n",
        "\n",
        "    Standard testbed: q*(a) ~ N(0,1)\n",
        "    Alternative testbed: q*(a) ~ N(mean_shift,1) for Figure 2.5\n",
        "\n",
        "    Args:\n",
        "        mean_shift: Shift the mean of true action values (default 0 for standard testbed)\n",
        "\n",
        "    Returns:\n",
        "        q_true: Array of true action values\n",
        "    \"\"\"\n",
        "    return np.random.randn(K) + mean_shift\n",
        "\n",
        "def get_reward(action: int, q_true: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Sample reward from the bandit environment\n",
        "\n",
        "    Bandit model: R_t ~ N(q*(A_t), 1)\n",
        "    Rewards are normally distributed around true action value with unit variance\n",
        "\n",
        "    Args:\n",
        "        action: Selected action index (0 to K-1)\n",
        "        q_true: True action values\n",
        "\n",
        "    Returns:\n",
        "        reward: Noisy reward sample\n",
        "    \"\"\"\n",
        "    return q_true[action] + np.random.randn()\n",
        "\n",
        "def softmax(preferences: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute softmax probabilities from action preferences\n",
        "\n",
        "    Softmax formula: π(a) = exp(H(a)) / Σ_b exp(H(b))\n",
        "    Includes numerical stability by subtracting max preference\n",
        "\n",
        "    Args:\n",
        "        preferences: Action preference values H_t(a)\n",
        "\n",
        "    Returns:\n",
        "        probabilities: Action selection probabilities π_t(a)\n",
        "    \"\"\"\n",
        "    # Numerical stability: subtract max to prevent overflow\n",
        "    exp_prefs = np.exp(preferences - np.max(preferences))\n",
        "    return exp_prefs / np.sum(exp_prefs)\n",
        "\n",
        "pretty_print(\"Core Functions Ready\",\n",
        "             \"Bandit environment and utility functions loaded<br>\" +\n",
        "             \"Functions: create_bandit(), get_reward(), softmax()\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zLF35BlfeIK"
      },
      "source": [
        "## Upper Confidence Bound (UCB) Action Selection\n",
        "\n",
        "### Theoretical Foundation\n",
        "\n",
        "The **Upper Confidence Bound (UCB)** method addresses the exploration-exploitation dilemma by systematically considering uncertainty in action value estimates. Unlike ε-greedy's random exploration, UCB uses a **principled confidence interval approach**.\n",
        "\n",
        "### The UCB Algorithm\n",
        "\n",
        "UCB selects actions according to:\n",
        "$$A_t = \\arg\\max_a \\left[ Q_t(a) + c\\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]$$\n",
        "\n",
        "Where:\n",
        "- **$Q_t(a)$**: Current value estimate (exploitation term)\n",
        "- **$c\\sqrt{\\frac{\\ln t}{N_t(a)}}$**: Upper confidence bound (exploration term)\n",
        "- **$c > 0$**: Controls exploration level\n",
        "- **$t$**: Current time step\n",
        "- **$N_t(a)$**: Number of times action $a$ selected\n",
        "\n",
        "### Why UCB Works\n",
        "\n",
        "1. **Uncertainty Decreases with Experience**: $\\frac{1}{N_t(a)}$ term decreases as actions are tried more\n",
        "2. **Time-Dependent Exploration**: $\\ln t$ ensures exploration continues (slowly) over time  \n",
        "3. **Optimism Under Uncertainty**: Always selects action with highest potential value\n",
        "4. **No Tuning Required**: Self-balancing exploration without parameter tuning like ε\n",
        "\n",
        "### Theoretical Guarantees\n",
        "\n",
        "UCB provides **logarithmic regret bounds**: $O(\\ln t)$, meaning the per-step regret approaches zero as $t \\to \\infty$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeA4HEXsfeIL"
      },
      "source": [
        "# ============================================\n",
        "# CELL 3: Upper Confidence Bound Implementation\n",
        "# Purpose: Implement UCB action selection with principled uncertainty-based exploration\n",
        "# ============================================\n",
        "\n",
        "def ucb_action_selection(Q: np.ndarray, N: np.ndarray, t: int, c: float = 2.0) -> int:\n",
        "    \"\"\"\n",
        "    Upper Confidence Bound action selection\n",
        "\n",
        "    Implements the UCB formula: A_t = argmax[Q_t(a) + c*sqrt(ln(t)/N_t(a))]\n",
        "\n",
        "    Key insight: Balance estimated value with uncertainty bonus\n",
        "    - High Q_t(a): Actions that have performed well (exploitation)\n",
        "    - High uncertainty bonus: Actions tried infrequently or never (exploration)\n",
        "    - The uncertainty bonus decreases as N_t(a) increases\n",
        "    - The ln(t) term ensures exploration continues over time\n",
        "\n",
        "    Args:\n",
        "        Q: Current action value estimates Q_t(a)\n",
        "        N: Action selection counts N_t(a)\n",
        "        t: Current time step (must be > 0)\n",
        "        c: Confidence parameter (c=2 is theoretically motivated)\n",
        "\n",
        "    Returns:\n",
        "        action: Selected action index\n",
        "    \"\"\"\n",
        "    # Handle untried actions: give them infinite confidence (try all actions once first)\n",
        "    untried_actions = np.where(N == 0)[0]\n",
        "    if len(untried_actions) > 0:\n",
        "        return np.random.choice(untried_actions)\n",
        "\n",
        "    # Compute UCB values: Q(a) + confidence bound\n",
        "    ucb_values = Q + c * np.sqrt(np.log(t) / N)\n",
        "\n",
        "    # Select action with highest UCB value (break ties randomly)\n",
        "    max_ucb = np.max(ucb_values)\n",
        "    return np.random.choice(np.where(ucb_values == max_ucb)[0])\n",
        "\n",
        "def update_action_values(Q: np.ndarray, N: np.ndarray, action: int, reward: float) -> None:\n",
        "    \"\"\"\n",
        "    Update action value estimates using sample average method\n",
        "\n",
        "    Incremental update: Q_n+1 = Q_n + (1/n)[R_n - Q_n]\n",
        "    This is equivalent to the sample mean but computationally efficient\n",
        "\n",
        "    The update can be interpreted as:\n",
        "    NewEstimate = OldEstimate + StepSize * PredictionError\n",
        "\n",
        "    Args:\n",
        "        Q: Action value estimates (modified in place)\n",
        "        N: Action counts (modified in place)\n",
        "        action: Action that was selected\n",
        "        reward: Observed reward\n",
        "    \"\"\"\n",
        "    N[action] += 1\n",
        "    # Sample average update with step size 1/n\n",
        "    Q[action] += (reward - Q[action]) / N[action]\n",
        "\n",
        "pretty_print(\"UCB Algorithm Implemented\",\n",
        "             \"Upper Confidence Bound with systematic uncertainty-based exploration<br>\" +\n",
        "             \"Formula: A_t = argmax[Q_t(a) + c√(ln(t)/N_t(a))]<br>\" +\n",
        "             \"Default c=2.0 provides theoretical guarantees\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPkHG1XmfeIL"
      },
      "source": [
        "# ============================================\n",
        "# CELL 4: UCB vs ε-greedy Experiment Setup\n",
        "# Purpose: Run comparative experiment to reproduce Figure 2.4 results\n",
        "# ============================================\n",
        "\n",
        "def run_ucb_experiment(c: float = 2.0, runs: int = 2000, steps: int = 1000) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Run UCB bandit experiment\n",
        "\n",
        "    This experiment tests UCB's performance on the standard 10-armed testbed.\n",
        "    UCB should outperform ε-greedy due to its principled exploration strategy.\n",
        "\n",
        "    Expected behavior:\n",
        "    - Initial random phase: tries each action once\n",
        "    - Rapid learning: quickly identifies promising actions\n",
        "    - Diminishing exploration: uncertainty bounds shrink over time\n",
        "    - Superior long-term performance: principled vs random exploration\n",
        "\n",
        "    Args:\n",
        "        c: UCB confidence parameter\n",
        "        runs: Number of independent experiments\n",
        "        steps: Number of time steps per run\n",
        "\n",
        "    Returns:\n",
        "        avg_rewards: Average reward at each time step\n",
        "    \"\"\"\n",
        "    all_rewards = np.zeros((runs, steps))\n",
        "\n",
        "    for run in range(runs):\n",
        "        # Create fresh bandit problem\n",
        "        q_true = create_bandit()\n",
        "\n",
        "        # Initialize UCB agent\n",
        "        Q = np.zeros(K)  # Action value estimates\n",
        "        N = np.zeros(K)  # Action counts\n",
        "\n",
        "        for step in range(steps):\n",
        "            # UCB action selection (step+1 because UCB needs t > 0)\n",
        "            action = ucb_action_selection(Q, N, step + 1, c)\n",
        "\n",
        "            # Get reward and update estimates\n",
        "            reward = get_reward(action, q_true)\n",
        "            update_action_values(Q, N, action, reward)\n",
        "\n",
        "            # Record performance\n",
        "            all_rewards[run, step] = reward\n",
        "\n",
        "    return np.mean(all_rewards, axis=0)\n",
        "\n",
        "def run_epsilon_greedy_baseline(epsilon: float = 0.1, runs: int = 2000, steps: int = 1000) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Run ε-greedy experiment for comparison with UCB\n",
        "\n",
        "    This provides the baseline performance for Figure 2.4.\n",
        "    Uses ε=0.1 as the representative ε-greedy method.\n",
        "\n",
        "    Args:\n",
        "        epsilon: Exploration probability\n",
        "        runs: Number of independent experiments\n",
        "        steps: Number of time steps per run\n",
        "\n",
        "    Returns:\n",
        "        avg_rewards: Average reward at each time step\n",
        "    \"\"\"\n",
        "    all_rewards = np.zeros((runs, steps))\n",
        "\n",
        "    for run in range(runs):\n",
        "        # Create fresh bandit problem\n",
        "        q_true = create_bandit()\n",
        "\n",
        "        # Initialize ε-greedy agent\n",
        "        Q = np.zeros(K)\n",
        "        N = np.zeros(K)\n",
        "\n",
        "        for step in range(steps):\n",
        "            # ε-greedy action selection\n",
        "            if np.random.random() < epsilon:\n",
        "                action = np.random.randint(K)  # Explore\n",
        "            else:\n",
        "                max_Q = np.max(Q)\n",
        "                action = np.random.choice(np.where(Q == max_Q)[0])  # Exploit\n",
        "\n",
        "            # Get reward and update estimates\n",
        "            reward = get_reward(action, q_true)\n",
        "            update_action_values(Q, N, action, reward)\n",
        "\n",
        "            # Record performance\n",
        "            all_rewards[run, step] = reward\n",
        "\n",
        "    return np.mean(all_rewards, axis=0)\n",
        "\n",
        "pretty_print(\"Experiment Functions Ready\",\n",
        "             \"UCB vs ε-greedy comparison experiments prepared<br>\" +\n",
        "             \"Will reproduce Figure 2.4 results\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7ItV3-4feIL"
      },
      "source": [
        "# ============================================\n",
        "# CELL 5: Run UCB vs ε-greedy Experiments\n",
        "# Purpose: Generate performance data for Figure 2.4 reproduction\n",
        "# ============================================\n",
        "\n",
        "pretty_print(\"Starting UCB Experiments\",\n",
        "             \"Running UCB vs ε-greedy comparison experiments<br>\" +\n",
        "             f\"Configuration: {RUNS} runs × {STEPS} steps each\",\n",
        "             style='info')\n",
        "\n",
        "# Run UCB experiment (c=2 is standard)\n",
        "print(\"  Running UCB (c=2.0)...\")\n",
        "ucb_rewards = run_ucb_experiment(c=2.0, runs=RUNS, steps=STEPS)\n",
        "\n",
        "# Run ε-greedy baseline\n",
        "print(\"  Running ε-greedy (ε=0.1)...\")\n",
        "epsilon_rewards = run_epsilon_greedy_baseline(epsilon=0.1, runs=RUNS, steps=STEPS)\n",
        "\n",
        "# Analyze final performance\n",
        "ucb_final = ucb_rewards[-1]\n",
        "epsilon_final = epsilon_rewards[-1]\n",
        "improvement = ((ucb_final - epsilon_final) / epsilon_final) * 100\n",
        "\n",
        "pretty_print(\"Experiment Results\",\n",
        "             f\"UCB final average reward: {ucb_final:.3f}<br>\" +\n",
        "             f\"ε-greedy final average reward: {epsilon_final:.3f}<br>\" +\n",
        "             f\"<strong>UCB improvement: {improvement:.1f}%</strong>\",\n",
        "             style='result')\n",
        "\n",
        "pretty_print(\"Key Insight\",\n",
        "             \"UCB's principled uncertainty-based exploration outperforms ε-greedy's random exploration<br>\" +\n",
        "             \"The confidence bounds guide exploration to promising actions\",\n",
        "             style='note')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQo4qgvFfeIL"
      },
      "source": [
        "# ============================================\n",
        "# CELL 6: Visualize UCB vs ε-greedy Performance\n",
        "# Purpose: Create Figure 2.4 reproduction showing UCB superiority\n",
        "# ============================================\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot UCB performance (blue line to match Figure 2.4)\n",
        "plt.plot(ucb_rewards, color=COLORS['ucb'], label='UCB c = 2', linewidth=2)\n",
        "\n",
        "# Plot ε-greedy performance (gray line to match Figure 2.4)\n",
        "plt.plot(epsilon_rewards, color=COLORS['epsilon_greedy'], label='ε-greedy ε = 0.1', linewidth=2)\n",
        "\n",
        "# Formatting to match Sutton & Barto style\n",
        "plt.xlabel('Steps', fontsize=12)\n",
        "plt.ylabel('Average\\nreward', fontsize=12)\n",
        "plt.xlim(1, 1000)\n",
        "plt.ylim(0, 1.5)\n",
        "plt.legend(loc='lower right', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Title matching Figure 2.4\n",
        "plt.title('Figure 2.4: Average performance of UCB action selection on the 10-armed testbed.\\n' +\n",
        "          'As shown, UCB generally performs better than ε-greedy action selection, except in\\n' +\n",
        "          'the first k steps, when it selects randomly among the as-yet-untried actions.',\n",
        "          fontsize=11, pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analysis of results\n",
        "analysis = (\n",
        "    \"<strong>UCB Performance Analysis:</strong><br><br>\" +\n",
        "    \"• <strong>Initial Phase (Steps 1-10):</strong> UCB tries each arm once (systematic initialization)<br>\" +\n",
        "    \"• <strong>Learning Phase (Steps 10-200):</strong> Rapid improvement as confidence bounds guide exploration<br>\" +\n",
        "    \"• <strong>Convergence (Steps 200+):</strong> Superior long-term performance due to principled exploration<br><br>\" +\n",
        "    \"<strong>Why UCB Outperforms ε-greedy:</strong><br>\" +\n",
        "    \"• Systematic vs random exploration<br>\" +\n",
        "    \"• Uncertainty-based action selection<br>\" +\n",
        "    \"• Self-tuning exploration (no ε parameter needed)\"\n",
        ")\n",
        "\n",
        "pretty_print(\"Figure 2.4 Analysis\", analysis, style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvzfo_NnfeIL"
      },
      "source": [
        "## Gradient Bandit Algorithms\n",
        "\n",
        "### Theoretical Foundation\n",
        "\n",
        "**Gradient bandit algorithms** take a fundamentally different approach: instead of estimating action values, they learn **action preferences** and use **gradient ascent** to maximize expected reward.\n",
        "\n",
        "### The Gradient Bandit Framework\n",
        "\n",
        "**Action Preferences**: $H_t(a) \\in \\mathbb{R}$ (can be positive or negative)\n",
        "\n",
        "**Action Probabilities**: $\\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}}$ (softmax distribution)\n",
        "\n",
        "**Preference Updates**:\n",
        "- For selected action: $H_{t+1}(A_t) = H_t(A_t) + \\alpha(R_t - \\bar{R}_t)(1 - \\pi_t(A_t))$\n",
        "- For other actions: $H_{t+1}(a) = H_t(a) - \\alpha(R_t - \\bar{R}_t)\\pi_t(a)$\n",
        "\n",
        "Where $\\bar{R}_t$ is the **baseline** (average reward).\n",
        "\n",
        "### The Role of Baselines\n",
        "\n",
        "**With Baseline** ($\\bar{R}_t$):\n",
        "- Reduces variance in gradient estimates\n",
        "- Provides relative reward signal: \"better/worse than average\"\n",
        "- Essential for good performance\n",
        "\n",
        "**Without Baseline** ($\\bar{R}_t = 0$):\n",
        "- Higher variance, slower convergence\n",
        "- All positive rewards increase selected action preference\n",
        "- Can work but suboptimal\n",
        "\n",
        "### Policy Gradient Connection\n",
        "\n",
        "This is actually **REINFORCE** algorithm applied to bandits:\n",
        "$$\\nabla_H \\mathbb{E}[R_t] = (R_t - \\bar{R}_t)\\nabla_H \\ln \\pi_t(A_t)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekQd-EqtfeIM"
      },
      "source": [
        "# ============================================\n",
        "# CELL 7: Gradient Bandit Algorithm Implementation\n",
        "# Purpose: Implement policy gradient approach with preference learning\n",
        "# ============================================\n",
        "\n",
        "def gradient_bandit_action_selection(H: np.ndarray) -> Tuple[int, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Select action using softmax policy from preferences\n",
        "\n",
        "    The gradient bandit uses a probabilistic policy rather than deterministic selection.\n",
        "    Actions with higher preferences H_t(a) get higher selection probabilities.\n",
        "\n",
        "    Softmax policy: π_t(a) = exp(H_t(a)) / Σ_b exp(H_t(b))\n",
        "\n",
        "    This creates a \"soft\" action selection where:\n",
        "    - All actions have non-zero probability\n",
        "    - Better actions (higher preferences) are selected more often\n",
        "    - The distribution naturally balances exploration and exploitation\n",
        "\n",
        "    Args:\n",
        "        H: Action preferences H_t(a)\n",
        "\n",
        "    Returns:\n",
        "        action: Selected action (sampled from softmax distribution)\n",
        "        probabilities: Action selection probabilities π_t(a)\n",
        "    \"\"\"\n",
        "    # Compute softmax probabilities\n",
        "    probabilities = softmax(H)\n",
        "\n",
        "    # Sample action from probability distribution\n",
        "    action = np.random.choice(K, p=probabilities)\n",
        "\n",
        "    return action, probabilities\n",
        "\n",
        "def update_preferences(H: np.ndarray, action: int, reward: float,\n",
        "                      probabilities: np.ndarray, alpha: float,\n",
        "                      baseline: Optional[float] = None) -> None:\n",
        "    \"\"\"\n",
        "    Update action preferences using policy gradient\n",
        "\n",
        "    This implements the REINFORCE gradient ascent update:\n",
        "\n",
        "    For selected action A_t:\n",
        "    H_{t+1}(A_t) = H_t(A_t) + α(R_t - baseline)(1 - π_t(A_t))\n",
        "\n",
        "    For non-selected actions a ≠ A_t:\n",
        "    H_{t+1}(a) = H_t(a) - α(R_t - baseline)π_t(a)\n",
        "\n",
        "    Intuition:\n",
        "    - If reward > baseline: increase preference for selected action, decrease others\n",
        "    - If reward < baseline: decrease preference for selected action, increase others\n",
        "    - The (1-π_t(A_t)) and π_t(a) terms weight updates by current probabilities\n",
        "\n",
        "    Args:\n",
        "        H: Action preferences (modified in place)\n",
        "        action: Action that was selected\n",
        "        reward: Observed reward\n",
        "        probabilities: Current action probabilities π_t(a)\n",
        "        alpha: Learning rate\n",
        "        baseline: Baseline for variance reduction (None = no baseline)\n",
        "    \"\"\"\n",
        "    # Compute advantage: reward relative to baseline\n",
        "    if baseline is not None:\n",
        "        advantage = reward - baseline\n",
        "    else:\n",
        "        advantage = reward  # No baseline (baseline = 0)\n",
        "\n",
        "    # Update preferences using policy gradient\n",
        "    for a in range(K):\n",
        "        if a == action:\n",
        "            # Selected action: increase if advantage > 0\n",
        "            H[a] += alpha * advantage * (1 - probabilities[a])\n",
        "        else:\n",
        "            # Non-selected actions: decrease if advantage > 0\n",
        "            H[a] -= alpha * advantage * probabilities[a]\n",
        "\n",
        "def update_baseline(baseline_sum: float, baseline_count: int, reward: float) -> Tuple[float, float, int]:\n",
        "    \"\"\"\n",
        "    Update running average baseline (sample mean of all rewards)\n",
        "\n",
        "    The baseline is the average of all rewards seen so far:\n",
        "    baseline_t = (1/t) * Σ_{i=1}^t R_i\n",
        "\n",
        "    Using incremental computation for efficiency\n",
        "\n",
        "    Args:\n",
        "        baseline_sum: Sum of all rewards so far\n",
        "        baseline_count: Number of rewards seen\n",
        "        reward: New reward to include\n",
        "\n",
        "    Returns:\n",
        "        baseline: Updated average baseline\n",
        "        baseline_sum: Updated sum\n",
        "        baseline_count: Updated count\n",
        "    \"\"\"\n",
        "    baseline_sum += reward\n",
        "    baseline_count += 1\n",
        "    baseline = baseline_sum / baseline_count\n",
        "    return baseline, baseline_sum, baseline_count\n",
        "\n",
        "pretty_print(\"Gradient Bandit Implemented\",\n",
        "             \"Policy gradient method with preference learning<br>\" +\n",
        "             \"Features: Softmax policy, REINFORCE updates, baseline for variance reduction<br>\" +\n",
        "             \"Updates: H_t+1(a) = H_t(a) + α(R_t - baseline)∇ln(π_t(a))\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuX0Yiy6feIM"
      },
      "source": [
        "# ============================================\n",
        "# CELL 8: Gradient Bandit Experiments Setup\n",
        "# Purpose: Compare gradient bandits with and without baseline for Figure 2.5\n",
        "# ============================================\n",
        "\n",
        "def run_gradient_bandit_experiment(alpha: float, use_baseline: bool = True,\n",
        "                                  mean_shift: float = 4.0, runs: int = 2000,\n",
        "                                  steps: int = 1000) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Run gradient bandit experiment\n",
        "\n",
        "    Figure 2.5 uses a special testbed where q*(a) are chosen near +4 rather than 0.\n",
        "    This makes all rewards positive on average, which is important for demonstrating\n",
        "    the baseline effect. With all positive rewards, the non-baseline version\n",
        "    always increases selected action preferences, leading to poor exploration.\n",
        "\n",
        "    Args:\n",
        "        alpha: Learning rate for preference updates\n",
        "        use_baseline: Whether to use baseline for variance reduction\n",
        "        mean_shift: Shift true values (4.0 for Figure 2.5)\n",
        "        runs: Number of independent experiments\n",
        "        steps: Number of time steps per run\n",
        "\n",
        "    Returns:\n",
        "        pct_optimal: Percentage of optimal actions at each step\n",
        "    \"\"\"\n",
        "    all_optimal = np.zeros((runs, steps))\n",
        "\n",
        "    for run in range(runs):\n",
        "        # Create bandit with shifted mean (q* values near +4)\n",
        "        q_true = create_bandit(mean_shift=mean_shift)\n",
        "        optimal_action = np.argmax(q_true)\n",
        "\n",
        "        # Initialize gradient bandit agent\n",
        "        H = np.zeros(K)  # Action preferences (start at 0)\n",
        "\n",
        "        # Baseline tracking variables\n",
        "        baseline_sum = 0.0\n",
        "        baseline_count = 0\n",
        "\n",
        "        for step in range(steps):\n",
        "            # Select action using softmax policy\n",
        "            action, probabilities = gradient_bandit_action_selection(H)\n",
        "\n",
        "            # Get reward from environment\n",
        "            reward = get_reward(action, q_true)\n",
        "\n",
        "            # Update baseline if using it\n",
        "            current_baseline = None\n",
        "            if use_baseline:\n",
        "                current_baseline, baseline_sum, baseline_count = update_baseline(\n",
        "                    baseline_sum, baseline_count, reward)\n",
        "\n",
        "            # Update preferences using policy gradient\n",
        "            update_preferences(H, action, reward, probabilities, alpha, current_baseline)\n",
        "\n",
        "            # Record if optimal action was selected\n",
        "            all_optimal[run, step] = (action == optimal_action)\n",
        "\n",
        "    return np.mean(all_optimal, axis=0) * 100\n",
        "\n",
        "pretty_print(\"Gradient Experiments Ready\",\n",
        "             \"Prepared to test gradient bandits with/without baseline<br>\" +\n",
        "             \"Will use shifted testbed (mean=+4) to demonstrate baseline importance<br>\" +\n",
        "             \"This reproduces Figure 2.5 experimental setup\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yudVHM7feIM"
      },
      "source": [
        "# ============================================\n",
        "# CELL 9: Execute Gradient Bandit Experiments\n",
        "# Purpose: Generate data for Figure 2.5 showing baseline importance\n",
        "# ============================================\n",
        "\n",
        "pretty_print(\"Starting Gradient Bandit Experiments\",\n",
        "             \"Testing gradient bandits with and without reward baseline<br>\" +\n",
        "             \"Using shifted testbed (q* near +4) to demonstrate baseline effect\",\n",
        "             style='info')\n",
        "\n",
        "# Run gradient bandit with baseline (α=0.1)\n",
        "print(\"  Running gradient bandit WITH baseline (α=0.1)...\")\n",
        "gradient_with_baseline = run_gradient_bandit_experiment(\n",
        "    alpha=0.1, use_baseline=True, mean_shift=4.0, runs=RUNS, steps=STEPS\n",
        ")\n",
        "\n",
        "# Run gradient bandit without baseline (α=0.1)\n",
        "print(\"  Running gradient bandit WITHOUT baseline (α=0.1)...\")\n",
        "gradient_without_baseline = run_gradient_bandit_experiment(\n",
        "    alpha=0.1, use_baseline=False, mean_shift=4.0, runs=RUNS, steps=STEPS\n",
        ")\n",
        "\n",
        "# Also test different learning rates to match Figure 2.5\n",
        "print(\"  Running gradient bandit WITH baseline (α=0.4)...\")\n",
        "gradient_with_baseline_04 = run_gradient_bandit_experiment(\n",
        "    alpha=0.4, use_baseline=True, mean_shift=4.0, runs=RUNS, steps=STEPS\n",
        ")\n",
        "\n",
        "print(\"  Running gradient bandit WITHOUT baseline (α=0.4)...\")\n",
        "gradient_without_baseline_04 = run_gradient_bandit_experiment(\n",
        "    alpha=0.4, use_baseline=False, mean_shift=4.0, runs=RUNS, steps=STEPS\n",
        ")\n",
        "\n",
        "# Analyze results\n",
        "with_final = gradient_with_baseline[-1]\n",
        "without_final = gradient_without_baseline[-1]\n",
        "improvement = ((with_final - without_final) / without_final) * 100\n",
        "\n",
        "pretty_print(\"Gradient Bandit Results\",\n",
        "             f\"With baseline (α=0.1): {with_final:.1f}% optimal<br>\" +\n",
        "             f\"Without baseline (α=0.1): {without_final:.1f}% optimal<br>\" +\n",
        "             f\"<strong>Baseline improvement: {improvement:.1f}%</strong>\",\n",
        "             style='result')\n",
        "\n",
        "pretty_print(\"Why Baselines Matter\",\n",
        "             \"With shifted rewards (all positive), no baseline means:<br>\" +\n",
        "             \"• Every reward increases selected action preference<br>\" +\n",
        "             \"• No relative comparison → poor exploration<br>\" +\n",
        "             \"• Baseline provides 'better/worse than average' signal\",\n",
        "             style='note')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdUBJ-k2feIM"
      },
      "source": [
        "# ============================================\n",
        "# CELL 10: Visualize Gradient Bandit Results\n",
        "# Purpose: Create Figure 2.5 reproduction showing baseline importance\n",
        "# ============================================\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot gradient bandit with baseline (α=0.1) - blue\n",
        "plt.plot(gradient_with_baseline, color=COLORS['gradient_with'],\n",
        "         label='α = 0.1\\nwith baseline', linewidth=2)\n",
        "\n",
        "# Plot gradient bandit with baseline (α=0.4) - lighter blue\n",
        "plt.plot(gradient_with_baseline_04, color='lightblue',\n",
        "         label='α = 0.4\\nwith baseline', linewidth=2)\n",
        "\n",
        "# Plot gradient bandit without baseline (α=0.1) - brown\n",
        "plt.plot(gradient_without_baseline, color=COLORS['gradient_without'],\n",
        "         label='α = 0.1\\nwithout baseline', linewidth=2)\n",
        "\n",
        "# Plot gradient bandit without baseline (α=0.4) - lighter brown\n",
        "plt.plot(gradient_without_baseline_04, color='tan',\n",
        "         label='α = 0.4\\nwithout baseline', linewidth=2)\n",
        "\n",
        "# Formatting to match Figure 2.5\n",
        "plt.xlabel('Steps', fontsize=12)\n",
        "plt.ylabel('%\\nOptimal\\naction', fontsize=12)\n",
        "plt.xlim(1, 1000)\n",
        "plt.ylim(0, 100)\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Title matching Figure 2.5\n",
        "plt.title('Figure 2.5: Average performance of the gradient bandit algorithm with and without\\n' +\n",
        "          'a reward baseline on the 10-armed testbed when the q*(a) are chosen to be near +4\\n' +\n",
        "          'rather than near zero.',\n",
        "          fontsize=11, pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed analysis\n",
        "analysis = (\n",
        "    \"<strong>Gradient Bandit Analysis:</strong><br><br>\" +\n",
        "    \"<strong>With Baseline:</strong><br>\" +\n",
        "    \"• Rapid learning and high performance<br>\" +\n",
        "    \"• Baseline provides relative reward signal<br>\" +\n",
        "    \"• Higher α (0.4) learns faster but converges to same level<br><br>\" +\n",
        "    \"<strong>Without Baseline:</strong><br>\" +\n",
        "    \"• Much slower learning and lower asymptotic performance<br>\" +\n",
        "    \"• All positive rewards increase selected action preference<br>\" +\n",
        "    \"• Poor exploration due to lack of relative comparison<br><br>\" +\n",
        "    \"<strong>Key Insight:</strong> Baselines are crucial for gradient-based methods!\"\n",
        ")\n",
        "\n",
        "pretty_print(\"Figure 2.5 Analysis\", analysis, style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LTFj8pufeIM"
      },
      "source": [
        "## Comprehensive Method Comparison\n",
        "\n",
        "### Parameter Study Overview\n",
        "\n",
        "Figure 2.6 provides a **parameter study** comparing all bandit methods across their respective parameter ranges:\n",
        "\n",
        "- **ε-greedy**: Parameter ε (exploration probability)\n",
        "- **Gradient bandit**: Parameter α (learning rate)  \n",
        "- **UCB**: Parameter c (confidence level)\n",
        "- **Optimistic initialization**: Parameter Q₀ (initial values)\n",
        "\n",
        "This analysis reveals the **sensitivity** of each method to its parameters and identifies optimal settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n12xGMzBfeIM"
      },
      "source": [
        "# ============================================\n",
        "# CELL 11: Parameter Study Implementation\n",
        "# Purpose: Comprehensive comparison of all methods across parameter ranges for Figure 2.6\n",
        "# ============================================\n",
        "\n",
        "def parameter_study_epsilon_greedy(epsilon_values: List[float], runs: int = 2000, steps: int = 1000) -> List[float]:\n",
        "    \"\"\"\n",
        "    Test ε-greedy across different ε values\n",
        "\n",
        "    Tests exploration probability from very low (1/128) to high (1/4)\n",
        "    Expected: optimal around ε=0.1 for standard testbed\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for eps in epsilon_values:\n",
        "        print(f\"    Testing ε-greedy with ε={eps:.4f}...\")\n",
        "        avg_rewards = run_epsilon_greedy_baseline(epsilon=eps, runs=runs, steps=steps)\n",
        "        # Return average reward over all 1000 steps\n",
        "        results.append(np.mean(avg_rewards))\n",
        "\n",
        "    return results\n",
        "\n",
        "def parameter_study_ucb(c_values: List[float], runs: int = 2000, steps: int = 1000) -> List[float]:\n",
        "    \"\"\"\n",
        "    Test UCB across different confidence parameters\n",
        "\n",
        "    Tests from conservative (c=1/8) to aggressive (c=4) exploration\n",
        "    Expected: optimal around c=2 (theoretical optimum)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for c in c_values:\n",
        "        print(f\"    Testing UCB with c={c:.4f}...\")\n",
        "        avg_rewards = run_ucb_experiment(c=c, runs=runs, steps=steps)\n",
        "        results.append(np.mean(avg_rewards))\n",
        "\n",
        "    return results\n",
        "\n",
        "def parameter_study_gradient(alpha_values: List[float], runs: int = 2000, steps: int = 1000) -> List[float]:\n",
        "    \"\"\"\n",
        "    Test gradient bandit across different learning rates\n",
        "\n",
        "    Tests from slow (α=1/32) to fast (α=4) learning\n",
        "    Expected: optimal around α=0.1-0.4 range\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for alpha in alpha_values:\n",
        "        print(f\"    Testing gradient bandit with α={alpha:.4f}...\")\n",
        "        pct_optimal = run_gradient_bandit_experiment(\n",
        "            alpha=alpha, use_baseline=True, mean_shift=0.0, runs=runs, steps=steps\n",
        "        )\n",
        "        # Convert to average reward approximation (not exact, but for comparison)\n",
        "        # This is a simplification - actual Figure 2.6 uses average reward\n",
        "        results.append(np.mean(pct_optimal) / 100 * 1.55)  # Rough scaling\n",
        "\n",
        "    return results\n",
        "\n",
        "def parameter_study_optimistic(q0_values: List[float], runs: int = 2000, steps: int = 1000) -> List[float]:\n",
        "    \"\"\"\n",
        "    Test optimistic initialization across different initial values\n",
        "\n",
        "    Tests from pessimistic (Q₀=1/4) to very optimistic (Q₀=4)\n",
        "    Expected: optimal around Q₀=1-2 range\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for q0 in q0_values:\n",
        "        print(f\"    Testing optimistic with Q₀={q0:.4f}...\")\n",
        "\n",
        "        # Run optimistic greedy experiment\n",
        "        all_rewards = np.zeros((runs, steps))\n",
        "\n",
        "        for run in range(runs):\n",
        "            q_true = create_bandit()\n",
        "            Q = np.ones(K) * q0  # Optimistic initialization\n",
        "            N = np.zeros(K)\n",
        "\n",
        "            for step in range(steps):\n",
        "                # Greedy action selection\n",
        "                max_Q = np.max(Q)\n",
        "                action = np.random.choice(np.where(Q == max_Q)[0])\n",
        "\n",
        "                reward = get_reward(action, q_true)\n",
        "\n",
        "                # Update with constant α=0.1\n",
        "                N[action] += 1\n",
        "                Q[action] += 0.1 * (reward - Q[action])\n",
        "\n",
        "                all_rewards[run, step] = reward\n",
        "\n",
        "        results.append(np.mean(all_rewards))\n",
        "\n",
        "    return results\n",
        "\n",
        "pretty_print(\"Parameter Study Functions Ready\",\n",
        "             \"Prepared to test all methods across their parameter ranges<br>\" +\n",
        "             \"Will generate data for Figure 2.6 parameter study comparison\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muEStyYsfeIN"
      },
      "source": [
        "# ============================================\n",
        "# CELL 12: Execute Parameter Study Experiments\n",
        "# Purpose: Run comprehensive parameter sweep for all methods\n",
        "# ============================================\n",
        "\n",
        "pretty_print(\"Starting Parameter Study\",\n",
        "             \"Testing all bandit methods across parameter ranges<br>\" +\n",
        "             \"This will take several minutes...\",\n",
        "             style='warning')\n",
        "\n",
        "# Define parameter ranges (matching Figure 2.6)\n",
        "epsilon_values = [1/128, 1/64, 1/32, 1/16, 1/8, 1/4]  # ε-greedy parameters\n",
        "alpha_values = [1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4]   # Gradient bandit parameters\n",
        "c_values = [1/16, 1/8, 1/4, 1/2, 1, 2, 4]             # UCB parameters\n",
        "q0_values = [1/4, 1/2, 1, 2, 4]                       # Optimistic initialization\n",
        "\n",
        "# Reduce runs for parameter study to save time\n",
        "study_runs = 1000\n",
        "study_steps = 1000\n",
        "\n",
        "# Run parameter studies\n",
        "print(\"Testing ε-greedy methods...\")\n",
        "epsilon_results = parameter_study_epsilon_greedy(epsilon_values, study_runs, study_steps)\n",
        "\n",
        "print(\"Testing UCB methods...\")\n",
        "ucb_results = parameter_study_ucb(c_values, study_runs, study_steps)\n",
        "\n",
        "print(\"Testing gradient bandit methods...\")\n",
        "gradient_results = parameter_study_gradient(alpha_values, study_runs, study_steps)\n",
        "\n",
        "print(\"Testing optimistic initialization...\")\n",
        "optimistic_results = parameter_study_optimistic(q0_values, study_runs, study_steps)\n",
        "\n",
        "pretty_print(\"Parameter Study Complete\",\n",
        "             \"All methods tested across parameter ranges<br>\" +\n",
        "             \"Ready to create Figure 2.6 parameter comparison plot\",\n",
        "             style='success')\n",
        "\n",
        "# Find best parameters\n",
        "best_epsilon = epsilon_values[np.argmax(epsilon_results)]\n",
        "best_c = c_values[np.argmax(ucb_results)]\n",
        "best_alpha = alpha_values[np.argmax(gradient_results)]\n",
        "best_q0 = q0_values[np.argmax(optimistic_results)]\n",
        "\n",
        "pretty_print(\"Optimal Parameters Found\",\n",
        "             f\"Best ε-greedy: ε = {best_epsilon:.4f}<br>\" +\n",
        "             f\"Best UCB: c = {best_c:.4f}<br>\" +\n",
        "             f\"Best gradient: α = {best_alpha:.4f}<br>\" +\n",
        "             f\"Best optimistic: Q₀ = {best_q0:.4f}\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEFMsBf7feIN"
      },
      "source": [
        "# ============================================\n",
        "# CELL 13: Create Parameter Study Visualization\n",
        "# Purpose: Generate Figure 2.6 showing method comparison across parameters\n",
        "# ============================================\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot ε-greedy results (red)\n",
        "plt.plot(epsilon_values, epsilon_results, 'o-', color=COLORS['greedy'],\n",
        "         label='ε-greedy', linewidth=2, markersize=6)\n",
        "\n",
        "# Plot gradient bandit results (green)\n",
        "plt.plot(alpha_values, gradient_results, 's-', color=COLORS['optimistic'],\n",
        "         label='gradient bandit', linewidth=2, markersize=6)\n",
        "\n",
        "# Plot UCB results (blue)\n",
        "plt.plot(c_values, ucb_results, '^-', color=COLORS['ucb'],\n",
        "         label='UCB', linewidth=2, markersize=6)\n",
        "\n",
        "# Plot optimistic initialization results (black)\n",
        "plt.plot(q0_values, optimistic_results, 'd-', color='black',\n",
        "         label='greedy with\\noptimistic\\ninitialization\\nα = 0.1', linewidth=2, markersize=6)\n",
        "\n",
        "# Formatting to match Figure 2.6\n",
        "plt.xlabel('Parameter Value', fontsize=12)\n",
        "plt.ylabel('Average\\nreward\\nover first\\n1000 steps', fontsize=11)\n",
        "plt.xscale('log', base=2)  # Log scale with base 2\n",
        "plt.xlim(1/128, 4)\n",
        "plt.ylim(1.0, 1.5)\n",
        "\n",
        "# Custom x-axis labels\n",
        "xticks = [1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4]\n",
        "xtick_labels = ['1/128', '1/64', '1/32', '1/16', '1/8', '1/4', '1/2', '1', '2', '4']\n",
        "plt.xticks(xticks, xtick_labels)\n",
        "\n",
        "# Add parameter labels\n",
        "plt.text(0.03, 1.3, 'ε', fontsize=14, color=COLORS['greedy'], weight='bold')\n",
        "plt.text(0.08, 1.35, 'α', fontsize=14, color=COLORS['optimistic'], weight='bold')\n",
        "plt.text(0.5, 1.45, 'c', fontsize=14, color=COLORS['ucb'], weight='bold')\n",
        "plt.text(1.5, 1.42, 'Q₀', fontsize=14, color='black', weight='bold')\n",
        "\n",
        "plt.legend(loc='lower left', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Title matching Figure 2.6\n",
        "plt.title('Figure 2.6: A parameter study of the various bandit algorithms presented in this chapter.\\n' +\n",
        "          'Each point is the average reward obtained over 1000 steps with a particular algorithm\\n' +\n",
        "          'at a particular setting of its parameter.',\n",
        "          fontsize=11, pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final analysis\n",
        "final_analysis = (\n",
        "    \"<strong>Parameter Study Insights:</strong><br><br>\" +\n",
        "    \"• <strong>UCB</strong>: Most robust, good performance across wide c range<br>\" +\n",
        "    \"• <strong>Gradient Bandit</strong>: High peak performance but sensitive to α<br>\" +\n",
        "    \"• <strong>Optimistic Initialization</strong>: Good performance, needs proper Q₀<br>\" +\n",
        "    \"• <strong>ε-greedy</strong>: Simple but requires careful ε tuning<br><br>\" +\n",
        "    \"<strong>Practical Recommendations:</strong><br>\" +\n",
        "    \"• Use UCB when you want robust performance without tuning<br>\" +\n",
        "    \"• Use gradient methods when you can tune parameters carefully<br>\" +\n",
        "    \"• ε-greedy remains a simple, interpretable baseline\"\n",
        ")\n",
        "\n",
        "pretty_print(\"Comprehensive Analysis\", final_analysis, style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iyx-ZlNfeIN"
      },
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Lab Summary</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>Advanced Methods Mastered:</strong></p>\n",
        "        <ul style=\"margin: 10px 0; padding-left: 20px;\">\n",
        "            <li><strong>Upper Confidence Bound (UCB):</strong> Principled uncertainty-based exploration</li>\n",
        "            <li><strong>Gradient Bandits:</strong> Policy gradient approach with preference learning</li>\n",
        "            <li><strong>Baseline Importance:</strong> Critical for variance reduction in gradient methods</li>\n",
        "            <li><strong>Parameter Sensitivity:</strong> Understanding optimal settings for each method</li>\n",
        "        </ul>\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkJiRu1afxFe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}