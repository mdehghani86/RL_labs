{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/RL_labs/blob/master/Lab_4_2_Policy_Improvement_and_Policy_Iteration_json.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JpbBlaTDfrI"
      },
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 4-2: Policy Improvement and Policy Iteration\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapter 4 | Advanced Level | 90 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Policy iteration combines policy evaluation and policy improvement to find optimal policies for MDPs.\n",
        "        This lab implements the complete policy iteration algorithm on <strong>Jack's Car Rental</strong> problem from\n",
        "        <a href=\"http://incompleteideas.net/book/the-book-2nd.html\" style=\"color: #17a2b8;\">Sutton & Barto (2018)</a>, Example 4.2.\n",
        "        This classic problem demonstrates how dynamic programming handles complex state spaces with multiple constraints\n",
        "        and stochastic dynamics modeled by <a href=\"https://en.wikipedia.org/wiki/Poisson_distribution\" style=\"color: #17a2b8;\">Poisson distributions</a>.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Implement policy improvement theorem</li>\n",
        "        <li>Understand policy iteration algorithm</li>\n",
        "        <li>Handle complex state-action spaces</li>\n",
        "        <li>Work with Poisson-distributed dynamics</li>\n",
        "        <li>Visualize policy and value function evolution</li>\n",
        "        <li>Solve Jack's Car Rental problem</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Problem Details</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">States</code> → (cars_loc1, cars_loc2) ∈ [0,20]×[0,20]</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Actions</code> → Transfer [-5, +5] cars between locations</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Rewards</code> → $10/rental - $2/transfer</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Dynamics</code> → Poisson(λ) for requests/returns</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Constraints</code> → Max 20 cars per location</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOJTR_zVDfrK"
      },
      "source": [
        "## Section 1: Environment Setup and Utilities\n",
        "\n",
        "We begin by importing necessary libraries and loading the pretty print utility for enhanced output formatting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff5ewPiCDfrK"
      },
      "source": [
        "\"\"\"\n",
        "Cell 1: Import Libraries and Load Pretty Print Utility\n",
        "Purpose: Set up computational environment with necessary libraries and pretty print utility\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import poisson\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "import time\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Fetch and execute the pretty print utility from GitHub\n",
        "try:\n",
        "    url = 'https://raw.githubusercontent.com/mdehghani86/RL_labs/master/utility/rl_utility.py'\n",
        "    response = requests.get(url)\n",
        "    exec(response.text)\n",
        "    pretty_print(\"Environment Ready\",\n",
        "                 \"Successfully loaded pretty_print utility<br>\" +\n",
        "                 \"Libraries imported: NumPy, SciPy, Matplotlib<br>\" +\n",
        "                 \"Ready for Policy Iteration implementation\",\n",
        "                 style='success')\n",
        "except Exception as e:\n",
        "    # Fallback definition if GitHub fetch fails\n",
        "    from IPython.display import display, HTML\n",
        "    def pretty_print(title, content, style='info'):\n",
        "        themes = {\n",
        "            'info': {'primary': '#17a2b8', 'secondary': '#0e5a63', 'background': '#f8f9fa'},\n",
        "            'success': {'primary': '#28a745', 'secondary': '#155724', 'background': '#f8fff9'},\n",
        "            'warning': {'primary': '#ffc107', 'secondary': '#e0a800', 'background': '#fffdf5'},\n",
        "            'result': {'primary': '#6f42c1', 'secondary': '#4e2c8e', 'background': '#faf5ff'},\n",
        "            'note': {'primary': '#20c997', 'secondary': '#0d7a5f', 'background': '#f0fdf9'}\n",
        "        }\n",
        "        theme = themes.get(style, themes['info'])\n",
        "        html = f'''\n",
        "        <div style=\"border-radius: 5px; margin: 10px 0; width: 20cm; max-width: 20cm; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
        "            <div style=\"background: linear-gradient(90deg, {theme['primary']} 0%, {theme['secondary']} 100%); padding: 10px 15px; border-radius: 5px 5px 0 0;\">\n",
        "                <strong style=\"color: white; font-size: 14px;\">{title}</strong>\n",
        "            </div>\n",
        "            <div style=\"background: {theme['background']}; padding: 10px 15px; border-radius: 0 0 5px 5px; border-left: 3px solid {theme['primary']};\">\n",
        "                <div style=\"color: rgba(0,0,0,0.8); font-size: 12px; line-height: 1.5;\">{content}</div>\n",
        "            </div>\n",
        "        </div>\n",
        "        '''\n",
        "        display(HTML(html))\n",
        "\n",
        "    pretty_print(\"Fallback Mode\",\n",
        "                 f\"Using local pretty_print definition<br>Error: {str(e)}\",\n",
        "                 style='warning')\n",
        "\n",
        "# Configure matplotlib for better visualizations\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAK3QmqADfrL"
      },
      "source": [
        "## Section 2: Jack's Car Rental Problem Setup\n",
        "\n",
        "### Problem Description\n",
        "\n",
        "Jack manages two car rental locations. Each day:\n",
        "- **Requests** arrive at each location (Poisson distributed)\n",
        "- **Returns** come back to each location (Poisson distributed)\n",
        "- Jack can **transfer** cars between locations overnight (max 5)\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "- **State**: $s = (n_1, n_2)$ where $n_i$ = number of cars at location $i$\n",
        "- **Action**: $a \\in \\{-5, -4, ..., 4, 5\\}$ (negative = move from 1 to 2)\n",
        "- **Reward**: $R = 10 \\times (rentals) - 2 \\times |transfers|$\n",
        "- **Dynamics**: Request/return rates follow Poisson distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WFUYugYDfrL"
      },
      "source": [
        "\"\"\"\n",
        "Cell 2: Define Problem Constants and Parameters\n",
        "Purpose: Set up all constants for Jack's Car Rental problem\n",
        "\"\"\"\n",
        "\n",
        "# ============================================\n",
        "# PROBLEM CONSTANTS\n",
        "# ============================================\n",
        "\n",
        "# Maximum number of cars at each location\n",
        "MAX_CARS = 20\n",
        "\n",
        "# Maximum cars that can be transferred overnight\n",
        "MAX_TRANSFER = 5\n",
        "\n",
        "# Rewards and costs\n",
        "RENTAL_REWARD = 10  # Revenue per car rental\n",
        "TRANSFER_COST = 2   # Cost per car transferred\n",
        "\n",
        "# Policy evaluation convergence threshold\n",
        "EPSILON = 1e-5\n",
        "\n",
        "# Discount factor for future rewards\n",
        "GAMMA = 0.9\n",
        "\n",
        "# Poisson distribution parameters (λ values)\n",
        "# Location 1: Requests ~ Poisson(3), Returns ~ Poisson(3)\n",
        "# Location 2: Requests ~ Poisson(4), Returns ~ Poisson(2)\n",
        "LAMBDA_REQUESTS_1 = 3\n",
        "LAMBDA_RETURNS_1 = 3\n",
        "LAMBDA_REQUESTS_2 = 4\n",
        "LAMBDA_RETURNS_2 = 2\n",
        "\n",
        "# Poisson pmf is negligible above this upper bound\n",
        "# This truncation speeds up computation\n",
        "POISSON_UPPER_BOUND = 12\n",
        "\n",
        "# Pre-compute Poisson PMFs for efficiency\n",
        "POISSON_RETURNS_2 = poisson.pmf(range(POISSON_UPPER_BOUND), LAMBDA_RETURNS_2)  # λ=2\n",
        "POISSON_REQUESTS_1 = poisson.pmf(range(POISSON_UPPER_BOUND), LAMBDA_REQUESTS_1)  # λ=3\n",
        "POISSON_RETURNS_1 = poisson.pmf(range(POISSON_UPPER_BOUND), LAMBDA_RETURNS_1)  # λ=3\n",
        "POISSON_REQUESTS_2 = poisson.pmf(range(POISSON_UPPER_BOUND), LAMBDA_REQUESTS_2)  # λ=4\n",
        "\n",
        "# Create state space: all possible (cars_at_loc1, cars_at_loc2) combinations\n",
        "STATES = np.array([[x, y] for x in range(MAX_CARS + 1)\n",
        "                          for y in range(MAX_CARS + 1)])\n",
        "\n",
        "pretty_print(\"Problem Constants Initialized\",\n",
        "             f\"State space size: {len(STATES)} states<br>\" +\n",
        "             f\"Action space: [{-MAX_TRANSFER}, {MAX_TRANSFER}]<br>\" +\n",
        "             f\"Discount factor γ = {GAMMA}<br>\" +\n",
        "             f\"Convergence threshold ε = {EPSILON}\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXQsuyD6DfrL"
      },
      "source": [
        "## Section 3: Policy Evaluation Implementation\n",
        "\n",
        "### The Bellman Equation for Policy Evaluation\n",
        "\n",
        "For a given policy $\\pi$, we compute:\n",
        "\n",
        "$$V^\\pi(s) = \\sum_{s',r} p(s',r|s,\\pi(s))[r + \\gamma V^\\pi(s')]$$\n",
        "\n",
        "Where the transition probabilities depend on Poisson-distributed requests and returns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnpKSnStDfrL"
      },
      "source": [
        "\"\"\"\n",
        "Cell 3: Policy Evaluation Function\n",
        "Purpose: Implement iterative policy evaluation for Jack's Car Rental\n",
        "\"\"\"\n",
        "\n",
        "def update_values(states, policy, values):\n",
        "    \"\"\"\n",
        "    Perform one sweep of policy evaluation across all states\n",
        "\n",
        "    This implements the Bellman expectation equation for the given policy.\n",
        "    For each state, we compute the expected value considering:\n",
        "    1. The action taken according to the policy\n",
        "    2. Stochastic rental requests (Poisson distributed)\n",
        "    3. Stochastic returns (Poisson distributed)\n",
        "    4. Rewards from rentals minus transfer costs\n",
        "\n",
        "    Args:\n",
        "        states: Array of all possible states\n",
        "        policy: Current policy (action for each state)\n",
        "        values: Current value function estimates\n",
        "\n",
        "    Returns:\n",
        "        values: Updated value function\n",
        "        delta: Maximum change in value (for convergence check)\n",
        "    \"\"\"\n",
        "    delta = 0\n",
        "\n",
        "    for state, action in zip(states, policy):\n",
        "        # Store old value for delta calculation\n",
        "        v_old = values[state[0], state[1]]\n",
        "\n",
        "        # Execute action: transfer cars between locations\n",
        "        # Positive action: transfer from loc1 to loc2\n",
        "        # Negative action: transfer from loc2 to loc1\n",
        "        state_after_transfer = state.copy()\n",
        "        state_after_transfer[0] -= action  # Location 1 loses action cars\n",
        "        state_after_transfer[1] += action  # Location 2 gains action cars\n",
        "\n",
        "        # Ensure state remains within bounds [0, 20]\n",
        "        state_after_transfer = np.clip(state_after_transfer, 0, MAX_CARS)\n",
        "\n",
        "        # Calculate expected value over all possible request/return combinations\n",
        "        # Using meshgrid for vectorized computation\n",
        "        n_returns_2, n_returns_1, n_requests_2, n_requests_1 = np.meshgrid(\n",
        "            range(POISSON_UPPER_BOUND), range(POISSON_UPPER_BOUND),\n",
        "            range(POISSON_UPPER_BOUND), range(POISSON_UPPER_BOUND)\n",
        "        )\n",
        "\n",
        "        # Compute joint probability of rental requests\n",
        "        requests_joint_prob = np.outer(POISSON_REQUESTS_1, POISSON_REQUESTS_2)\n",
        "\n",
        "        # Actual rentals = min(requests, available_cars)\n",
        "        n_rentals_1 = np.minimum(n_requests_1, state_after_transfer[0])\n",
        "        n_rentals_2 = np.minimum(n_requests_2, state_after_transfer[1])\n",
        "\n",
        "        # Calculate immediate rewards\n",
        "        rewards = (RENTAL_REWARD * (n_rentals_1 + n_rentals_2).flatten()\n",
        "                  - TRANSFER_COST * abs(action))\n",
        "\n",
        "        # Compute joint probability of returns\n",
        "        returns_joint_prob = np.outer(POISSON_RETURNS_1, POISSON_RETURNS_2)\n",
        "\n",
        "        # Calculate final state after rentals and returns\n",
        "        n_final_1 = np.minimum(\n",
        "            state_after_transfer[0] - n_rentals_1 + n_returns_1, MAX_CARS\n",
        "        ).flatten()\n",
        "        n_final_2 = np.minimum(\n",
        "            state_after_transfer[1] - n_rentals_2 + n_returns_2, MAX_CARS\n",
        "        ).flatten()\n",
        "\n",
        "        # Look up values of next states\n",
        "        v_next = values[n_final_1, n_final_2]\n",
        "\n",
        "        # Compute total joint probability\n",
        "        joint_prob = np.outer(requests_joint_prob, returns_joint_prob).flatten()\n",
        "\n",
        "        # Bellman update: expected value = Σ p(s',r|s,a)[r + γV(s')]\n",
        "        values[state[0], state[1]] = joint_prob @ (rewards + GAMMA * v_next)\n",
        "\n",
        "        # Track maximum change for convergence\n",
        "        delta = max(delta, abs(v_old - values[state[0], state[1]]))\n",
        "\n",
        "    return values, delta\n",
        "\n",
        "pretty_print(\"Policy Evaluation Ready\",\n",
        "             \"Bellman expectation equation implemented<br>\" +\n",
        "             \"Handles Poisson-distributed dynamics<br>\" +\n",
        "             \"Vectorized for computational efficiency\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XXPIU8CDfrL"
      },
      "source": [
        "## Section 4: Policy Improvement Implementation\n",
        "\n",
        "### Policy Improvement Theorem\n",
        "\n",
        "Given a value function $V^\\pi$, we can improve the policy by acting greedily:\n",
        "\n",
        "$$\\pi'(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^\\pi(s')]$$\n",
        "\n",
        "This guarantees $V^{\\pi'} \\geq V^\\pi$ for all states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl3giUmiDfrM"
      },
      "source": [
        "\"\"\"\n",
        "Cell 4: Policy Improvement Function\n",
        "Purpose: Implement greedy policy improvement based on current value function\n",
        "\"\"\"\n",
        "\n",
        "def update_policy(states, policy, values):\n",
        "    \"\"\"\n",
        "    Improve policy by acting greedily with respect to current value function\n",
        "\n",
        "    For each state, we evaluate all possible actions and select the one\n",
        "    that maximizes expected return. This implements the policy improvement\n",
        "    theorem, guaranteeing monotonic improvement.\n",
        "\n",
        "    Args:\n",
        "        states: Array of all possible states\n",
        "        policy: Current policy to be improved\n",
        "        values: Current value function\n",
        "\n",
        "    Returns:\n",
        "        policy: Improved policy\n",
        "        stable: True if policy unchanged (convergence)\n",
        "    \"\"\"\n",
        "    stable = True\n",
        "\n",
        "    for i in range(states.shape[0]):\n",
        "        state = states[i]\n",
        "        old_action = policy[i]\n",
        "\n",
        "        # Determine valid action range considering constraints:\n",
        "        # 1. Can't transfer more cars than available at source\n",
        "        # 2. Can't exceed capacity at destination\n",
        "        # 3. Maximum transfer limit of 5 cars\n",
        "\n",
        "        # Lower bound: max transfer from loc2 to loc1\n",
        "        actions_lb = max(\n",
        "            -state[1],          # Can't transfer more than available at loc2\n",
        "            state[0] - MAX_CARS,  # Can't exceed capacity at loc1\n",
        "            -MAX_TRANSFER       # Transfer limit\n",
        "        )\n",
        "\n",
        "        # Upper bound: max transfer from loc1 to loc2\n",
        "        actions_ub = min(\n",
        "            state[0],           # Can't transfer more than available at loc1\n",
        "            MAX_CARS - state[1],  # Can't exceed capacity at loc2\n",
        "            MAX_TRANSFER        # Transfer limit\n",
        "        )\n",
        "\n",
        "        # Create action space for this state\n",
        "        actions = np.arange(actions_lb, actions_ub + 1)\n",
        "        action_values = []\n",
        "\n",
        "        # Evaluate each possible action\n",
        "        for action in actions:\n",
        "            # Apply action to get state after transfer\n",
        "            state_after_transfer = state.copy()\n",
        "            state_after_transfer[0] -= action\n",
        "            state_after_transfer[1] += action\n",
        "            state_after_transfer = np.clip(state_after_transfer, 0, MAX_CARS)\n",
        "\n",
        "            # Calculate expected value for this action\n",
        "            # (Similar computation as in policy evaluation)\n",
        "            n_returns_2, n_returns_1, n_requests_2, n_requests_1 = np.meshgrid(\n",
        "                range(POISSON_UPPER_BOUND), range(POISSON_UPPER_BOUND),\n",
        "                range(POISSON_UPPER_BOUND), range(POISSON_UPPER_BOUND)\n",
        "            )\n",
        "\n",
        "            # Joint probability of requests\n",
        "            requests_joint_prob = np.outer(POISSON_REQUESTS_1, POISSON_REQUESTS_2)\n",
        "\n",
        "            # Calculate rentals (limited by available cars)\n",
        "            n_rentals_1 = np.minimum(n_requests_1, state_after_transfer[0])\n",
        "            n_rentals_2 = np.minimum(n_requests_2, state_after_transfer[1])\n",
        "\n",
        "            # Immediate rewards\n",
        "            rewards = (RENTAL_REWARD * (n_rentals_1 + n_rentals_2).flatten()\n",
        "                      - TRANSFER_COST * abs(action))\n",
        "\n",
        "            # Joint probability of returns\n",
        "            returns_joint_prob = np.outer(POISSON_RETURNS_1, POISSON_RETURNS_2)\n",
        "\n",
        "            # Final states after rentals and returns\n",
        "            n_final_1 = np.minimum(\n",
        "                state_after_transfer[0] - n_rentals_1 + n_returns_1, MAX_CARS\n",
        "            ).flatten()\n",
        "            n_final_2 = np.minimum(\n",
        "                state_after_transfer[1] - n_rentals_2 + n_returns_2, MAX_CARS\n",
        "            ).flatten()\n",
        "\n",
        "            # Look up values of next states\n",
        "            v_next = values[n_final_1, n_final_2]\n",
        "\n",
        "            # Total joint probability\n",
        "            joint_prob = np.outer(requests_joint_prob, returns_joint_prob).flatten()\n",
        "\n",
        "            # Q(s,a) = expected return for this state-action pair\n",
        "            action_value = joint_prob @ (rewards + GAMMA * v_next)\n",
        "            action_values.append(action_value)\n",
        "\n",
        "        # Select action with maximum expected value (greedy)\n",
        "        policy[i] = actions[np.argmax(action_values)]\n",
        "\n",
        "        # Check if policy changed\n",
        "        if stable and policy[i] != old_action:\n",
        "            stable = False\n",
        "\n",
        "    return policy, stable\n",
        "\n",
        "pretty_print(\"Policy Improvement Ready\",\n",
        "             \"Greedy policy improvement implemented<br>\" +\n",
        "             \"Evaluates all valid actions per state<br>\" +\n",
        "             \"Selects action maximizing expected return\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64KKgGzxDfrM"
      },
      "source": [
        "## Section 5: Policy Iteration Algorithm\n",
        "\n",
        "### The Complete Algorithm\n",
        "\n",
        "Policy Iteration alternates between:\n",
        "1. **Policy Evaluation**: Compute $V^\\pi$ for current policy\n",
        "2. **Policy Improvement**: Update $\\pi$ to be greedy w.r.t. $V^\\pi$\n",
        "\n",
        "This process converges to the optimal policy $\\pi^*$ and optimal value function $V^*$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfUbjVh1DfrM"
      },
      "source": [
        "\"\"\"\n",
        "Cell 5: Main Policy Iteration Implementation\n",
        "Purpose: Combine policy evaluation and improvement to find optimal policy\n",
        "\"\"\"\n",
        "\n",
        "def policy_iteration():\n",
        "    \"\"\"\n",
        "    Execute complete policy iteration algorithm for Jack's Car Rental\n",
        "\n",
        "    Alternates between:\n",
        "    1. Policy Evaluation: Compute V^π for current policy\n",
        "    2. Policy Improvement: Make π greedy w.r.t. V^π\n",
        "\n",
        "    Continues until policy is stable (no further improvements possible)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize value function to zero for all states\n",
        "    values = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
        "\n",
        "    # Initialize policy: do nothing (transfer 0 cars) for all states\n",
        "    policy = np.zeros(STATES.shape[0], dtype=int)\n",
        "\n",
        "    pretty_print(\"Starting Policy Iteration\",\n",
        "                 f\"Initial policy: No transfers<br>\" +\n",
        "                 f\"State space: {MAX_CARS + 1} × {MAX_CARS + 1} = {len(STATES)} states<br>\" +\n",
        "                 f\"Action space: {2 * MAX_TRANSFER + 1} possible actions per state\",\n",
        "                 style='info')\n",
        "\n",
        "    # Plot initial policy\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(policy.reshape(MAX_CARS + 1, MAX_CARS + 1), cmap='RdBu_r',\n",
        "               vmin=-MAX_TRANSFER, vmax=MAX_TRANSFER, origin='lower')\n",
        "    plt.colorbar(label='Cars transferred (+ from 1 to 2, - from 2 to 1)')\n",
        "    plt.xlabel('Cars at Location 1')\n",
        "    plt.ylabel('Cars at Location 2')\n",
        "    plt.title('Initial Policy (Iteration 0)')\n",
        "    plt.show()\n",
        "\n",
        "    # Main policy iteration loop\n",
        "    stable = False\n",
        "    iteration = 0\n",
        "\n",
        "    while not stable:\n",
        "        iteration += 1\n",
        "        start_time = time.time()\n",
        "\n",
        "        pretty_print(f\"Iteration {iteration}\",\n",
        "                    \"Starting policy evaluation...\",\n",
        "                    style='info')\n",
        "\n",
        "        # POLICY EVALUATION\n",
        "        # Iterate until value function converges\n",
        "        eval_iterations = 0\n",
        "        while True:\n",
        "            values, delta = update_values(STATES, policy, values)\n",
        "            eval_iterations += 1\n",
        "\n",
        "            if eval_iterations % 10 == 0:\n",
        "                print(f\"  Evaluation iteration {eval_iterations}: δ = {delta:.6f}\")\n",
        "\n",
        "            # Check convergence\n",
        "            if delta < EPSILON:\n",
        "                pretty_print(\"Policy Evaluation Complete\",\n",
        "                           f\"Converged after {eval_iterations} iterations<br>\" +\n",
        "                           f\"Final δ = {delta:.8f}\",\n",
        "                           style='success')\n",
        "                break\n",
        "\n",
        "        # POLICY IMPROVEMENT\n",
        "        pretty_print(f\"Iteration {iteration}\",\n",
        "                    \"Starting policy improvement...\",\n",
        "                    style='info')\n",
        "\n",
        "        policy, stable = update_policy(STATES, policy, values)\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        if stable:\n",
        "            pretty_print(\"Policy Iteration Complete!\",\n",
        "                       f\"Optimal policy found after {iteration} iterations<br>\" +\n",
        "                       f\"Last iteration time: {elapsed_time:.2f} seconds\",\n",
        "                       style='result')\n",
        "        else:\n",
        "            pretty_print(f\"Iteration {iteration} Complete\",\n",
        "                       f\"Policy improved<br>\" +\n",
        "                       f\"Time: {elapsed_time:.2f} seconds\",\n",
        "                       style='success')\n",
        "\n",
        "        # Visualize current policy\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        policy_grid = policy.reshape(MAX_CARS + 1, MAX_CARS + 1)\n",
        "        plt.imshow(policy_grid, cmap='RdBu_r',\n",
        "                  vmin=-MAX_TRANSFER, vmax=MAX_TRANSFER, origin='lower')\n",
        "        plt.colorbar(label='Cars transferred (+ from 1 to 2, - from 2 to 1)')\n",
        "        plt.xlabel('Cars at Location 1')\n",
        "        plt.ylabel('Cars at Location 2')\n",
        "        plt.title(f'Policy at Iteration {iteration}')\n",
        "\n",
        "        # Add contour lines for better visualization\n",
        "        X, Y = np.meshgrid(range(MAX_CARS + 1), range(MAX_CARS + 1))\n",
        "        plt.contour(X, Y, policy_grid, levels=range(-5, 6),\n",
        "                   colors='black', alpha=0.4, linewidths=0.5)\n",
        "        plt.show()\n",
        "\n",
        "    return policy, values\n",
        "\n",
        "pretty_print(\"Policy Iteration Function Ready\",\n",
        "             \"Complete algorithm implemented<br>\" +\n",
        "             \"Will iterate until optimal policy found\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpft47LoDfrM"
      },
      "source": [
        "## Section 6: Execute Policy Iteration and Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9iURIwNDfrM"
      },
      "source": [
        "\"\"\"\n",
        "Cell 6: Run Policy Iteration and Generate Final Visualizations\n",
        "Purpose: Execute the complete algorithm and visualize optimal policy and value function\n",
        "\"\"\"\n",
        "\n",
        "# Run policy iteration to find optimal policy\n",
        "pretty_print(\"Executing Policy Iteration\",\n",
        "             \"This will take several minutes to converge...<br>\" +\n",
        "             \"Watch as the policy evolves toward optimality!\",\n",
        "             style='warning')\n",
        "\n",
        "optimal_policy, optimal_values = policy_iteration()\n",
        "\n",
        "# Create comprehensive visualization of results\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Subplot 1: Final optimal policy\n",
        "ax1 = plt.subplot(1, 3, 1)\n",
        "policy_grid = optimal_policy.reshape(MAX_CARS + 1, MAX_CARS + 1)\n",
        "im1 = ax1.imshow(policy_grid, cmap='RdBu_r',\n",
        "                 vmin=-MAX_TRANSFER, vmax=MAX_TRANSFER, origin='lower')\n",
        "ax1.set_xlabel('Cars at Location 1')\n",
        "ax1.set_ylabel('Cars at Location 2')\n",
        "ax1.set_title('Optimal Policy π*')\n",
        "plt.colorbar(im1, ax=ax1, label='Transfer')\n",
        "\n",
        "# Add contour lines\n",
        "X, Y = np.meshgrid(range(MAX_CARS + 1), range(MAX_CARS + 1))\n",
        "ax1.contour(X, Y, policy_grid, levels=range(-5, 6),\n",
        "           colors='black', alpha=0.4, linewidths=0.5)\n",
        "\n",
        "# Subplot 2: Value function heatmap\n",
        "ax2 = plt.subplot(1, 3, 2)\n",
        "im2 = ax2.imshow(optimal_values, cmap='viridis', origin='lower')\n",
        "ax2.set_xlabel('Cars at Location 1')\n",
        "ax2.set_ylabel('Cars at Location 2')\n",
        "ax2.set_title('Value Function V*')\n",
        "plt.colorbar(im2, ax=ax2, label='Expected Return')\n",
        "\n",
        "# Subplot 3: 3D surface plot of value function\n",
        "ax3 = plt.subplot(1, 3, 3, projection='3d')\n",
        "ax3.plot_surface(X, Y, optimal_values, cmap='viridis',\n",
        "                 edgecolor='none', alpha=0.8)\n",
        "ax3.set_xlabel('Cars at Location 1')\n",
        "ax3.set_ylabel('Cars at Location 2')\n",
        "ax3.set_zlabel('Expected Return')\n",
        "ax3.set_title('Value Function V* (3D)')\n",
        "ax3.view_init(elev=30, azim=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze optimal policy characteristics\n",
        "positive_transfers = np.sum(optimal_policy > 0)\n",
        "negative_transfers = np.sum(optimal_policy < 0)\n",
        "no_transfers = np.sum(optimal_policy == 0)\n",
        "max_value = np.max(optimal_values)\n",
        "min_value = np.min(optimal_values)\n",
        "\n",
        "analysis_text = f\"\"\"\n",
        "<strong>Optimal Policy Analysis:</strong><br><br>\n",
        "• States with transfers from Location 1 to 2: {positive_transfers} ({100*positive_transfers/len(STATES):.1f}%)<br>\n",
        "• States with transfers from Location 2 to 1: {negative_transfers} ({100*negative_transfers/len(STATES):.1f}%)<br>\n",
        "• States with no transfer: {no_transfers} ({100*no_transfers/len(STATES):.1f}%)<br><br>\n",
        "<strong>Value Function Statistics:</strong><br>\n",
        "• Maximum expected return: ${max_value:.2f}<br>\n",
        "• Minimum expected return: ${min_value:.2f}<br>\n",
        "• Average expected return: ${np.mean(optimal_values):.2f}<br><br>\n",
        "<strong>Key Insights:</strong><br>\n",
        "• The policy tends to balance cars between locations<br>\n",
        "• Location 2 has higher demand (λ=4) but lower returns (λ=2)<br>\n",
        "• Optimal policy compensates by transferring cars to Location 2<br>\n",
        "• Transfer costs create a threshold effect in the policy\n",
        "\"\"\"\n",
        "\n",
        "pretty_print(\"Results Analysis\", analysis_text, style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqhiM_P1DfrN"
      },
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Key Findings</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>1. Policy Structure:</strong> The optimal policy shows a clear diagonal pattern, transferring cars from Location 1 to 2 when Location 1 has excess inventory and Location 2 is low.</p>\n",
        "        <p><strong>2. Asymmetric Dynamics:</strong> The different Poisson parameters at each location create an asymmetric optimal policy that favors transfers to Location 2.</p>\n",
        "        <p><strong>3. Transfer Threshold:</strong> Due to transfer costs ($2 per car), small imbalances are not corrected - there's a threshold effect.</p>\n",
        "        <p><strong>4. Convergence:</strong> Policy iteration converges in relatively few iterations (typically 4-6) despite the large state space (441 states).</p>\n",
        "        <p><strong>5. Value Function:</strong> The value function is smooth and increases toward balanced inventory states, reflecting higher earning potential.</p>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>How would the optimal policy change if transfer costs increased to $5 per car?</li>\n",
        "        <li>What if we added a capacity constraint on overnight transfers (e.g., only one truck available)?</li>\n",
        "        <li>How would non-linear transfer costs (e.g., fixed cost + per-car cost) affect the policy?</li>\n",
        "        <li>Could we speed up convergence using value iteration instead of policy iteration?</li>\n",
        "        <li>How would the solution change with different Poisson parameters?</li>\n",
        "        <li>What real-world factors are we ignoring that might affect Jack's optimal strategy?</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 4-2/3: Policy Improvement and Policy Iteration</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Next: Lab 5 - Monte Carlo Methods</p>\n",
        "</div>"
      ]
    }
  ]
}