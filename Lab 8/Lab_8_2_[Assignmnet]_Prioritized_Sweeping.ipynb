{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8-2: Dyna-Q and Prioritized Sweeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this Lab! In this notebook, you will:\n",
    "1. implement the Dyna-Q with and without Prioritized Sweeping!\n",
    "2. compare their performance on different scenarios\n",
    "\n",
    "We will give you the environment and infrastructure to run the experiment and visualize the performance. These libraries need to be imported for the repository.\n",
    "\n",
    "The assignment will be graded automatically by comparing the behavior of your agent to our implementations of the algorithms. The random seed will be set explicitly to avoid different behaviors due to randomness. \n",
    "\n",
    "Please go through the cells in order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone to Repository\n",
    "All of the required files for this lab are placed in a repository. So, you need to clone the notebook to this repository in order to add th provided libraries and files to the Contents of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the notebook to the other required libraries for this lab\n",
    "!git clone https://github.com/mdehghani86/MazeExampleRep.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Shortcut Maze Environment\n",
    "\n",
    "In this maze environment, the goal is to reach the goal state (G) as fast as possible from the starting state (S). There are four actions – up, down, right, left – which take the agent deterministically from a state to the corresponding neighboring states, except when movement is blocked by a wall (denoted by grey) or the edge of the maze, in which case the agent remains where it is. The reward is +1 on reaching the goal state, 0 otherwise. On reaching the goal state G, the agent returns to the start state S to being a new episode. This is a discounted, episodic task with $\\gamma = 0.95$.\n",
    "\n",
    "<img src=\"MazeExampleRep/images/shortcut_env.png\" alt=\"environment\" width=\"400\"/>\n",
    "\n",
    "Later in the assignment, we will use a variant of this maze in which a 'shortcut' opens up after a certain number of timesteps. We will test if the the Dyna-Q and Dyna-Q+ agents are able to find the newly-opened shorter route to the goal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "We import the following libraries that are required for this assignment. Primarily, we shall be using the following libraries:\n",
    "1. numpy: the fundamental package for scientific computing with Python.\n",
    "2. matplotlib: the library for plotting graphs in Python.\n",
    "3. RL-Glue: the library for reinforcement learning experiments.\n",
    "4. queue: to maitaine \"prioritized queue\". For its documentaiton visit [here](https://python.readthedocs.io/en/latest/library/asyncio-queue.html#)\n",
    "\n",
    "**Please do not import other libraries** — this will break the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install jdc\n",
    "import os, jdc, shutil\n",
    "from tqdm import tqdm\n",
    "from queue import PriorityQueue\n",
    "\n",
    "!pip install rlglue\n",
    "from MazeExampleRep.rl_glue import RLGlue\n",
    "from MazeExampleRep.agent import BaseAgent\n",
    "from MazeExampleRep.maze_env import ShortcutMazeEnvironment\n",
    "\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.rcParams.update({'figure.figsize': [8,5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Dyna-Q with \"Prioritized Sweeping\"\n",
    "\n",
    "Let's start with a quick recap of the Prioritized sweeping Pseudocode.\n",
    "\n",
    "<div style=\"width:80%\"><img src=\"MazeExampleRep/images/prioritized_sweeping.png\" alt=\"DynaQ_pseudocode\"></div>\n",
    "\n",
    "Priority Sweeping involves five basic steps:\n",
    "1. State/Action Selection: select a nonterminal state and an action to be performed.\n",
    "2. Model learning: using the observed next state and reward, update the model (here, updating a table as the environment is assumed to be deterministic).\n",
    "3. Prioritizing: A queue is maintained of every state–action pair whose estimated value would change nontrivially if updated , prioritized by the size of the change. If the effects of changes are efficiently propagated backward until quiescence\n",
    "4. Planning/Sweeing: update the action values by generating $n$ simulated experiences selected form top of the queue and perform one-step tabular Q-planning method. \n",
    "5. Update Queue: the effects of changes are efficiently propagated backward until quiescence\n",
    "\n",
    "Please check [8.4 Prioritized Sweeping](http://www.incompleteideas.net/book/RLbook2018.pdf#page=191) for more detials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this down in pieces and do it one-by-one.\n",
    "\n",
    "PriorityAgent has the same structure as the DynaQAgent with three major differences. \n",
    "In the init function, we initialised a threshold $\\theta$ , a priority queue to store state and action pairs by their priority, and a dictionary of predecessors in order to update all states lead to the current updated state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "#PriorityAgent with \"Prioritized Sweeping\"\n",
    "\n",
    "class PriorityAgent(BaseAgent):\n",
    "\n",
    "    def agent_init(self, agent_info):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Args:\n",
    "            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n",
    "            {\n",
    "                num_states (int): The number of states,\n",
    "                num_actions (int): The number of actions,\n",
    "                epsilon (float): The parameter for epsilon-greedy exploration,\n",
    "                step_size (float): The step-size,\n",
    "                discount (float): The discount factor,\n",
    "                planning_steps (int): The number of planning steps per environmental interaction\n",
    "                step_size (delta): small threshold used for the Prioritized queue,\n",
    "                random_seed (int): the seed for the RNG used in epsilon-greedy\n",
    "                planning_random_seed (int): the seed for the RNG used in the planner\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "        # First, we get the relevant information from agent_info \n",
    "        # NOTE: we use np.random.RandomState(seed) to set the two different RNGs\n",
    "        # for the planner and the rest of the code\n",
    "        try:\n",
    "            self.num_states = agent_info[\"num_states\"]\n",
    "            self.num_actions = agent_info[\"num_actions\"]\n",
    "        except:\n",
    "            print(\"You need to pass both 'num_states' and 'num_actions' \\\n",
    "                   in agent_info to initialize the action-value table\")\n",
    "        self.gamma = agent_info.get(\"discount\", 0.95)\n",
    "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        self.planning_steps = agent_info.get(\"planning_steps\", 10)\n",
    "\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get('random_seed', 50))\n",
    "        self.planning_rand_generator = np.random.RandomState(agent_info.get('planning_random_seed', 50))\n",
    "\n",
    "        # Next, we initialize the attributes required by the agent, e.g., q_values, model, etc.\n",
    "        # A simple way to implement the model is to have a dictionary of dictionaries, \n",
    "        #        mapping each state to a dictionary which maps actions to (reward, next state) tuples.\n",
    "        self.q_values = np.zeros((self.num_states, self.num_actions))\n",
    "        self.actions = list(range(self.num_actions))\n",
    "        self.past_action = -1\n",
    "        self.past_state = -1\n",
    "        self.model = {} # model is a dictionary of dictionaries, which maps states to actions to \n",
    "                        # (reward, next_state) tuples\n",
    "            \n",
    "            \n",
    "        # for priority sweeping\n",
    "        self.theta = agent_info.get(\"theta\", 0.05)\n",
    "        self.queue = PriorityQueue()\n",
    "        self.predecessors = {}  # s' -> list[(s, a)...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating model is exactly the same for both PriorityAgent, and DynaQAgent.\n",
    "It takes a (s, a, s', r) tuple and stores the next state and reward corresponding to a state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to PriorityAgent\n",
    "\n",
    "# [GRADED]\n",
    "\n",
    "def update_model(self, past_state, past_action, state, reward):\n",
    "    \"\"\"updates the model \n",
    "    \n",
    "    Args:\n",
    "        past_state       (int): s\n",
    "        past_action      (int): a\n",
    "        state            (int): s'\n",
    "        reward           (int): r\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    # Update the model with the (s,a,s',r) tuple (1~4 lines)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "\n",
    "## Test code for update_model() ##\n",
    "\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 3, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 1.0, \n",
    "              \"random_seed\": 0,\n",
    "              \"planning_random_seed\": 0}\n",
    "test_agent = PriorityAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "test_agent.update_model(0,2,0,1)\n",
    "test_agent.update_model(2,0,1,1)\n",
    "test_agent.update_model(0,3,1,2)\n",
    "print(\"Model: \\n\", test_agent.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to PriorityAgent\n",
    "\n",
    "# [GRADED]\n",
    "# update predecessors 1~4 lines\n",
    "\n",
    "def update_predecessors(self, past_state, past_action, state):\n",
    "    \"\"\"updates the predecessors\n",
    "    \n",
    "    Args:\n",
    "        past_state       (int): s\n",
    "        past_action      (int): a\n",
    "        state            (int): s'\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "\n",
    "## Test code for update_predecessors() ##\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 4, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 0.9, \n",
    "              \"planning_steps\": 5,\n",
    "              \"random_seed\": 0,\n",
    "              \"planning_random_seed\": 5,\n",
    "              \"theta\" : 0.05}\n",
    "\n",
    "test_agent = PriorityAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "\n",
    "test_agent.update_predecessors(2,1,0)\n",
    "test_agent.update_predecessors(2,2,2)\n",
    "test_agent.update_predecessors(2,3,2)\n",
    "test_agent.update_predecessors(1,1,2)\n",
    "test_agent.update_predecessors(1,4,3)\n",
    "\n",
    "# Print the queue items one by one (based on priority)\n",
    "print(test_agent.predecessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to PriorityAgent\n",
    "\n",
    "# [GRADED]\n",
    "\n",
    "def update_queue(self, past_state, past_action, state, reward):\n",
    "    \"\"\"updates the model \n",
    "    \n",
    "    Args:\n",
    "        past_state       (int): s\n",
    "        past_action      (int): a\n",
    "        state            (int): s'\n",
    "        reward           (int): r\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "       \n",
    "    # Step 1: Find the absolute value of the difference between the current value of s in self.values and the highest\n",
    "        # Q-value across all possible actions from s (this represents what the value should be); \n",
    "        # call this number DynaQ_psDelta. Do NOT update self.values[s] in this step.\n",
    "    # Step 2: Update self.Pqueue : Push s and a into the priority queue with priority -diff (note that this is negative).\n",
    "        # We use a negative because the priority queue is a min heap, but we want to prioritize updating states that have\n",
    "        # a higher error.\n",
    "    \n",
    "    ### START CODE HERE ### 2-4 lines\n",
    "    \n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "\n",
    "## Test code for update_queue() ##\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 4, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 0.9, \n",
    "              \"planning_steps\": 5,\n",
    "              \"random_seed\": 0,\n",
    "              \"planning_random_seed\": 5,\n",
    "              \"theta\" : 0.05}\n",
    "test_agent = PriorityAgent()\n",
    "\n",
    "test_agent.agent_init(agent_info)\n",
    "test_agent.update_queue(0,2,1,1)\n",
    "test_agent.update_queue(0,1,3,0)\n",
    "test_agent.update_queue(2,2,3,2)\n",
    "test_agent.update_queue(3,1,2,3)\n",
    "\n",
    "# Print the queue items one by one (based on priority)\n",
    "# Do not this part, loop for is not needed\n",
    "print(test_agent.queue.get()[1])\n",
    "print(test_agent.queue.get()[1])\n",
    "print(test_agent.queue.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to PriorityAgent\n",
    "\n",
    "# [GRADED]\n",
    "\n",
    "def planning_step(self):\n",
    "    \"\"\"performs planning, i.e. indirect RL.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    \n",
    "    # The indirect RL step:\n",
    "    # - Choose a state and action from the queue (~2 lines)\n",
    "    # - Query the model with this state-action pair for the predicted next state and reward.(~1 line)\n",
    "    # - Update the action values with this simulated experience.                            (2~4 lines)\n",
    "    # - Repeat until the queue is empty.\n",
    "    #\n",
    "    # Note that the update equation is different for terminal and non-terminal transitions. \n",
    "    # To differentiate between a terminal and a non-terminal next state, assume that the model stores\n",
    "    # the terminal state as a dummy state like -1\n",
    "    #\n",
    "    # Important: remember you have a random number generator 'planning_rand_generator' as \n",
    "    #     a part of the class which you need to use as self.planning_rand_generator.choice()\n",
    "    #     For the sake of reproducibility and grading, *do not* use anything else like \n",
    "    #     np.random.choice() for performing search control.\n",
    "\n",
    "    ### START CODE HERE ###    \n",
    "    # Planning loop\n",
    "   \n",
    "        #Part 1: state-action selection from top of the queue\n",
    "      \n",
    "        #Part 2: Query the model (s,a)-> (s',r)\n",
    "       \n",
    "        #Part 3: Plannig update\n",
    "           \n",
    "        \n",
    "        #Part 4: loop for all state, action predicted lead to _state\n",
    "       \n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "\n",
    "## Test code for planning_step() ##\n",
    "actions = []\n",
    "agent_info = {\"num_actions\": 4, \n",
    "              \"num_states\": 6, \n",
    "              \"epsilon\": 0.1, \n",
    "              \"step_size\": 0.1, \n",
    "              \"discount\": 0.9, \n",
    "              \"planning_steps\": 5,\n",
    "              \"random_seed\": 0,\n",
    "              \"planning_random_seed\": 5,\n",
    "              \"theta\" : 0.05}\n",
    "test_agent = PriorityAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "\n",
    "test_agent.update_model(0,2,1,1)\n",
    "test_agent.update_queue(0,2,1,1)\n",
    "test_agent.update_predecessors(0,2,1)\n",
    "\n",
    "test_agent.update_model(0,1,3,0)\n",
    "test_agent.update_queue(0,1,3,0)\n",
    "test_agent.update_predecessors(0,1,3)\n",
    "\n",
    "test_agent.update_model(2,2,5,2)\n",
    "test_agent.update_queue(2,2,5,2)\n",
    "test_agent.update_predecessors(2,2,5)\n",
    "\n",
    "test_agent.update_model(3,1,2,1)\n",
    "test_agent.update_queue(3,1,2,1)\n",
    "test_agent.update_predecessors(3,1,2)\n",
    "\n",
    "test_agent.planning_step()\n",
    "print(\"q_values\", test_agent.q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to PriorityAgent\n",
    "\n",
    "# Do not modify this cell!\n",
    "\n",
    "def argmax(self, q_values):\n",
    "    \"\"\"argmax with random tie-breaking\n",
    "    Args:\n",
    "        q_values (Numpy array): the array of action values\n",
    "    Returns:\n",
    "        action (int): an action with the highest value\n",
    "    \"\"\"\n",
    "    top = float(\"-inf\")\n",
    "    ties = []\n",
    "\n",
    "    for i in range(len(q_values)):\n",
    "        if q_values[i] > top:\n",
    "            top = q_values[i]\n",
    "            ties = []\n",
    "\n",
    "        if q_values[i] == top:\n",
    "            ties.append(i)\n",
    "\n",
    "    return self.rand_generator.choice(ties)\n",
    "\n",
    "def choose_action_egreedy(self, state):\n",
    "    \"\"\"returns an action using an epsilon-greedy policy w.r.t. the current action-value function.\n",
    "\n",
    "    Important: assume you have a random number generator 'rand_generator' as a part of the class\n",
    "                which you can use as self.rand_generator.choice() or self.rand_generator.rand()\n",
    "\n",
    "    Args:\n",
    "        state (List): coordinates of the agent (two elements)\n",
    "    Returns:\n",
    "        The action taken w.r.t. the aforementioned epsilon-greedy policy\n",
    "    \"\"\"\n",
    "\n",
    "    if self.rand_generator.rand() < self.epsilon:\n",
    "        action = self.rand_generator.choice(self.actions)\n",
    "    else:\n",
    "        values = self.q_values[state]\n",
    "        action = self.argmax(values)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to PriorityAgent\n",
    "\n",
    "# [GRADED]\n",
    "\n",
    "def agent_start(self, state):\n",
    "    \"\"\"The first method called when the experiment starts, \n",
    "    called after the environment starts.\n",
    "    Args:\n",
    "        state (Numpy array): the state from the\n",
    "            environment's env_start function.\n",
    "    Returns:\n",
    "        (int) the first action the agent takes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # given the state, select the action using self.choose_action_egreedy()), \n",
    "    # and save current state and action (~2 lines)\n",
    "    ### self.past_state = ?\n",
    "    ### self.past_action = ?\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "   \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return self.past_action\n",
    "\n",
    "def agent_step(self, reward, state):\n",
    "    \"\"\"A step taken by the agent.\n",
    "\n",
    "    Args:\n",
    "        reward (float): the reward received for taking the last action taken\n",
    "        state (Numpy array): the state from the\n",
    "            environment's step based on where the agent ended up after the\n",
    "            last step\n",
    "    Returns:\n",
    "        (int) The action the agent takes given this state.\n",
    "    \"\"\"\n",
    "    \n",
    "    # - Direct-RL step (~1-3 lines)\n",
    "    # - Model Update step (~1 line)\n",
    "    # - planning_step` (~1 line)\n",
    "    # - Action Selection step (~1 line)\n",
    "    # Save the current state and action before returning the action to be performed. (~2 lines)\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    #Part 1: last state - last action\n",
    "   \n",
    "    #Part 2:  Update Model  \n",
    "    \n",
    "    # Part 3: Update Queue\n",
    "\n",
    "    # Part 4: Update predecessors\n",
    "    \n",
    "    #Part 5: Planning_step\n",
    "\n",
    "    #Part 6: Action Selection step (~1 line)\n",
    "\n",
    "    #Part 5: Save the current state and action\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return self.past_action\n",
    "\n",
    "def agent_end(self, reward):\n",
    "    \"\"\"Called when the agent terminates.\n",
    "\n",
    "    Args:\n",
    "        reward (float): the reward the agent received for entering the\n",
    "            terminal state.\n",
    "    \"\"\"\n",
    "    \n",
    "    # - Direct RL update with this final transition (1~2 lines)\n",
    "    # - Model Update step with this final transition (~1 line)\n",
    "    # - One final `planning_step` (~1 line)\n",
    "    #\n",
    "    # Note: the final transition needs to be handled carefully. Since there is no next state, \n",
    "    #       you will have to pass a dummy state (like -1), which you will be using in the planning_step() to \n",
    "    #       differentiate between updates with usual terminal and non-terminal transitions.\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    #Part 1: last state - last action\n",
    "\n",
    "    \n",
    "    #Part 2:  Update Model  \n",
    "    \n",
    "    #Part 3: Update Queue\n",
    "    \n",
    "    # Part 4: Update predecessors\n",
    "\n",
    "    #Part 5: Planning_step\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "\n",
    "def run_experiment(env, agent, env_parameters, agent_parameters, exp_parameters):\n",
    "\n",
    "    # Experiment settings\n",
    "    num_runs = exp_parameters['num_runs']\n",
    "    num_episodes = exp_parameters['num_episodes']\n",
    "    planning_steps_all = agent_parameters['planning_steps']\n",
    "\n",
    "    env_info = env_parameters                     \n",
    "    agent_info = {\"num_states\" : agent_parameters[\"num_states\"],  # We pass the agent the information it needs. \n",
    "                  \"num_actions\" : agent_parameters[\"num_actions\"],\n",
    "                  \"epsilon\": agent_parameters[\"epsilon\"],\n",
    "                  \"theta\": agent_parameters[\"theta\"],\n",
    "                  \"discount\": env_parameters[\"discount\"],\n",
    "                  \"step_size\" : agent_parameters[\"step_size\"]}\n",
    "\n",
    "    all_averages = np.zeros((len(planning_steps_all), num_runs, num_episodes)) # for collecting metrics \n",
    "    log_data = {'planning_steps_all' : planning_steps_all}                     # that shall be plotted later\n",
    "\n",
    "    for idx, planning_steps in enumerate(planning_steps_all):\n",
    "\n",
    "        print('Planning steps : ', planning_steps)\n",
    "        os.system('sleep 0.5')                    # to prevent tqdm printing out-of-order before the above print()\n",
    "        agent_info[\"planning_steps\"] = planning_steps  \n",
    "\n",
    "        for i in tqdm(range(num_runs)):\n",
    "\n",
    "            agent_info['random_seed'] = i\n",
    "            agent_info['planning_random_seed'] = i\n",
    "\n",
    "            rl_glue = RLGlue(env, agent)          # Creates a new RLGlue experiment with the env and agent we chose above\n",
    "            rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n",
    "\n",
    "            for j in range(num_episodes):\n",
    "\n",
    "                rl_glue.rl_start()                # We start an episode. Here we aren't using rl_glue.rl_episode()\n",
    "                                                  # like the other assessments because we'll be requiring some \n",
    "                is_terminal = False               # data from within the episodes in some of the experiments here \n",
    "                num_steps = 0\n",
    "                while not is_terminal:\n",
    "                    reward, _, action, is_terminal = rl_glue.rl_step()  # The environment and agent take a step \n",
    "                    num_steps += 1                                      # and return the reward and action taken.\n",
    "\n",
    "                all_averages[idx][i][j] = num_steps\n",
    "\n",
    "    log_data['all_averages'] = all_averages\n",
    "    np.save(\"results/Priority-Sweeping_steps\", log_data)\n",
    "    \n",
    "\n",
    "def plot_steps_per_episode(file_path):\n",
    "\n",
    "    data = np.load(file_path).item()\n",
    "    all_averages = data['all_averages']\n",
    "    planning_steps_all = data['planning_steps_all']\n",
    "\n",
    "    for i, planning_steps in enumerate(planning_steps_all):\n",
    "        plt.plot(np.mean(all_averages[i], axis=0), label='Planning steps = '+str(planning_steps))\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Steps\\nper\\nepisode', rotation=0, labelpad=40)\n",
    "    plt.axhline(y=16, linestyle='--', color='grey', alpha=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do NOT modify the parameter settings.\n",
    "\n",
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 20,                     # The number of times we run the experiment\n",
    "    \"num_episodes\" : 30,                 # The number of episodes per experiment\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = { \n",
    "    \"discount\": 0.95,\n",
    "}\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {  \n",
    "    \"num_states\" : 54,\n",
    "    \"num_actions\" : 4, \n",
    "    \"epsilon\": 0.1, \n",
    "    \"step_size\" : 0.125,\n",
    "    \"theta\" : 0.2,\n",
    "    \"planning_steps\" : [25]      # The list of planning_steps we want to try\n",
    "}\n",
    "\n",
    "current_env = ShortcutMazeEnvironment   # The environment\n",
    "current_agent = PriorityAgent              # The agent\n",
    "\n",
    "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n",
    "plot_steps_per_episode('results/Priority-Sweeping_steps.npy')   \n",
    "shutil.make_archive('results', 'zip', 'results');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's begin coding.\n",
    "\n",
    "You already know to to code Dyna-Q, you can easily execute cells of this section and jump into section 2, Dyna-Q with \"Prioritized Sweeping\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
