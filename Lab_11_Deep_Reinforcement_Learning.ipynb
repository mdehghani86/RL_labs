{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Lab_11_Deep_Reinforcement_Learning.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 11: Deep Reinforcement Learning - From DQN to Actor-Critic\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapters 9, 13 | Advanced Level | 120 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        This lab introduces <strong>deep reinforcement learning</strong>, where neural networks approximate value functions and policies.\n",
        "        We begin with <a href=\"https://www.nature.com/articles/nature14236\" style=\"color: #17a2b8;\">Deep Q-Networks (DQN)</a> by \n",
        "        <a href=\"https://deepmind.google/discover/blog/deep-reinforcement-learning/\" style=\"color: #17a2b8;\">Mnih et al. (2015)</a>,\n",
        "        which extended Q-learning to high-dimensional state spaces. We then explore policy gradient methods from\n",
        "        <a href=\"http://incompleteideas.net/book/the-book-2nd.html\" style=\"color: #17a2b8;\">Sutton & Barto (2018)</a> Chapter 13,\n",
        "        culminating in Actor-Critic architectures that power modern algorithms like\n",
        "        <a href=\"https://arxiv.org/abs/1707.06347\" style=\"color: #17a2b8;\">PPO</a> and\n",
        "        <a href=\"https://arxiv.org/abs/1801.01290\" style=\"color: #17a2b8;\">SAC</a>.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Understand function approximation with neural networks</li>\n",
        "        <li>Implement Deep Q-Networks with experience replay</li>\n",
        "        <li>Recognize and fix the overestimation bias with Double DQN</li>\n",
        "        <li>Master policy gradient methods and REINFORCE algorithm</li>\n",
        "        <li>Build Actor-Critic architectures for stable learning</li>\n",
        "        <li>Compare value-based vs policy-based approaches</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Algorithms Covered</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">DQN</code> → Q-learning + Neural Networks</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">DDQN</code> → Fixes Q-value overestimation</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">REINFORCE</code> → Monte Carlo policy gradient</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Actor-Critic</code> → Policy + Value function</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 1: Environment Setup and CartPole Introduction</h2>\n",
        "</div>\n",
        "\n",
        "### The CartPole Environment\n",
        "\n",
        "CartPole-v1 is a classic control problem where the goal is to balance a pole on a moving cart. The environment provides:\n",
        "\n",
        "**State Space (4 continuous values):**\n",
        "- Cart Position: $x \\in [-2.4, 2.4]$\n",
        "- Cart Velocity: $\\dot{x} \\in [-\\infty, \\infty]$\n",
        "- Pole Angle: $\\theta \\in [-0.418, 0.418]$ rad (about ±24°)\n",
        "- Pole Angular Velocity: $\\dot{\\theta} \\in [-\\infty, \\infty]$\n",
        "\n",
        "**Action Space (2 discrete actions):**\n",
        "- 0: Push cart to the left\n",
        "- 1: Push cart to the right\n",
        "\n",
        "**Rewards:**\n",
        "- +1 for every timestep the pole remains upright\n",
        "- Episode ends when pole angle > ±12° or cart position > ±2.4\n",
        "- Maximum episode length: 500 steps\n",
        "- **Solved:** Average reward ≥ 475 over 100 consecutive episodes\n",
        "\n",
        "<!-- IMAGE PLACEHOLDER: CartPole diagram showing state variables -->\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <p style=\"color: #666; font-size: 12px; font-style: italic;\">[Image: CartPole state space visualization]</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 1: Install Dependencies and Load Utilities\n",
        "\n",
        "Purpose:\n",
        "  - Install required packages (PyTorch, Gym, etc.)\n",
        "  - Load pretty_print utility for formatted output\n",
        "  - Set random seeds for reproducibility\n",
        "\n",
        "Key Libraries:\n",
        "  - torch: Neural network implementation and training\n",
        "  - gym: OpenAI Gym environments\n",
        "  - numpy: Numerical operations\n",
        "  - matplotlib: Visualization\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages\n",
        "!pip install torch gym numpy matplotlib tqdm -q\n",
        "\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from tqdm.autonotebook import tqdm\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Fetch pretty_print utility\n",
        "try:\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/mdehghani86/RL_labs/master/utility/rl_utility.py'\n",
        "    response = requests.get(url)\n",
        "    exec(response.text)\n",
        "    pretty_print(\"Environment Ready\", \n",
        "                 \"Successfully loaded dependencies and utilities<br>\" +\n",
        "                 \"PyTorch, Gym, and visualization tools are ready<br>\" +\n",
        "                 f\"PyTorch version: {torch.__version__}\", \n",
        "                 style='success')\n",
        "except Exception as e:\n",
        "    # Fallback pretty_print\n",
        "    def pretty_print(title, content, style='info'):\n",
        "        themes = {\n",
        "            'info': {'primary': '#17a2b8', 'secondary': '#0e5a63', 'background': '#f8f9fa'},\n",
        "            'success': {'primary': '#28a745', 'secondary': '#155724', 'background': '#f8fff9'},\n",
        "            'warning': {'primary': '#ffc107', 'secondary': '#e0a800', 'background': '#fffdf5'},\n",
        "            'result': {'primary': '#6f42c1', 'secondary': '#4e2c8e', 'background': '#faf5ff'}\n",
        "        }\n",
        "        theme = themes.get(style, themes['info'])\n",
        "        html = f'''\n",
        "        <div style=\"border-radius: 5px; margin: 10px 0; width: 20cm; max-width: 20cm; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
        "            <div style=\"background: linear-gradient(90deg, {theme['primary']} 0%, {theme['secondary']} 100%); padding: 10px 15px; border-radius: 5px 5px 0 0;\">\n",
        "                <strong style=\"color: white; font-size: 14px;\">{title}</strong>\n",
        "            </div>\n",
        "            <div style=\"background: {theme['background']}; padding: 10px 15px; border-radius: 0 0 5px 5px; border-left: 3px solid {theme['primary']};\">\n",
        "                <div style=\"color: rgba(0,0,0,0.8); font-size: 12px; line-height: 1.5;\">{content}</div>\n",
        "            </div>\n",
        "        </div>\n",
        "        '''\n",
        "        display(HTML(html))\n",
        "    \n",
        "    pretty_print(\"Fallback Mode\", \n",
        "                 f\"Using local pretty_print<br>Dependencies loaded successfully\", \n",
        "                 style='warning')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 2: Create and Visualize CartPole Environment\n",
        "\n",
        "Purpose:\n",
        "  - Initialize the CartPole-v1 environment\n",
        "  - Understand state and action spaces\n",
        "  - Visualize a random agent's performance\n",
        "\n",
        "Expected Output:\n",
        "  - Environment specifications\n",
        "  - Animated visualization of untrained behavior\n",
        "\"\"\"\n",
        "\n",
        "# Create environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Get environment specifications\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "pretty_print(\"CartPole-v1 Environment\",\n",
        "             f\"State space dimension: {state_dim}<br>\" +\n",
        "             f\"Action space dimension: {action_dim}<br>\" +\n",
        "             f\"State: [position, velocity, angle, angular_velocity]<br>\" +\n",
        "             f\"Actions: 0=Left, 1=Right\",\n",
        "             style='info')\n",
        "\n",
        "# Demonstrate random agent\n",
        "def run_random_agent(env, num_episodes=3, render=True):\n",
        "    \"\"\"\n",
        "    Run episodes with random action selection.\n",
        "    \n",
        "    Args:\n",
        "        env: Gym environment\n",
        "        num_episodes: Number of episodes to run\n",
        "        render: Whether to render (set False in Colab)\n",
        "    \n",
        "    Returns:\n",
        "        List of episode rewards\n",
        "    \"\"\"\n",
        "    episode_rewards = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            if render:\n",
        "                try:\n",
        "                    env.render()\n",
        "                except:\n",
        "                    pass  # Skip rendering if not available\n",
        "            \n",
        "            action = env.action_space.sample()  # Random action\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
        "    \n",
        "    env.close()\n",
        "    return episode_rewards\n",
        "\n",
        "# Run random agent\n",
        "print(\"\\nRandom Agent Performance:\")\n",
        "random_rewards = run_random_agent(env, num_episodes=3, render=False)\n",
        "\n",
        "pretty_print(\"Random Agent Baseline\",\n",
        "             f\"Average reward: {np.mean(random_rewards):.1f}<br>\" +\n",
        "             f\"The pole falls quickly with random actions<br>\" +\n",
        "             f\"Our goal: Train agents to achieve ~475+ reward\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 2: Deep Q-Network (DQN)</h2>\n",
        "</div>\n",
        "\n",
        "### From Tabular Q-Learning to Function Approximation\n",
        "\n",
        "In Lab 6, we used **tabular Q-learning** where Q-values were stored in a lookup table. This works well for discrete, small state spaces (Taxi: 500 states). However, CartPole has **continuous states** and even simple discretization leads to millions of states.\n",
        "\n",
        "**Solution:** Use a **neural network** to approximate $Q(s,a)$.\n",
        "\n",
        "### The Q-Learning Update Rule\n",
        "\n",
        "Recall the Q-learning update from Chapter 6:\n",
        "\n",
        "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\right]$$\n",
        "\n",
        "With function approximation using parameters $\\theta$:\n",
        "\n",
        "$$Q(s,a;\\theta) \\approx Q^*(s,a)$$\n",
        "\n",
        "The loss function for a single transition becomes:\n",
        "\n",
        "$$L(\\theta) = \\left[r + \\gamma \\max_{a'} Q(s',a';\\theta) - Q(s,a;\\theta)\\right]^2$$\n",
        "\n",
        "### DQN Innovations\n",
        "\n",
        "**1. Experience Replay Buffer**\n",
        "\n",
        "Neural networks require i.i.d. (independent and identically distributed) data, but RL generates correlated sequential data. Experience replay breaks these correlations:\n",
        "\n",
        "$$\\mathcal{D} = \\{(s_t, a_t, r_t, s_{t+1})\\}_{t=1}^N$$\n",
        "\n",
        "Sample random minibatches from $\\mathcal{D}$ for training.\n",
        "\n",
        "**2. Target Network**\n",
        "\n",
        "The Q-learning target $r + \\gamma \\max_{a'} Q(s',a';\\theta)$ uses the same network being updated, causing instability. DQN uses a separate **target network** $\\theta^-$ that is updated periodically:\n",
        "\n",
        "$$L(\\theta) = \\left[r + \\gamma \\max_{a'} Q(s',a';\\theta^-) - Q(s,a;\\theta)\\right]^2$$\n",
        "\n",
        "Every $C$ steps: $\\theta^- \\leftarrow \\theta$\n",
        "\n",
        "<!-- IMAGE PLACEHOLDER: DQN architecture diagram -->\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <p style=\"color: #666; font-size: 12px; font-style: italic;\">[Image: DQN architecture with experience replay and target network]</p>\n",
        "</div>\n",
        "\n",
        "### DQN Algorithm Pseudocode\n",
        "\n",
        "```\n",
        "Initialize Q-network Q(s,a;θ) with random weights\n",
        "Initialize target network Q(s,a;θ⁻) = Q(s,a;θ)\n",
        "Initialize replay buffer D\n",
        "\n",
        "For episode = 1 to M:\n",
        "    Initialize state s\n",
        "    For t = 1 to T:\n",
        "        Select action: a = ε-greedy(Q(s,·;θ))\n",
        "        Execute action a, observe r, s'\n",
        "        Store transition (s,a,r,s') in D\n",
        "        \n",
        "        Sample random minibatch from D\n",
        "        For each transition (sⱼ,aⱼ,rⱼ,s'ⱼ):\n",
        "            yⱼ = rⱼ + γ max_a' Q(s'ⱼ,a';θ⁻)\n",
        "        \n",
        "        Update θ by minimizing (yⱼ - Q(sⱼ,aⱼ;θ))²\n",
        "        \n",
        "        Every C steps: θ⁻ ← θ\n",
        "        s ← s'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 3: DQN Neural Network Architecture\n",
        "\n",
        "Purpose:\n",
        "  - Define the Q-network structure\n",
        "  - Implement forward pass for Q-value prediction\n",
        "\n",
        "Network Architecture:\n",
        "  Input Layer: state_dim neurons (4 for CartPole)\n",
        "  Hidden Layer 1: 128 neurons with ReLU activation\n",
        "  Hidden Layer 2: 128 neurons with ReLU activation\n",
        "  Output Layer: action_dim neurons (2 for CartPole)\n",
        "  \n",
        "  Output: Q(s,a) for each action a\n",
        "\"\"\"\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Q-Network for value function approximation.\n",
        "    \n",
        "    Architecture:\n",
        "        state → FC(128) → ReLU → FC(128) → ReLU → FC(action_dim)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        \"\"\"\n",
        "        Initialize DQN.\n",
        "        \n",
        "        Args:\n",
        "            state_dim (int): Dimension of state space\n",
        "            action_dim (int): Number of actions\n",
        "            hidden_dim (int): Hidden layer size\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
        "    \n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Forward pass through network.\n",
        "        \n",
        "        Args:\n",
        "            state: State tensor [batch_size, state_dim]\n",
        "        \n",
        "        Returns:\n",
        "            Q-values for each action [batch_size, action_dim]\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        q_values = self.fc3(x)\n",
        "        return q_values\n",
        "\n",
        "# Test network\n",
        "test_network = DQN(state_dim=4, action_dim=2)\n",
        "test_state = torch.randn(1, 4)  # Batch of 1 state\n",
        "test_output = test_network(test_state)\n",
        "\n",
        "pretty_print(\"DQN Network Initialized\",\n",
        "             f\"Network architecture: {state_dim} → 128 → 128 → {action_dim}<br>\" +\n",
        "             f\"Total parameters: {sum(p.numel() for p in test_network.parameters()):,}<br>\" +\n",
        "             f\"Test output shape: {test_output.shape}<br>\" +\n",
        "             f\"Sample Q-values: {test_output.detach().numpy()}\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 4: Experience Replay Buffer\n",
        "\n",
        "Purpose:\n",
        "  - Implement replay buffer for storing transitions\n",
        "  - Enable random sampling for breaking correlations\n",
        "\n",
        "Key Methods:\n",
        "  - push(): Add new transition to buffer\n",
        "  - sample(): Sample random minibatch\n",
        "  - __len__(): Return current buffer size\n",
        "\"\"\"\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Experience Replay Buffer for DQN.\n",
        "    \n",
        "    Stores transitions (s, a, r, s', done) and samples random minibatches.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, capacity=10000):\n",
        "        \"\"\"\n",
        "        Initialize replay buffer.\n",
        "        \n",
        "        Args:\n",
        "            capacity (int): Maximum buffer size\n",
        "        \"\"\"\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Add transition to buffer.\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "            action: Action taken\n",
        "            reward: Reward received\n",
        "            next_state: Next state\n",
        "            done: Whether episode ended\n",
        "        \"\"\"\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample random minibatch from buffer.\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int): Size of minibatch\n",
        "        \n",
        "        Returns:\n",
        "            Tuple of (states, actions, rewards, next_states, dones)\n",
        "        \"\"\"\n",
        "        # Random sample\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        # Unzip batch\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        \n",
        "        # Convert to numpy arrays\n",
        "        states = np.array(states)\n",
        "        actions = np.array(actions)\n",
        "        rewards = np.array(rewards)\n",
        "        next_states = np.array(next_states)\n",
        "        dones = np.array(dones)\n",
        "        \n",
        "        return states, actions, rewards, next_states, dones\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"Return current buffer size.\"\"\"\n",
        "        return len(self.buffer)\n",
        "\n",
        "# Test replay buffer\n",
        "test_buffer = ReplayBuffer(capacity=1000)\n",
        "for i in range(5):\n",
        "    test_buffer.push(np.zeros(4), 0, 1.0, np.zeros(4), False)\n",
        "\n",
        "pretty_print(\"Replay Buffer Initialized\",\n",
        "             f\"Buffer capacity: 10,000 transitions<br>\" +\n",
        "             f\"Current size: {len(test_buffer)}<br>\" +\n",
        "             f\"Sample batch shape will be: (batch_size, state_dim)\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 5: DQN Agent Implementation\n",
        "\n",
        "Purpose:\n",
        "  - Implement complete DQN agent\n",
        "  - Epsilon-greedy action selection\n",
        "  - Training step with target network\n",
        "\n",
        "Key Components:\n",
        "  - Main Q-network (updated every step)\n",
        "  - Target Q-network (updated every C steps)\n",
        "  - Experience replay buffer\n",
        "  - Epsilon-greedy exploration\n",
        "\"\"\"\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    DQN Agent with experience replay and target network.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001, \n",
        "                 gamma=0.99, epsilon=1.0, epsilon_decay=0.995, \n",
        "                 epsilon_min=0.01, buffer_size=10000, batch_size=64,\n",
        "                 target_update=10):\n",
        "        \"\"\"\n",
        "        Initialize DQN agent.\n",
        "        \n",
        "        Args:\n",
        "            state_dim: Dimension of state space\n",
        "            action_dim: Number of actions\n",
        "            learning_rate: Learning rate for optimizer\n",
        "            gamma: Discount factor\n",
        "            epsilon: Initial exploration rate\n",
        "            epsilon_decay: Epsilon decay rate\n",
        "            epsilon_min: Minimum epsilon\n",
        "            buffer_size: Replay buffer capacity\n",
        "            batch_size: Minibatch size\n",
        "            target_update: Steps between target network updates\n",
        "        \"\"\"\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update = target_update\n",
        "        self.step_count = 0\n",
        "        \n",
        "        # Initialize networks\n",
        "        self.q_network = DQN(state_dim, action_dim).to(device)\n",
        "        self.target_network = DQN(state_dim, action_dim).to(device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        \n",
        "        # Optimizer and replay buffer\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
        "    \n",
        "    def select_action(self, state, train=True):\n",
        "        \"\"\"\n",
        "        Select action using epsilon-greedy policy.\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "            train: If True, use epsilon-greedy; else greedy\n",
        "        \n",
        "        Returns:\n",
        "            Selected action\n",
        "        \"\"\"\n",
        "        if train and random.random() < self.epsilon:\n",
        "            # Explore: random action\n",
        "            return random.randint(0, self.action_dim - 1)\n",
        "        else:\n",
        "            # Exploit: best action\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                q_values = self.q_network(state_tensor)\n",
        "                return q_values.argmax().item()\n",
        "    \n",
        "    def train_step(self):\n",
        "        \"\"\"\n",
        "        Perform one training step.\n",
        "        \n",
        "        Returns:\n",
        "            Loss value\n",
        "        \"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        # Sample minibatch\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.LongTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        next_states = torch.FloatTensor(next_states).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "        \n",
        "        # Current Q-values: Q(s,a)\n",
        "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "        \n",
        "        # Target Q-values: r + γ max_a' Q_target(s',a')\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_network(next_states).max(1)[0]\n",
        "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(current_q_values, target_q_values)\n",
        "        \n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Update target network\n",
        "        self.step_count += 1\n",
        "        if self.step_count % self.target_update == 0:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        \n",
        "        # Decay epsilon\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "        \n",
        "        return loss.item()\n",
        "\n",
        "pretty_print(\"DQN Agent Initialized\",\n",
        "             f\"Main Q-network parameters: {sum(p.numel() for p in DQN(state_dim, action_dim).parameters()):,}<br>\" +\n",
        "             f\"Target network: Separate copy, updated every {10} steps<br>\" +\n",
        "             f\"Replay buffer capacity: 10,000 transitions<br>\" +\n",
        "             f\"Ready to train!\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 6: Train DQN Agent\n",
        "\n",
        "Purpose:\n",
        "  - Train DQN agent on CartPole\n",
        "  - Track episode rewards and losses\n",
        "  - Visualize learning progress\n",
        "\n",
        "Training Loop:\n",
        "  1. Select action using epsilon-greedy\n",
        "  2. Execute action and store transition\n",
        "  3. Sample minibatch and update Q-network\n",
        "  4. Periodically update target network\n",
        "\n",
        "Expected Results:\n",
        "  - Convergence to ~475+ reward in 200-400 episodes\n",
        "\"\"\"\n",
        "\n",
        "def train_dqn(num_episodes=400, max_steps=500):\n",
        "    \"\"\"\n",
        "    Train DQN agent.\n",
        "    \n",
        "    Args:\n",
        "        num_episodes: Number of training episodes\n",
        "        max_steps: Maximum steps per episode\n",
        "    \n",
        "    Returns:\n",
        "        episode_rewards: List of episode rewards\n",
        "        losses: List of training losses\n",
        "    \"\"\"\n",
        "    # Initialize agent and environment\n",
        "    agent = DQNAgent(state_dim, action_dim)\n",
        "    env = gym.make('CartPole-v1')\n",
        "    \n",
        "    episode_rewards = []\n",
        "    losses = []\n",
        "    \n",
        "    pretty_print(\"Training DQN\",\n",
        "                 f\"Episodes: {num_episodes}<br>\" +\n",
        "                 f\"Max steps per episode: {max_steps}<br>\" +\n",
        "                 f\"This may take 5-10 minutes...\",\n",
        "                 style='info')\n",
        "    \n",
        "    for episode in tqdm(range(num_episodes), desc=\"Training DQN\"):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_losses = []\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Select and execute action\n",
        "            action = agent.select_action(state, train=True)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # Store transition\n",
        "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
        "            \n",
        "            # Train\n",
        "            loss = agent.train_step()\n",
        "            if loss is not None:\n",
        "                episode_losses.append(loss)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        if episode_losses:\n",
        "            losses.append(np.mean(episode_losses))\n",
        "        \n",
        "        # Print progress every 50 episodes\n",
        "        if (episode + 1) % 50 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-50:])\n",
        "            print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}, Epsilon = {agent.epsilon:.3f}\")\n",
        "    \n",
        "    env.close()\n",
        "    return agent, episode_rewards, losses\n",
        "\n",
        "# Train DQN\n",
        "dqn_agent, dqn_rewards, dqn_losses = train_dqn(num_episodes=400)\n",
        "\n",
        "pretty_print(\"DQN Training Complete\",\n",
        "             f\"Final 100-episode average: {np.mean(dqn_rewards[-100:]):.2f}<br>\" +\n",
        "             f\"Best episode reward: {max(dqn_rewards):.0f}<br>\" +\n",
        "             f\"Final epsilon: {dqn_agent.epsilon:.3f}\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 7: Visualize DQN Training Results\n",
        "\n",
        "Purpose:\n",
        "  - Plot learning curves\n",
        "  - Analyze training dynamics\n",
        "\"\"\"\n",
        "\n",
        "def plot_training_results(rewards, losses, title, window=10):\n",
        "    \"\"\"\n",
        "    Plot training results.\n",
        "    \n",
        "    Args:\n",
        "        rewards: List of episode rewards\n",
        "        losses: List of losses\n",
        "        title: Plot title\n",
        "        window: Moving average window\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot rewards\n",
        "    ax1.plot(rewards, alpha=0.3, color='gray', label='Episode reward')\n",
        "    moving_avg = [np.mean(rewards[max(0, i-window):i+1]) for i in range(len(rewards))]\n",
        "    ax1.plot(moving_avg, color='#17a2b8', linewidth=2, label=f'{window}-episode average')\n",
        "    ax1.axhline(y=475, color='green', linestyle='--', linewidth=2, label='Solved threshold')\n",
        "    ax1.set_xlabel('Episode')\n",
        "    ax1.set_ylabel('Reward')\n",
        "    ax1.set_title(f'{title} - Episode Rewards')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot losses\n",
        "    if losses:\n",
        "        ax2.plot(losses, color='#e74c3c', alpha=0.6)\n",
        "        ax2.set_xlabel('Episode')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_title(f'{title} - Training Loss')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot DQN results\n",
        "plot_training_results(dqn_rewards, dqn_losses, 'DQN', window=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 3: Double DQN (DDQN)</h2>\n",
        "</div>\n",
        "\n",
        "### The Overestimation Problem\n",
        "\n",
        "Standard DQN suffers from **Q-value overestimation** due to the max operator in the target:\n",
        "\n",
        "$$y = r + \\gamma \\max_{a'} Q(s',a';\\theta^-)$$\n",
        "\n",
        "The same network is used to both **select** and **evaluate** the best action. This leads to systematic positive bias:\n",
        "\n",
        "$$\\mathbb{E}[\\max_{a'} Q(s',a')] \\geq \\max_{a'} \\mathbb{E}[Q(s',a')]$$\n",
        "\n",
        "### Double Q-Learning Solution\n",
        "\n",
        "**Key Idea:** Decouple action selection from action evaluation using two networks.\n",
        "\n",
        "**DQN Target:**\n",
        "$$y^{\\text{DQN}} = r + \\gamma Q(s', \\arg\\max_{a'} Q(s',a';\\theta^-); \\theta^-)$$\n",
        "$$\\text{↑ Same network selects AND evaluates}$$\n",
        "\n",
        "**DDQN Target:**\n",
        "$$y^{\\text{DDQN}} = r + \\gamma Q(s', \\arg\\max_{a'} Q(s',a';\\theta); \\theta^-)$$\n",
        "$$\\text{↑ Main network selects, target network evaluates}$$\n",
        "\n",
        "<!-- IMAGE PLACEHOLDER: DQN vs DDQN comparison diagram -->\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <p style=\"color: #666; font-size: 12px; font-style: italic;\">[Image: DQN overestimation vs DDQN correction]</p>\n",
        "</div>\n",
        "\n",
        "### Implementation Change\n",
        "\n",
        "DDQN requires only a **single line change** from DQN:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">Hands-On Exercise 1: Implement Double DQN</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Modify the DQN training step to implement DDQN. Only the target computation needs to change.\n",
        "        <br><br>\n",
        "        <strong>Hint:</strong> Use the main Q-network to select the best action, then use the target network to evaluate it.\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 8: Double DQN Agent Implementation\n",
        "\n",
        "Purpose:\n",
        "  - Implement DDQN by modifying the target computation\n",
        "  - Compare performance with standard DQN\n",
        "\n",
        "Key Modification:\n",
        "  - Use main network for action selection\n",
        "  - Use target network for action evaluation\n",
        "\"\"\"\n",
        "\n",
        "class DoubleDQNAgent(DQNAgent):\n",
        "    \"\"\"\n",
        "    Double DQN Agent - inherits from DQN, modifies target computation.\n",
        "    \"\"\"\n",
        "    \n",
        "    def train_step(self):\n",
        "        \"\"\"\n",
        "        Perform one DDQN training step.\n",
        "        \n",
        "        Key difference from DQN: Decoupled action selection and evaluation.\n",
        "        \n",
        "        Returns:\n",
        "            Loss value\n",
        "        \"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        # Sample minibatch\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.LongTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        next_states = torch.FloatTensor(next_states).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "        \n",
        "        # Current Q-values: Q(s,a)\n",
        "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "        \n",
        "        # ================================================================\n",
        "        # DOUBLE DQN TARGET COMPUTATION\n",
        "        # ================================================================\n",
        "        with torch.no_grad():\n",
        "            # HANDS-ON EXERCISE 1: Complete the DDQN target computation\n",
        "            \n",
        "            # Step 1: Use MAIN network to SELECT best actions for next states\n",
        "            # Hint: next_actions = self.q_network(next_states).argmax(1)\n",
        "            next_actions = ### YOUR CODE HERE ###\n",
        "            \n",
        "            # Step 2: Use TARGET network to EVALUATE those actions\n",
        "            # Hint: next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "            next_q_values = ### YOUR CODE HERE ###\n",
        "            \n",
        "            # Step 3: Compute target Q-values\n",
        "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "        \n",
        "        # Compute loss (same as DQN)\n",
        "        loss = F.mse_loss(current_q_values, target_q_values)\n",
        "        \n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Update target network\n",
        "        self.step_count += 1\n",
        "        if self.step_count % self.target_update == 0:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        \n",
        "        # Decay epsilon\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "        \n",
        "        return loss.item()\n",
        "\n",
        "pretty_print(\"Double DQN Implementation\",\n",
        "             \"Key modification: Decoupled action selection and evaluation<br>\" +\n",
        "             \"Main network selects actions, target network evaluates them<br>\" +\n",
        "             \"This reduces overestimation bias\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 9: Train Double DQN Agent\n",
        "\n",
        "Purpose:\n",
        "  - Train DDQN agent\n",
        "  - Compare with DQN performance\n",
        "\"\"\"\n",
        "\n",
        "def train_ddqn(num_episodes=400, max_steps=500):\n",
        "    \"\"\"\n",
        "    Train Double DQN agent.\n",
        "    \n",
        "    Args:\n",
        "        num_episodes: Number of training episodes\n",
        "        max_steps: Maximum steps per episode\n",
        "    \n",
        "    Returns:\n",
        "        agent: Trained agent\n",
        "        episode_rewards: List of episode rewards\n",
        "        losses: List of training losses\n",
        "    \"\"\"\n",
        "    # Initialize agent and environment\n",
        "    agent = DoubleDQNAgent(state_dim, action_dim)\n",
        "    env = gym.make('CartPole-v1')\n",
        "    \n",
        "    episode_rewards = []\n",
        "    losses = []\n",
        "    \n",
        "    pretty_print(\"Training Double DQN\",\n",
        "                 f\"Episodes: {num_episodes}<br>\" +\n",
        "                 f\"Expected improvement: More stable, less overestimation\",\n",
        "                 style='info')\n",
        "    \n",
        "    for episode in tqdm(range(num_episodes), desc=\"Training DDQN\"):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_losses = []\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            action = agent.select_action(state, train=True)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
        "            \n",
        "            loss = agent.train_step()\n",
        "            if loss is not None:\n",
        "                episode_losses.append(loss)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        if episode_losses:\n",
        "            losses.append(np.mean(episode_losses))\n",
        "        \n",
        "        if (episode + 1) % 50 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-50:])\n",
        "            print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}\")\n",
        "    \n",
        "    env.close()\n",
        "    return agent, episode_rewards, losses\n",
        "\n",
        "# Train DDQN\n",
        "ddqn_agent, ddqn_rewards, ddqn_losses = train_ddqn(num_episodes=400)\n",
        "\n",
        "pretty_print(\"Double DQN Training Complete\",\n",
        "             f\"Final 100-episode average: {np.mean(ddqn_rewards[-100:]):.2f}<br>\" +\n",
        "             f\"Best episode reward: {max(ddqn_rewards):.0f}<br>\" +\n",
        "             f\"Comparison with DQN: Check plots below\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 10: Compare DQN vs DDQN\n",
        "\n",
        "Purpose:\n",
        "  - Visualize performance comparison\n",
        "  - Analyze differences in learning dynamics\n",
        "\"\"\"\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "window = 10\n",
        "\n",
        "# DQN\n",
        "dqn_ma = [np.mean(dqn_rewards[max(0, i-window):i+1]) for i in range(len(dqn_rewards))]\n",
        "plt.plot(dqn_ma, label='DQN', color='#17a2b8', linewidth=2)\n",
        "\n",
        "# DDQN\n",
        "ddqn_ma = [np.mean(ddqn_rewards[max(0, i-window):i+1]) for i in range(len(ddqn_rewards))]\n",
        "plt.plot(ddqn_ma, label='Double DQN', color='#e74c3c', linewidth=2)\n",
        "\n",
        "plt.axhline(y=475, color='green', linestyle='--', linewidth=2, label='Solved threshold')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel(f'{window}-Episode Moving Average Reward')\n",
        "plt.title('DQN vs Double DQN Performance Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "pretty_print(\"DQN vs DDQN Comparison\",\n",
        "             f\"<strong>DQN:</strong><br>\" +\n",
        "             f\"  • Final average: {np.mean(dqn_rewards[-100:]):.2f}<br>\" +\n",
        "             f\"  • Best episode: {max(dqn_rewards):.0f}<br><br>\" +\n",
        "             f\"<strong>Double DQN:</strong><br>\" +\n",
        "             f\"  • Final average: {np.mean(ddqn_rewards[-100:]):.2f}<br>\" +\n",
        "             f\"  • Best episode: {max(ddqn_rewards):.0f}<br><br>\" +\n",
        "             f\"<strong>Observation:</strong> DDQN often shows more stable learning\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 4: Policy Gradient Methods - REINFORCE</h2>\n",
        "</div>\n",
        "\n",
        "### A Different Approach: Direct Policy Optimization\n",
        "\n",
        "So far, we've used **value-based methods** (DQN, DDQN) that:\n",
        "1. Learn Q(s,a)\n",
        "2. Derive policy: $\\pi(s) = \\arg\\max_a Q(s,a)$\n",
        "\n",
        "**Policy gradient methods** take a fundamentally different approach:\n",
        "- **Directly parameterize the policy:** $\\pi(a|s;\\theta)$\n",
        "- **Optimize policy parameters** to maximize expected return\n",
        "\n",
        "### Why Policy Gradients?\n",
        "\n",
        "**Advantages:**\n",
        "1. Can handle **continuous action spaces** (DQN cannot)\n",
        "2. Can learn **stochastic policies** (sometimes optimal)\n",
        "3. Better **convergence properties** (smooth policy changes)\n",
        "\n",
        "**Disadvantages:**\n",
        "1. Higher **variance** in gradient estimates\n",
        "2. Can converge to **local optima**\n",
        "3. Typically requires more **samples**\n",
        "\n",
        "### Policy Parameterization\n",
        "\n",
        "For discrete actions, use softmax policy:\n",
        "\n",
        "$$\\pi(a|s;\\theta) = \\frac{e^{h(s,a;\\theta)}}{\\sum_{a'} e^{h(s,a';\\theta)}}$$\n",
        "\n",
        "where $h(s,a;\\theta)$ are **action preferences** from a neural network.\n",
        "\n",
        "### The Policy Gradient Theorem (Sutton & Barto, Section 13.2)\n",
        "\n",
        "**Objective:** Maximize expected return\n",
        "\n",
        "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$$\n",
        "\n",
        "where $\\tau$ is a trajectory and $R(\\tau) = \\sum_t r_t$ is the return.\n",
        "\n",
        "**The gradient:**\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi(a_t|s_t;\\theta) \\cdot G_t\\right]$$\n",
        "\n",
        "where $G_t = \\sum_{t'=t}^T \\gamma^{t'-t} r_{t'}$ is the return from timestep $t$.\n",
        "\n",
        "**Intuition:**\n",
        "- $G_t > 0$: Increase probability of action $a_t$ in state $s_t$\n",
        "- $G_t < 0$: Decrease probability of action $a_t$ in state $s_t$\n",
        "\n",
        "### REINFORCE Algorithm (Monte Carlo Policy Gradient)\n",
        "\n",
        "```\n",
        "Initialize policy parameters θ\n",
        "\n",
        "For each episode:\n",
        "    Generate episode τ = (s₀, a₀, r₁, s₁, a₁, r₂, ..., s_T) using π(·|·;θ)\n",
        "    \n",
        "    For each timestep t in episode:\n",
        "        Compute return: G_t = Σ γᵏ r_{t+k}\n",
        "        Update: θ ← θ + α G_t ∇_θ log π(a_t|s_t;θ)\n",
        "```\n",
        "\n",
        "<!-- IMAGE PLACEHOLDER: Policy gradient intuition diagram -->\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <p style=\"color: #666; font-size: 12px; font-style: italic;\">[Image: Policy gradient making good actions more likely]</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 11: Policy Network Implementation\n",
        "\n",
        "Purpose:\n",
        "  - Implement policy network that outputs action probabilities\n",
        "  - Enable action sampling and log probability computation\n",
        "\n",
        "Network Architecture:\n",
        "  Input: state\n",
        "  Hidden layers: 128 neurons with ReLU\n",
        "  Output: action probabilities (softmax)\n",
        "\"\"\"\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Policy Network for REINFORCE.\n",
        "    Outputs probability distribution over actions.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        \"\"\"\n",
        "        Initialize policy network.\n",
        "        \n",
        "        Args:\n",
        "            state_dim: Dimension of state space\n",
        "            action_dim: Number of actions\n",
        "            hidden_dim: Hidden layer size\n",
        "        \"\"\"\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
        "    \n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            state: State tensor\n",
        "        \n",
        "        Returns:\n",
        "            Action probabilities (after softmax)\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        action_probs = F.softmax(self.fc3(x), dim=-1)\n",
        "        return action_probs\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        Select action by sampling from policy.\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "        \n",
        "        Returns:\n",
        "            action: Sampled action\n",
        "            log_prob: Log probability of action\n",
        "        \"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        action_probs = self.forward(state_tensor)\n",
        "        \n",
        "        # Sample action from distribution\n",
        "        dist = torch.distributions.Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "        \n",
        "        return action.item(), log_prob\n",
        "\n",
        "# Test policy network\n",
        "test_policy = PolicyNetwork(state_dim, action_dim)\n",
        "test_state = np.random.randn(state_dim)\n",
        "test_action, test_log_prob = test_policy.select_action(test_state)\n",
        "\n",
        "pretty_print(\"Policy Network Initialized\",\n",
        "             f\"Network outputs action probabilities<br>\" +\n",
        "             f\"Sample action: {test_action}<br>\" +\n",
        "             f\"Log probability: {test_log_prob.item():.3f}<br>\" +\n",
        "             f\"Actions are sampled stochastically\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 12: REINFORCE Agent Implementation\n",
        "\n",
        "Purpose:\n",
        "  - Implement complete REINFORCE algorithm\n",
        "  - Monte Carlo return computation\n",
        "  - Policy gradient update\n",
        "\n",
        "Key Components:\n",
        "  - Episode memory (states, actions, rewards)\n",
        "  - Return computation with discount\n",
        "  - Policy gradient ascent\n",
        "\"\"\"\n",
        "\n",
        "class REINFORCEAgent:\n",
        "    \"\"\"\n",
        "    REINFORCE (Monte Carlo Policy Gradient) Agent.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99):\n",
        "        \"\"\"\n",
        "        Initialize REINFORCE agent.\n",
        "        \n",
        "        Args:\n",
        "            state_dim: Dimension of state space\n",
        "            action_dim: Number of actions\n",
        "            learning_rate: Learning rate\n",
        "            gamma: Discount factor\n",
        "        \"\"\"\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # Initialize policy network\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
        "        \n",
        "        # Episode memory\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.log_probs = []\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        Select action using current policy.\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "        \n",
        "        Returns:\n",
        "            action: Selected action\n",
        "        \"\"\"\n",
        "        action, log_prob = self.policy.select_action(state)\n",
        "        \n",
        "        # Store for episode\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.log_probs.append(log_prob)\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    def store_reward(self, reward):\n",
        "        \"\"\"Store reward for current timestep.\"\"\"\n",
        "        self.rewards.append(reward)\n",
        "    \n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        Perform REINFORCE update after episode completion.\n",
        "        \n",
        "        Steps:\n",
        "          1. Compute returns G_t for each timestep\n",
        "          2. Compute policy gradient\n",
        "          3. Update policy parameters\n",
        "        \"\"\"\n",
        "        # Compute returns\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(self.rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        \n",
        "        # Normalize returns (reduce variance)\n",
        "        returns = torch.FloatTensor(returns).to(device)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "        \n",
        "        # Compute policy loss\n",
        "        policy_loss = []\n",
        "        for log_prob, G in zip(self.log_probs, returns):\n",
        "            policy_loss.append(-log_prob * G)  # Negative for gradient ascent\n",
        "        \n",
        "        policy_loss = torch.stack(policy_loss).sum()\n",
        "        \n",
        "        # Update policy\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Clear episode memory\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.log_probs = []\n",
        "        \n",
        "        return policy_loss.item()\n",
        "\n",
        "pretty_print(\"REINFORCE Agent Initialized\",\n",
        "             \"Algorithm: Monte Carlo Policy Gradient<br>\" +\n",
        "             \"Update: After each complete episode<br>\" +\n",
        "             \"No replay buffer needed (on-policy)\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 13: Train REINFORCE Agent\n",
        "\n",
        "Purpose:\n",
        "  - Train REINFORCE agent on CartPole\n",
        "  - Compare with value-based methods (DQN/DDQN)\n",
        "\n",
        "Expected Behavior:\n",
        "  - Higher variance than DQN\n",
        "  - May take longer to converge\n",
        "  - But demonstrates direct policy optimization\n",
        "\"\"\"\n",
        "\n",
        "def train_reinforce(num_episodes=1000, max_steps=500):\n",
        "    \"\"\"\n",
        "    Train REINFORCE agent.\n",
        "    \n",
        "    Args:\n",
        "        num_episodes: Number of training episodes\n",
        "        max_steps: Maximum steps per episode\n",
        "    \n",
        "    Returns:\n",
        "        agent: Trained agent\n",
        "        episode_rewards: List of episode rewards\n",
        "        losses: List of losses\n",
        "    \"\"\"\n",
        "    agent = REINFORCEAgent(state_dim, action_dim, learning_rate=0.001)\n",
        "    env = gym.make('CartPole-v1')\n",
        "    \n",
        "    episode_rewards = []\n",
        "    losses = []\n",
        "    \n",
        "    pretty_print(\"Training REINFORCE\",\n",
        "                 f\"Episodes: {num_episodes}<br>\" +\n",
        "                 f\"Note: Policy gradient methods typically have higher variance\",\n",
        "                 style='info')\n",
        "    \n",
        "    for episode in tqdm(range(num_episodes), desc=\"Training REINFORCE\"):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            agent.store_reward(reward)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Update policy after episode\n",
        "        loss = agent.update()\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        losses.append(loss)\n",
        "        \n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}\")\n",
        "    \n",
        "    env.close()\n",
        "    return agent, episode_rewards, losses\n",
        "\n",
        "# Train REINFORCE\n",
        "reinforce_agent, reinforce_rewards, reinforce_losses = train_reinforce(num_episodes=1000)\n",
        "\n",
        "pretty_print(\"REINFORCE Training Complete\",\n",
        "             f\"Final 100-episode average: {np.mean(reinforce_rewards[-100:]):.2f}<br>\" +\n",
        "             f\"Best episode reward: {max(reinforce_rewards):.0f}<br>\" +\n",
        "             f\"Notice the higher variance compared to DQN\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 14: Visualize REINFORCE Results\n",
        "\n",
        "Purpose:\n",
        "  - Plot REINFORCE learning curves\n",
        "  - Analyze variance in policy gradient methods\n",
        "\"\"\"\n",
        "\n",
        "plot_training_results(reinforce_rewards, reinforce_losses, 'REINFORCE', window=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 5: Actor-Critic Methods</h2>\n",
        "</div>\n",
        "\n",
        "### Combining the Best of Both Worlds\n",
        "\n",
        "REINFORCE has high variance because it uses full episode returns. We can reduce variance by introducing a **baseline**.\n",
        "\n",
        "### Actor-Critic Architecture\n",
        "\n",
        "**Two networks:**\n",
        "1. **Actor:** Policy network $\\pi(a|s;\\theta)$ (decides which action to take)\n",
        "2. **Critic:** Value network $V(s;w)$ (evaluates how good the state is)\n",
        "\n",
        "**The advantage function:**\n",
        "\n",
        "$$A(s,a) = Q(s,a) - V(s)$$\n",
        "\n",
        "Measures how much better action $a$ is compared to the average action in state $s$.\n",
        "\n",
        "Using TD(0), we approximate the advantage:\n",
        "\n",
        "$$A(s,a) \\approx r + \\gamma V(s') - V(s) = \\delta$$\n",
        "\n",
        "This is the **TD error**.\n",
        "\n",
        "### Actor-Critic Update Rules\n",
        "\n",
        "**Critic update (TD learning):**\n",
        "$$w \\leftarrow w + \\alpha_w \\delta \\nabla_w V(s;w)$$\n",
        "\n",
        "**Actor update (policy gradient with advantage):**\n",
        "$$\\theta \\leftarrow \\theta + \\alpha_\\theta \\delta \\nabla_\\theta \\log \\pi(a|s;\\theta)$$\n",
        "\n",
        "<!-- IMAGE PLACEHOLDER: Actor-Critic architecture diagram -->\n",
        "<div style=\"text-align: center; margin: 20px 0;\">\n",
        "    <p style=\"color: #666; font-size: 12px; font-style: italic;\">[Image: Actor-Critic architecture with two networks]</p>\n",
        "</div>\n",
        "\n",
        "### Advantages of Actor-Critic\n",
        "\n",
        "1. **Lower variance** than REINFORCE (uses TD error instead of returns)\n",
        "2. **Online learning** (updates every step, not just end of episode)\n",
        "3. **Foundation for modern algorithms** (PPO, A3C, SAC, TD3)\n",
        "4. **Combines value and policy** approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin: 20px 0; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0;\">Hands-On Exercise 2: Implement Actor-Critic Agent</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        Complete the Actor-Critic implementation by filling in the update step.\n",
        "        <br><br>\n",
        "        <strong>Key steps:</strong><br>\n",
        "        1. Compute TD error: δ = r + γV(s') - V(s)<br>\n",
        "        2. Update critic using TD error<br>\n",
        "        3. Update actor using TD error as advantage\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 15: Value Network for Critic\n",
        "\n",
        "Purpose:\n",
        "  - Implement value network V(s;w)\n",
        "  - Estimates state values for advantage computation\n",
        "\"\"\"\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Value Network for Actor-Critic (Critic).\n",
        "    Estimates V(s).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, hidden_dim=128):\n",
        "        \"\"\"\n",
        "        Initialize value network.\n",
        "        \n",
        "        Args:\n",
        "            state_dim: Dimension of state space\n",
        "            hidden_dim: Hidden layer size\n",
        "        \"\"\"\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, 1)  # Output: single value V(s)\n",
        "    \n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            state: State tensor\n",
        "        \n",
        "        Returns:\n",
        "            State value V(s)\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        value = self.fc3(x)\n",
        "        return value\n",
        "\n",
        "pretty_print(\"Value Network (Critic)\",\n",
        "             \"Network outputs state values V(s)<br>\" +\n",
        "             \"Used to compute TD error for advantage estimation\",\n",
        "             style='info')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 16: Actor-Critic Agent Implementation\n",
        "\n",
        "Purpose:\n",
        "  - Implement complete Actor-Critic algorithm\n",
        "  - Combine policy network (actor) and value network (critic)\n",
        "  - Update both networks using TD error\n",
        "\"\"\"\n",
        "\n",
        "class ActorCriticAgent:\n",
        "    \"\"\"\n",
        "    Actor-Critic Agent.\n",
        "    \n",
        "    Components:\n",
        "      - Actor: Policy network\n",
        "      - Critic: Value network\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, \n",
        "                 actor_lr=0.001, critic_lr=0.005, gamma=0.99):\n",
        "        \"\"\"\n",
        "        Initialize Actor-Critic agent.\n",
        "        \n",
        "        Args:\n",
        "            state_dim: Dimension of state space\n",
        "            action_dim: Number of actions\n",
        "            actor_lr: Actor learning rate\n",
        "            critic_lr: Critic learning rate (typically higher)\n",
        "            gamma: Discount factor\n",
        "        \"\"\"\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # Actor (policy)\n",
        "        self.actor = PolicyNetwork(state_dim, action_dim).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "        \n",
        "        # Critic (value function)\n",
        "        self.critic = ValueNetwork(state_dim).to(device)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        Select action using actor.\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "        \n",
        "        Returns:\n",
        "            action: Selected action\n",
        "            log_prob: Log probability of action\n",
        "        \"\"\"\n",
        "        return self.actor.select_action(state)\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, done, log_prob):\n",
        "        \"\"\"\n",
        "        Perform Actor-Critic update.\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "            action: Action taken\n",
        "            reward: Reward received\n",
        "            next_state: Next state\n",
        "            done: Whether episode ended\n",
        "            log_prob: Log probability of action\n",
        "        \n",
        "        Returns:\n",
        "            actor_loss: Actor loss value\n",
        "            critic_loss: Critic loss value\n",
        "        \"\"\"\n",
        "        # Convert to tensors\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
        "        reward_tensor = torch.FloatTensor([reward]).to(device)\n",
        "        done_tensor = torch.FloatTensor([done]).to(device)\n",
        "        \n",
        "        # ================================================================\n",
        "        # HANDS-ON EXERCISE 2: Complete the Actor-Critic update\n",
        "        # ================================================================\n",
        "        \n",
        "        # Step 1: Compute state values\n",
        "        # Hint: value = self.critic(state_tensor)\n",
        "        value = ### YOUR CODE HERE ###\n",
        "        \n",
        "        # Step 2: Compute next state value (with no grad)\n",
        "        with torch.no_grad():\n",
        "            # Hint: next_value = self.critic(next_state_tensor)\n",
        "            next_value = ### YOUR CODE HERE ###\n",
        "        \n",
        "        # Step 3: Compute TD error (advantage)\n",
        "        # Hint: td_error = reward + gamma * next_value * (1 - done) - value\n",
        "        td_error = ### YOUR CODE HERE ###\n",
        "        \n",
        "        # Step 4: Update Critic\n",
        "        # Hint: critic_loss = td_error.pow(2)\n",
        "        critic_loss = ### YOUR CODE HERE ###\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward(retain_graph=True)\n",
        "        self.critic_optimizer.step()\n",
        "        \n",
        "        # Step 5: Update Actor\n",
        "        # Hint: actor_loss = -log_prob * td_error.detach()\n",
        "        actor_loss = ### YOUR CODE HERE ###\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        return actor_loss.item(), critic_loss.item()\n",
        "\n",
        "pretty_print(\"Actor-Critic Agent Initialized\",\n",
        "             \"Two networks: Actor (policy) + Critic (value)<br>\" +\n",
        "             \"Update strategy: TD error as advantage<br>\" +\n",
        "             \"Lower variance than REINFORCE\",\n",
        "             style='success')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 17: Train Actor-Critic Agent\n",
        "\n",
        "Purpose:\n",
        "  - Train Actor-Critic agent\n",
        "  - Compare with all previous algorithms\n",
        "\n",
        "Expected Results:\n",
        "  - Lower variance than REINFORCE\n",
        "  - Faster convergence than REINFORCE\n",
        "  - Competitive with DQN/DDQN\n",
        "\"\"\"\n",
        "\n",
        "def train_actor_critic(num_episodes=500, max_steps=500):\n",
        "    \"\"\"\n",
        "    Train Actor-Critic agent.\n",
        "    \n",
        "    Args:\n",
        "        num_episodes: Number of training episodes\n",
        "        max_steps: Maximum steps per episode\n",
        "    \n",
        "    Returns:\n",
        "        agent: Trained agent\n",
        "        episode_rewards: List of episode rewards\n",
        "        actor_losses: List of actor losses\n",
        "        critic_losses: List of critic losses\n",
        "    \"\"\"\n",
        "    agent = ActorCriticAgent(state_dim, action_dim)\n",
        "    env = gym.make('CartPole-v1')\n",
        "    \n",
        "    episode_rewards = []\n",
        "    actor_losses = []\n",
        "    critic_losses = []\n",
        "    \n",
        "    pretty_print(\"Training Actor-Critic\",\n",
        "                 f\"Episodes: {num_episodes}<br>\" +\n",
        "                 f\"Expected: Lower variance, faster convergence\",\n",
        "                 style='info')\n",
        "    \n",
        "    for episode in tqdm(range(num_episodes), desc=\"Training Actor-Critic\"):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_actor_losses = []\n",
        "        episode_critic_losses = []\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            action, log_prob = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # Update both networks\n",
        "            actor_loss, critic_loss = agent.update(state, action, reward, \n",
        "                                                   next_state, done, log_prob)\n",
        "            \n",
        "            episode_actor_losses.append(actor_loss)\n",
        "            episode_critic_losses.append(critic_loss)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        actor_losses.append(np.mean(episode_actor_losses))\n",
        "        critic_losses.append(np.mean(episode_critic_losses))\n",
        "        \n",
        "        if (episode + 1) % 50 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-50:])\n",
        "            print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}\")\n",
        "    \n",
        "    env.close()\n",
        "    return agent, episode_rewards, actor_losses, critic_losses\n",
        "\n",
        "# Train Actor-Critic\n",
        "ac_agent, ac_rewards, ac_actor_losses, ac_critic_losses = train_actor_critic(num_episodes=500)\n",
        "\n",
        "pretty_print(\"Actor-Critic Training Complete\",\n",
        "             f\"Final 100-episode average: {np.mean(ac_rewards[-100:]):.2f}<br>\" +\n",
        "             f\"Best episode reward: {max(ac_rewards):.0f}<br>\" +\n",
        "             f\"Notice smoother learning than REINFORCE\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 18: Visualize Actor-Critic Results\n",
        "\n",
        "Purpose:\n",
        "  - Plot Actor-Critic learning curves\n",
        "  - Show both actor and critic losses\n",
        "\"\"\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Rewards\n",
        "window = 10\n",
        "ac_ma = [np.mean(ac_rewards[max(0, i-window):i+1]) for i in range(len(ac_rewards))]\n",
        "ax1.plot(ac_rewards, alpha=0.3, color='gray', label='Episode reward')\n",
        "ax1.plot(ac_ma, color='#17a2b8', linewidth=2, label=f'{window}-episode average')\n",
        "ax1.axhline(y=475, color='green', linestyle='--', linewidth=2, label='Solved')\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Reward')\n",
        "ax1.set_title('Actor-Critic - Episode Rewards')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Losses\n",
        "ax2.plot(ac_actor_losses, alpha=0.7, label='Actor Loss', color='#e74c3c')\n",
        "ax2.plot(ac_critic_losses, alpha=0.7, label='Critic Loss', color='#3498db')\n",
        "ax2.set_xlabel('Episode')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Actor-Critic - Training Losses')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<div style=\"border-left: 4px solid #17a2b8; padding-left: 12px; margin: 20px 0;\">\n",
        "  <h2 style=\"color: #17a2b8; margin: 0; font-size: 18px;\">Section 6: Comprehensive Comparison</h2>\n",
        "</div>\n",
        "\n",
        "Now let's compare all four algorithms we've implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 19: Compare All Algorithms\n",
        "\n",
        "Purpose:\n",
        "  - Plot learning curves for all algorithms\n",
        "  - Analyze strengths and weaknesses\n",
        "\"\"\"\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "window = 20\n",
        "\n",
        "# DQN\n",
        "dqn_ma = [np.mean(dqn_rewards[max(0, i-window):i+1]) for i in range(len(dqn_rewards))]\n",
        "plt.plot(dqn_ma, label='DQN', color='#3498db', linewidth=2)\n",
        "\n",
        "# Double DQN\n",
        "ddqn_ma = [np.mean(ddqn_rewards[max(0, i-window):i+1]) for i in range(len(ddqn_rewards))]\n",
        "plt.plot(ddqn_ma, label='Double DQN', color='#e74c3c', linewidth=2)\n",
        "\n",
        "# REINFORCE\n",
        "reinforce_ma = [np.mean(reinforce_rewards[max(0, i-window):i+1]) for i in range(len(reinforce_rewards))]\n",
        "plt.plot(reinforce_ma, label='REINFORCE', color='#9b59b6', linewidth=2, alpha=0.7)\n",
        "\n",
        "# Actor-Critic\n",
        "ac_ma = [np.mean(ac_rewards[max(0, i-window):i+1]) for i in range(len(ac_rewards))]\n",
        "plt.plot(ac_ma, label='Actor-Critic', color='#2ecc71', linewidth=2)\n",
        "\n",
        "plt.axhline(y=475, color='gray', linestyle='--', linewidth=2, label='Solved threshold')\n",
        "plt.xlabel('Episode', fontsize=12)\n",
        "plt.ylabel(f'{window}-Episode Moving Average Reward', fontsize=12)\n",
        "plt.title('Deep RL Algorithm Comparison on CartPole-v1', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "def get_stats(rewards, name):\n",
        "    final_avg = np.mean(rewards[-100:])\n",
        "    best = max(rewards)\n",
        "    # Find when solved (average > 475 for 100 episodes)\n",
        "    solved_at = None\n",
        "    for i in range(100, len(rewards)):\n",
        "        if np.mean(rewards[i-100:i]) >= 475:\n",
        "            solved_at = i\n",
        "            break\n",
        "    return name, final_avg, best, solved_at\n",
        "\n",
        "stats = [\n",
        "    get_stats(dqn_rewards, 'DQN'),\n",
        "    get_stats(ddqn_rewards, 'Double DQN'),\n",
        "    get_stats(reinforce_rewards, 'REINFORCE'),\n",
        "    get_stats(ac_rewards, 'Actor-Critic')\n",
        "]\n",
        "\n",
        "pretty_print(\"Algorithm Comparison Summary\",\n",
        "             \"<table style='width: 100%; border-collapse: collapse;'>\" +\n",
        "             \"<tr style='background: #f8f9fa;'>\" +\n",
        "             \"<th style='padding: 8px; text-align: left;'>Algorithm</th>\" +\n",
        "             \"<th style='padding: 8px; text-align: center;'>Final Avg</th>\" +\n",
        "             \"<th style='padding: 8px; text-align: center;'>Best</th>\" +\n",
        "             \"<th style='padding: 8px; text-align: center;'>Solved At</th>\" +\n",
        "             \"</tr>\" +\n",
        "             \"\".join([\n",
        "                 f\"<tr>\" +\n",
        "                 f\"<td style='padding: 8px;'>{name}</td>\" +\n",
        "                 f\"<td style='padding: 8px; text-align: center;'>{final:.1f}</td>\" +\n",
        "                 f\"<td style='padding: 8px; text-align: center;'>{best:.0f}</td>\" +\n",
        "                 f\"<td style='padding: 8px; text-align: center;'>{solved if solved else 'N/A'}</td>\" +\n",
        "                 f\"</tr>\"\n",
        "                 for name, final, best, solved in stats\n",
        "             ]) +\n",
        "             \"</table>\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Cell 20: Test Trained Agents\n",
        "\n",
        "Purpose:\n",
        "  - Visualize trained agent performance\n",
        "  - Compare with initial random agent\n",
        "\"\"\"\n",
        "\n",
        "def test_agent(agent, agent_type='dqn', num_episodes=5, render=False):\n",
        "    \"\"\"\n",
        "    Test trained agent.\n",
        "    \n",
        "    Args:\n",
        "        agent: Trained agent\n",
        "        agent_type: Type of agent ('dqn', 'reinforce', 'actor_critic')\n",
        "        num_episodes: Number of test episodes\n",
        "        render: Whether to render\n",
        "    \n",
        "    Returns:\n",
        "        List of episode rewards\n",
        "    \"\"\"\n",
        "    env = gym.make('CartPole-v1')\n",
        "    rewards = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            if render:\n",
        "                try:\n",
        "                    env.render()\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            # Select action (no exploration)\n",
        "            if agent_type in ['dqn', 'ddqn']:\n",
        "                action = agent.select_action(state, train=False)\n",
        "            elif agent_type == 'reinforce':\n",
        "                action, _ = agent.policy.select_action(state)\n",
        "            elif agent_type == 'actor_critic':\n",
        "                action, _ = agent.select_action(state)\n",
        "            \n",
        "            state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "        \n",
        "        rewards.append(episode_reward)\n",
        "        print(f\"Test Episode {episode + 1}: Reward = {episode_reward}\")\n",
        "    \n",
        "    env.close()\n",
        "    return rewards\n",
        "\n",
        "# Test best agent (choose one)\n",
        "print(\"Testing Actor-Critic agent:\")\n",
        "test_rewards = test_agent(ac_agent, agent_type='actor_critic', num_episodes=5)\n",
        "\n",
        "pretty_print(\"Trained Agent Performance\",\n",
        "             f\"Average test reward: {np.mean(test_rewards):.2f}<br>\" +\n",
        "             f\"Compare with random agent (~20-40 reward)<br>\" +\n",
        "             f\"Trained agent achieves near-optimal performance!\",\n",
        "             style='result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Key Findings</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>1. Function Approximation:</strong> Neural networks enable RL in high-dimensional continuous state spaces where tabular methods fail.</p>\n",
        "        \n",
        "        <p><strong>2. DQN Innovations:</strong> Experience replay and target networks are crucial for stable deep RL training. Without them, neural networks often diverge.</p>\n",
        "        \n",
        "        <p><strong>3. Overestimation Bias:</strong> Double DQN's simple modification (decoupling action selection and evaluation) significantly improves stability and performance.</p>\n",
        "        \n",
        "        <p><strong>4. Policy Gradients:</strong> REINFORCE demonstrates direct policy optimization but suffers from high variance. Suitable for continuous action spaces where value-based methods struggle.</p>\n",
        "        \n",
        "        <p><strong>5. Actor-Critic Synthesis:</strong> Combining policy gradients with value functions (Actor-Critic) achieves lower variance and more stable learning than pure policy gradient methods.</p>\n",
        "        \n",
        "        <p><strong>6. Algorithm Selection:</strong> Value-based (DQN/DDQN) excel in discrete actions with sample efficiency. Policy-based (REINFORCE/Actor-Critic) handle continuous actions and stochastic policies. Actor-Critic offers best overall balance.</p>\n",
        "        \n",
        "        <p><strong>7. Modern Extensions:</strong> These foundational algorithms form the basis for state-of-the-art methods: PPO, A3C, SAC, TD3, which add further stability and performance improvements.</p>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>Why is experience replay essential for DQN but not needed for Actor-Critic? What makes off-policy vs on-policy learning different?</li>\n",
        "        <li>How would you adapt DQN to handle continuous action spaces? What modifications would be required?</li>\n",
        "        <li>What causes the high variance in REINFORCE? How does the Actor-Critic baseline reduce this variance?</li>\n",
        "        <li>Why does Double DQN use the main network for action selection and target network for evaluation, rather than the reverse?</li>\n",
        "        <li>How do these algorithms scale to more complex environments like Atari games or robotic control?</li>\n",
        "        <li>What are the trade-offs between sample efficiency (DQN) and stable convergence (Actor-Critic)?</li>\n",
        "        <li>How would you implement multi-step returns (n-step) or eligibility traces in these algorithms?</li>\n",
        "        <li>What role does the learning rate ratio (actor vs critic) play in Actor-Critic stability?</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #e8f5e9; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #4caf50;\">\n",
        "    <h3 style=\"color: #2e7d32; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Extensions and Next Steps</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>Advanced Algorithms to Explore:</strong></p>\n",
        "        <ul style=\"margin: 8px 0; padding-left: 20px;\">\n",
        "            <li><strong>PPO (Proximal Policy Optimization):</strong> Improves upon Actor-Critic with clipped objective for more stable updates</li>\n",
        "            <li><strong>A3C (Asynchronous Advantage Actor-Critic):</strong> Parallelizes Actor-Critic across multiple environments</li>\n",
        "            <li><strong>SAC (Soft Actor-Critic):</strong> Adds entropy regularization for better exploration in continuous control</li>\n",
        "            <li><strong>TD3 (Twin Delayed DDPG):</strong> Extends DDQN ideas to continuous actions with multiple critics</li>\n",
        "            <li><strong>Rainbow DQN:</strong> Combines multiple DQN improvements (prioritized replay, dueling networks, noisy nets, etc.)</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 11: Deep Reinforcement Learning</p>\n",
        "    <p style=\"margin: 5px 0 0 0; font-size: 11px; opacity: 0.9;\">Congratulations on mastering the foundations of modern deep RL!</p>\n",
        "</div>"
      ]
    }
  ]
}
