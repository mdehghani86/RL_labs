{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/RL_labs/blob/master/enhanced_mab_lab_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eh-vkdf-Er-"
      },
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 2: Multi-Armed Bandits\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">¬© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapter 2 | Intermediate Level | 120 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        The multi-armed bandit problem models decision-making under uncertainty. An agent repeatedly chooses among k actions,\n",
        "        receiving numerical rewards from stationary probability distributions. The challenge is balancing exploration\n",
        "        (trying different actions to find the best) with exploitation (choosing the current best action).\n",
        "        This lab reproduces key results from <a href=\"http://incompleteideas.net/book/the-book-2nd.html\" style=\"color: #17a2b8;\">Sutton & Barto (2018)</a>, Chapter 2.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Understand the exploration-exploitation tradeoff</li>\n",
        "        <li>Implement the 10-armed testbed</li>\n",
        "        <li>Compare Œµ-greedy strategies</li>\n",
        "        <li>Analyze optimistic initial values</li>\n",
        "        <li>Reproduce Figures 2.1, 2.2, and 2.3</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Key Concepts</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">q*(a)</code> = true action value</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Qt(a)</code> = estimated value at time t</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Œµ-greedy</code> = exploration strategy</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Œ±</code> = step-size parameter</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V1998jP-EsD"
      },
      "source": [
        "## Configuration and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm9L3ssd-EsE"
      },
      "source": [
        "# ============================================\n",
        "# CELL 1: Environment Setup and Configuration\n",
        "# Purpose: Import libraries and set visualization parameters\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, List\n",
        "import warnings\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurable color scheme - modify these to change plot colors\n",
        "COLORS = {\n",
        "    'greedy': '#008000',      # Green for Œµ=0\n",
        "    'epsilon_01': '#FF0000',  # Red for Œµ=0.01\n",
        "    'epsilon_1': '#0000FF',   # Blue for Œµ=0.1\n",
        "    'optimistic': '#00BFFF',  # Cyan for optimistic\n",
        "    'realistic': '#808080',   # Gray for realistic\n",
        "    'violin': '#7f7f7f'       # Gray for violin plots\n",
        "}\n",
        "\n",
        "# Standard parameters from Sutton & Barto\n",
        "K = 10          # Number of arms\n",
        "STEPS = 1000    # Time steps per run\n",
        "RUNS = 2000     # Number of independent runs\n",
        "\n",
        "# Configure matplotlib for better plots\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.labelsize'] = 11\n",
        "plt.rcParams['axes.titlesize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 10\n",
        "\n",
        "print(\"‚úÖ Environment setup complete!\")\n",
        "print(f\"üìä Default configuration: {K} arms, {STEPS} steps, {RUNS} runs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_x6cxuS-EsG"
      },
      "source": [
        "# ============================================\n",
        "# CELL 2: Custom Pretty Print Function\n",
        "# Purpose: Create beautiful output displays\n",
        "# ============================================\n",
        "\n",
        "def pretty_print(title, content, color='#17a2b8'):\n",
        "    \"\"\"Display formatted output in a gradient box\"\"\"\n",
        "    # Create gradient from color to darker version\n",
        "    darker_color = '#0e5a63'\n",
        "\n",
        "    html = f'''\n",
        "    <div style=\"border-radius: 5px;\n",
        "                margin: 10px 0;\n",
        "                width: 30cm;\n",
        "                max-width: 30cm;\n",
        "                box-sizing: border-box;\n",
        "                box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
        "        <div style=\"background: linear-gradient(90deg, {color} 0%, {darker_color} 100%);\n",
        "                    padding: 10px 15px; border-radius: 5px 5px 0 0;\">\n",
        "            <strong style=\"color: white; font-size: 14px;\">{title}</strong>\n",
        "        </div>\n",
        "        <div style=\"background: #f8f9fa; padding: 10px 15px; border-radius: 0 0 5px 5px;\n",
        "                    border-left: 3px solid {color};\">\n",
        "            <div style=\"color: rgba(73,80,87,0.8); font-size: 12px;\">{content}</div>\n",
        "        </div>\n",
        "    </div>\n",
        "    '''\n",
        "    display(HTML(html))\n",
        "\n",
        "# Test our pretty print function\n",
        "pretty_print(\"Welcome to RL Lab Series!\",\n",
        "             \"This function will help us display information beautifully throughout the lab.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD__yEkZ-EsG"
      },
      "source": [
        "## Interactive Exploration: Understanding Multi-Armed Bandits\n",
        "\n",
        "Before diving into algorithms, let's build intuition about the multi-armed bandit problem through interactive exploration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PRHSge0-EsG"
      },
      "source": [
        "# ============================================\n",
        "# CELL 3: Interactive Bandit Exploration - Basic Setup\n",
        "# Purpose: Let students interact with a simple bandit\n",
        "# ============================================\n",
        "\n",
        "class InteractiveBandit:\n",
        "    \"\"\"Simple bandit for interactive exploration\"\"\"\n",
        "\n",
        "    def __init__(self, k_arms=5, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.k = k_arms\n",
        "        # True action values (hidden from student initially)\n",
        "        self.q_true = np.random.randn(k_arms)\n",
        "        # Keep track of what student has tried\n",
        "        self.action_counts = np.zeros(k_arms)\n",
        "        self.total_rewards = np.zeros(k_arms)\n",
        "        self.history = []\n",
        "\n",
        "    def pull_arm(self, action):\n",
        "        \"\"\"Pull an arm and get reward\"\"\"\n",
        "        if action < 0 or action >= self.k:\n",
        "            return None\n",
        "\n",
        "        # Get noisy reward: R ~ N(q*(a), 1)\n",
        "        reward = self.q_true[action] + np.random.randn()\n",
        "\n",
        "        # Update statistics\n",
        "        self.action_counts[action] += 1\n",
        "        self.total_rewards[action] += reward\n",
        "        self.history.append((action, reward))\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def get_estimates(self):\n",
        "        \"\"\"Get current action value estimates\"\"\"\n",
        "        estimates = np.zeros(self.k)\n",
        "        for i in range(self.k):\n",
        "            if self.action_counts[i] > 0:\n",
        "                estimates[i] = self.total_rewards[i] / self.action_counts[i]\n",
        "        return estimates\n",
        "\n",
        "    def show_status(self):\n",
        "        \"\"\"Display current status\"\"\"\n",
        "        estimates = self.get_estimates()\n",
        "\n",
        "        status_html = \"<div style='font-family: monospace;'>\"\n",
        "        status_html += \"<table style='border-collapse: collapse; margin: 10px 0;'>\"\n",
        "        status_html += \"<tr><th style='padding: 5px; border: 1px solid #ccc;'>Arm</th>\"\n",
        "        status_html += \"<th style='padding: 5px; border: 1px solid #ccc;'>Times Pulled</th>\"\n",
        "        status_html += \"<th style='padding: 5px; border: 1px solid #ccc;'>Avg Reward</th></tr>\"\n",
        "\n",
        "        for i in range(self.k):\n",
        "            status_html += f\"<tr><td style='padding: 5px; border: 1px solid #ccc; text-align: center;'>{i+1}</td>\"\n",
        "            status_html += f\"<td style='padding: 5px; border: 1px solid #ccc; text-align: center;'>{int(self.action_counts[i])}</td>\"\n",
        "            if self.action_counts[i] > 0:\n",
        "                status_html += f\"<td style='padding: 5px; border: 1px solid #ccc; text-align: center;'>{estimates[i]:.3f}</td></tr>\"\n",
        "            else:\n",
        "                status_html += \"<td style='padding: 5px; border: 1px solid #ccc; text-align: center;'>---</td></tr>\"\n",
        "\n",
        "        status_html += \"</table></div>\"\n",
        "\n",
        "        total_pulls = int(np.sum(self.action_counts))\n",
        "        if total_pulls > 0:\n",
        "            best_arm = np.argmax(estimates) + 1\n",
        "            avg_reward = np.sum(self.total_rewards) / total_pulls\n",
        "            status_html += f\"<p><strong>Total pulls:</strong> {total_pulls} | \"\n",
        "            status_html += f\"<strong>Current best arm:</strong> {best_arm} | \"\n",
        "            status_html += f\"<strong>Overall avg reward:</strong> {avg_reward:.3f}</p>\"\n",
        "\n",
        "        display(HTML(status_html))\n",
        "\n",
        "    def reveal_truth(self):\n",
        "        \"\"\"Reveal the true action values\"\"\"\n",
        "        optimal_arm = np.argmax(self.q_true) + 1\n",
        "        pretty_print(\"üéØ TRUE ACTION VALUES REVEALED!\",\n",
        "                    f\"True values: {[f'{q:.3f}' for q in self.q_true]}<br>\" +\n",
        "                    f\"Optimal arm: {optimal_arm} (value: {self.q_true[optimal_arm-1]:.3f})\")\n",
        "\n",
        "# Create a bandit for exploration\n",
        "bandit = InteractiveBandit(k_arms=5, seed=42)\n",
        "\n",
        "pretty_print(\"üé∞ Interactive Bandit Ready!\",\n",
        "             \"You have a 5-armed bandit. Each arm gives rewards from a different distribution.<br>\" +\n",
        "             \"Your goal: figure out which arm is best by pulling arms and observing rewards.<br>\" +\n",
        "             \"<strong>Think:</strong> How will you balance trying new arms vs. sticking with good ones?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz3fpJY7-EsH"
      },
      "source": [
        "# ============================================\n",
        "# CELL 4: Interactive Bandit - Manual Pulling\n",
        "# Purpose: Let students manually pull arms\n",
        "# ============================================\n",
        "\n",
        "@interact_manual(arm=widgets.IntSlider(min=1, max=5, value=1, description='Arm to pull:'))\n",
        "def pull_bandit_arm(arm):\n",
        "    \"\"\"Pull a bandit arm and see the result\"\"\"\n",
        "    reward = bandit.pull_arm(arm - 1)  # Convert to 0-indexed\n",
        "\n",
        "    if reward is not None:\n",
        "        pretty_print(f\"üé≤ Pulled Arm {arm}\",\n",
        "                    f\"Reward received: {reward:.3f}\",\n",
        "                    color='#28a745' if reward > 0 else '#dc3545')\n",
        "        bandit.show_status()\n",
        "    else:\n",
        "        pretty_print(\"‚ùå Invalid Arm\", \"Please select an arm between 1 and 5\")\n",
        "\n",
        "print(\"üëÜ Use the slider to select an arm, then click 'Run Interact' to pull it!\")\n",
        "print(\"üí° Try different strategies: random exploration, stick with best, systematic testing...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGVQFQKd-EsH"
      },
      "source": [
        "# ============================================\n",
        "# CELL 5: Interactive Strategy Comparison\n",
        "# Purpose: Let students compare different exploration strategies\n",
        "# ============================================\n",
        "\n",
        "def simulate_strategy(strategy_name, n_steps=50):\n",
        "    \"\"\"Simulate different bandit strategies\"\"\"\n",
        "    # Create fresh bandit for each strategy\n",
        "    test_bandit = InteractiveBandit(k_arms=5, seed=42)\n",
        "\n",
        "    rewards = []\n",
        "    actions = []\n",
        "\n",
        "    for step in range(n_steps):\n",
        "        if strategy_name == \"Random\":\n",
        "            # Pure random exploration\n",
        "            action = np.random.randint(0, 5)\n",
        "\n",
        "        elif strategy_name == \"Greedy\":\n",
        "            # Always pick current best (pure exploitation)\n",
        "            estimates = test_bandit.get_estimates()\n",
        "            if step < 5:  # Try each arm once first\n",
        "                action = step\n",
        "            else:\n",
        "                action = np.argmax(estimates)\n",
        "\n",
        "        elif strategy_name == \"Œµ-greedy (Œµ=0.1)\":\n",
        "            # 10% random, 90% greedy\n",
        "            estimates = test_bandit.get_estimates()\n",
        "            if step < 5:  # Try each arm once first\n",
        "                action = step\n",
        "            elif np.random.random() < 0.1:\n",
        "                action = np.random.randint(0, 5)\n",
        "            else:\n",
        "                action = np.argmax(estimates)\n",
        "\n",
        "        reward = test_bandit.pull_arm(action)\n",
        "        rewards.append(reward)\n",
        "        actions.append(action)\n",
        "\n",
        "    return rewards, actions, test_bandit\n",
        "\n",
        "@interact(strategy=widgets.Dropdown(\n",
        "    options=[\"Random\", \"Greedy\", \"Œµ-greedy (Œµ=0.1)\"],\n",
        "    value=\"Random\",\n",
        "    description='Strategy:'\n",
        "))\n",
        "def compare_strategies(strategy):\n",
        "    \"\"\"Compare different bandit strategies\"\"\"\n",
        "    rewards, actions, test_bandit = simulate_strategy(strategy, n_steps=100)\n",
        "\n",
        "    # Calculate metrics\n",
        "    total_reward = np.sum(rewards)\n",
        "    avg_reward = np.mean(rewards)\n",
        "    optimal_arm = np.argmax(test_bandit.q_true)\n",
        "    optimal_selections = np.sum(np.array(actions) == optimal_arm)\n",
        "    optimal_percentage = (optimal_selections / len(actions)) * 100\n",
        "\n",
        "    # Plot results\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Cumulative reward\n",
        "    ax1.plot(np.cumsum(rewards), linewidth=2)\n",
        "    ax1.set_xlabel('Steps')\n",
        "    ax1.set_ylabel('Cumulative Reward')\n",
        "    ax1.set_title(f'{strategy}: Cumulative Reward')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Action selection histogram\n",
        "    ax2.hist(actions, bins=np.arange(6)-0.5, alpha=0.7, edgecolor='black')\n",
        "    ax2.set_xlabel('Arm')\n",
        "    ax2.set_ylabel('Times Selected')\n",
        "    ax2.set_title(f'{strategy}: Arm Selection Frequency')\n",
        "    ax2.set_xticks(range(5))\n",
        "    ax2.set_xticklabels([f'Arm {i+1}' for i in range(5)])\n",
        "\n",
        "    # Highlight optimal arm\n",
        "    ax2.axvline(optimal_arm, color='red', linestyle='--', alpha=0.7, label=f'Optimal (Arm {optimal_arm+1})')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Display results\n",
        "    results_text = f\"Total Reward: {total_reward:.2f}<br>\"\n",
        "    results_text += f\"Average Reward: {avg_reward:.3f}<br>\"\n",
        "    results_text += f\"Optimal Arm Selected: {optimal_percentage:.1f}% of the time<br>\"\n",
        "    results_text += f\"Strategy: {strategy}\"\n",
        "\n",
        "    pretty_print(f\"üìà Results for {strategy}\", results_text)\n",
        "\n",
        "print(\"üîÑ Select different strategies to see how they perform!\")\n",
        "print(\"üéØ Notice the trade-off between exploration and exploitation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmyXC5cQ-EsI"
      },
      "source": [
        "# ============================================\n",
        "# CELL 6: Reveal the Truth and Reflect\n",
        "# Purpose: Show true values and discuss strategies\n",
        "# ============================================\n",
        "\n",
        "# Reveal the true action values\n",
        "bandit.reveal_truth()\n",
        "\n",
        "# Show final status of student's exploration\n",
        "pretty_print(\"üìä Your Exploration Results\", \"How well did you do compared to the optimal strategy?\")\n",
        "bandit.show_status()\n",
        "\n",
        "pretty_print(\"ü§î Reflection Questions\",\n",
        "             \"1. How did you decide which arms to try?<br>\" +\n",
        "             \"2. When did you stop exploring and start exploiting?<br>\" +\n",
        "             \"3. What would you do differently with more time?<br>\" +\n",
        "             \"4. How did the different automated strategies compare?\")\n",
        "\n",
        "pretty_print(\"üéØ Key Insight\",\n",
        "             \"This is the <strong>exploration-exploitation dilemma</strong>!<br>\" +\n",
        "             \"‚Ä¢ <strong>Explore</strong>: Try new actions to find better options<br>\" +\n",
        "             \"‚Ä¢ <strong>Exploit</strong>: Use current knowledge to maximize reward<br>\" +\n",
        "             \"The challenge is finding the right balance between these two.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdcv1v0a-EsJ"
      },
      "source": [
        "## Part 1: The 10-Armed Testbed (Figure 2.1)\n",
        "\n",
        "### Mathematical Foundation\n",
        "\n",
        "The **k-armed bandit problem** is a fundamental framework for studying sequential decision-making under uncertainty. Formally:\n",
        "\n",
        "- **Actions**: $A_t \\in \\{1, 2, ..., k\\}$ at time step $t$\n",
        "- **True Action Values**: $q_*(a) = \\mathbb{E}[R_t | A_t = a]$ for each action $a$\n",
        "- **Rewards**: $R_t \\sim \\mathcal{N}(q_*(A_t), 1)$ (Gaussian with unit variance)\n",
        "- **Goal**: Maximize cumulative reward $\\sum_{t=1}^T R_t$\n",
        "\n",
        "### Historical Context\n",
        "\n",
        "The multi-armed bandit problem originated from the \"one-armed bandit\" slot machines in casinos. The term was coined by Herbert Robbins in 1952, though the mathematical foundations trace back to Thompson (1933). It has become a cornerstone of:\n",
        "\n",
        "- **Online advertising** (ad selection)\n",
        "- **Clinical trials** (treatment allocation)\n",
        "- **Recommendation systems** (content selection)\n",
        "- **Resource allocation** (server load balancing)\n",
        "\n",
        "The **10-armed testbed** specifically uses $k=10$ actions with true values $q_*(a) \\sim \\mathcal{N}(0, 1)$, providing a standardized benchmark for algorithm comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWjBPeFD-EsJ"
      },
      "source": [
        "# ============================================\n",
        "# CELL 7: Create and Visualize the 10-Armed Testbed\n",
        "# Purpose: Generate Figure 2.1 showing reward distributions\n",
        "# Mathematical Note: q*(a) ~ N(0,1), R_t ~ N(q*(a), 1)\n",
        "# ============================================\n",
        "\n",
        "# Set seed for reproducible example (matching Sutton & Barto)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate true action values q*(a) ~ N(0, 1)\n",
        "# This represents the expected reward for each action\n",
        "q_true = np.random.randn(K)\n",
        "\n",
        "# Display the true values\n",
        "pretty_print(\"üéØ True Action Values Generated\",\n",
        "             f\"q*(a) for each arm: {[f'{q:.3f}' for q in q_true]}<br>\" +\n",
        "             f\"Optimal action: Arm {np.argmax(q_true) + 1} with value {np.max(q_true):.3f}\")\n",
        "\n",
        "# Create Figure 2.1: Reward distributions\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Generate reward distribution samples for visualization\n",
        "# Each action gives rewards R ~ N(q*(a), 1)\n",
        "n_samples = 10000\n",
        "rewards = np.zeros((n_samples, K))\n",
        "for i in range(K):\n",
        "    # Rewards are normally distributed around the true action value\n",
        "    rewards[:, i] = q_true[i] + np.random.randn(n_samples)\n",
        "\n",
        "# Create violin plots to show reward distributions\n",
        "parts = ax.violinplot(rewards, positions=range(1, K+1), widths=0.7,\n",
        "                      showmeans=False, showextrema=False)\n",
        "\n",
        "# Style the violins\n",
        "for pc in parts['bodies']:\n",
        "    pc.set_facecolor(COLORS['violin'])\n",
        "    pc.set_alpha(0.7)\n",
        "    pc.set_edgecolor('black')\n",
        "    pc.set_linewidth(0.5)\n",
        "\n",
        "# Add horizontal lines for true values q*(a)\n",
        "for i in range(K):\n",
        "    ax.hlines(q_true[i], i+0.7, i+1.3, colors='black', linewidth=1.5)\n",
        "    ax.text(i+1.4, q_true[i], f'$q_*({i+1})$', fontsize=10, va='center')\n",
        "\n",
        "# Formatting\n",
        "ax.set_xlabel('Action', fontsize=12)\n",
        "ax.set_ylabel('Reward\\ndistribution', fontsize=12)\n",
        "ax.set_xticks(range(1, K+1))\n",
        "ax.set_xlim(0.5, K+0.5)\n",
        "ax.set_ylim(-3, 3)\n",
        "ax.axhline(y=0, color='black', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.title('Figure 2.1: The 10-armed testbed', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "pretty_print(\"üìä Testbed Analysis\",\n",
        "             f\"Each violin shows the distribution of rewards for that action.<br>\" +\n",
        "             f\"The black lines show the true expected values q*(a).<br>\" +\n",
        "             f\"Notice how rewards are noisy - this creates the exploration challenge!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrEbwmzP-EsK"
      },
      "source": [
        "## Part 2: Œµ-Greedy Action Selection\n",
        "\n",
        "### Algorithm Theory\n",
        "\n",
        "The **Œµ-greedy** method is one of the simplest approaches to balance exploration and exploitation. It works by:\n",
        "\n",
        "1. **Exploitation** (probability $1-Œµ$): Choose the action with highest estimated value\n",
        "   $$A_t = \\arg\\max_a Q_t(a)$$\n",
        "\n",
        "2. **Exploration** (probability $Œµ$): Choose a random action uniformly\n",
        "   $$A_t \\sim \\text{Uniform}(\\{1, 2, ..., k\\})$$\n",
        "\n",
        "### Action Value Estimation\n",
        "\n",
        "We maintain estimates $Q_t(a)$ of the true action values $q_*(a)$ using the **sample average method**:\n",
        "\n",
        "$$Q_t(a) = \\frac{\\text{sum of rewards when action } a \\text{ taken}}{\\text{number of times action } a \\text{ taken}} = \\frac{\\sum_{i=1}^{t-1} R_i \\cdot \\mathbf{1}_{A_i = a}}{\\sum_{i=1}^{t-1} \\mathbf{1}_{A_i = a}}$$\n",
        "\n",
        "This can be computed incrementally as:\n",
        "$$Q_{n+1} = Q_n + \\frac{1}{n}[R_n - Q_n]$$\n",
        "\n",
        "where $n$ is the number of times action $a$ has been selected.\n",
        "\n",
        "### Theoretical Properties\n",
        "\n",
        "- **Convergence**: As $t \\to \\infty$, $Q_t(a) \\to q_*(a)$ with probability 1 (by Law of Large Numbers)\n",
        "- **Exploration guarantee**: Every action has probability $Œµ/k$ of being selected\n",
        "- **Regret**: The difference between optimal and achieved performance decreases over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JekZzRGL-EsK"
      },
      "source": [
        "# ============================================\n",
        "# CELL 8: Core Œµ-Greedy Algorithm Implementation\n",
        "# Purpose: Define action selection and value update methods with detailed explanations\n",
        "# ============================================\n",
        "\n",
        "def create_bandit() -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Create a new 10-armed bandit problem\n",
        "\n",
        "    Returns:\n",
        "        q_true: Array of true action values q*(a) ~ N(0,1)\n",
        "    \"\"\"\n",
        "    return np.random.randn(K)\n",
        "\n",
        "def get_reward(action: int, q_true: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Get reward for an action following the bandit model\n",
        "\n",
        "    Mathematical model: R ~ N(q*(a), 1)\n",
        "    This means rewards are normally distributed around the true action value\n",
        "    with unit variance, creating noise that makes learning challenging.\n",
        "\n",
        "    Args:\n",
        "        action: Selected action index (0 to K-1)\n",
        "        q_true: True action values\n",
        "\n",
        "    Returns:\n",
        "        reward: Noisy reward sampled from N(q*(action), 1)\n",
        "    \"\"\"\n",
        "    return q_true[action] + np.random.randn()\n",
        "\n",
        "def epsilon_greedy(Q: np.ndarray, epsilon: float) -> int:\n",
        "    \"\"\"\n",
        "    Œµ-greedy action selection - the heart of exploration vs exploitation\n",
        "\n",
        "    This function implements the fundamental trade-off in reinforcement learning:\n",
        "    - With probability Œµ: EXPLORE (try random actions to learn more)\n",
        "    - With probability 1-Œµ: EXPLOIT (use current knowledge optimally)\n",
        "\n",
        "    Mathematical formulation:\n",
        "    A_t = {\n",
        "        argmax_a Q_t(a)           with probability 1-Œµ  (greedy)\n",
        "        random action             with probability Œµ     (exploration)\n",
        "    }\n",
        "\n",
        "    Args:\n",
        "        Q: Current action value estimates Q_t(a)\n",
        "        epsilon: Exploration probability Œµ ‚àà [0,1]\n",
        "\n",
        "    Returns:\n",
        "        Selected action index\n",
        "    \"\"\"\n",
        "    if np.random.random() < epsilon:\n",
        "        # EXPLORATION: Choose random action\n",
        "        # This helps us learn about actions we haven't tried much\n",
        "        return np.random.randint(K)\n",
        "    else:\n",
        "        # EXPLOITATION: Choose greedy action (break ties randomly)\n",
        "        # Find the action(s) with maximum estimated value\n",
        "        max_Q = np.max(Q)\n",
        "        # Handle ties by randomly selecting among equally good actions\n",
        "        return np.random.choice(np.where(Q == max_Q)[0])\n",
        "\n",
        "def update_estimates(Q: np.ndarray, N: np.ndarray, action: int,\n",
        "                    reward: float, alpha: float = None) -> None:\n",
        "    \"\"\"\n",
        "    Update action value estimates using incremental sample average\n",
        "\n",
        "    This implements the fundamental learning rule in bandits:\n",
        "    NewEstimate = OldEstimate + StepSize √ó [Target - OldEstimate]\n",
        "\n",
        "    Two update rules:\n",
        "    1. Sample Average (Œ± = 1/n): Q_n+1 = Q_n + (1/n)[R_n - Q_n]\n",
        "       - Gives equal weight to all past rewards\n",
        "       - Converges to true mean with probability 1\n",
        "\n",
        "    2. Constant Step Size (Œ± = constant): Q_n+1 = Q_n + Œ±[R_n - Q_n]\n",
        "       - Recent rewards have more influence\n",
        "       - Better for non-stationary problems\n",
        "\n",
        "    Args:\n",
        "        Q: Action value estimates (modified in place)\n",
        "        N: Action selection counts\n",
        "        action: Action that was selected\n",
        "        reward: Observed reward R_t\n",
        "        alpha: Step size parameter (None for sample average)\n",
        "    \"\"\"\n",
        "    # Increment the count for this action\n",
        "    N[action] += 1\n",
        "\n",
        "    if alpha is None:\n",
        "        # Sample average update: step size = 1/n\n",
        "        # This gives equal weight to all past observations\n",
        "        step_size = 1.0 / N[action]\n",
        "        Q[action] += step_size * (reward - Q[action])\n",
        "    else:\n",
        "        # Constant step size update\n",
        "        # This gives more weight to recent observations\n",
        "        Q[action] += alpha * (reward - Q[action])\n",
        "\n",
        "# Display the key algorithmic components\n",
        "pretty_print(\"üß† Œµ-Greedy Algorithm Components\",\n",
        "             \"<strong>Action Selection:</strong> Œµ-greedy with exploration probability Œµ<br>\" +\n",
        "             \"<strong>Value Estimation:</strong> Incremental sample average Q_n+1 = Q_n + (1/n)[R_n - Q_n]<br>\" +\n",
        "             \"<strong>Key Insight:</strong> Balance between exploration (learning) and exploitation (earning)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMMXSGrd-EsK"
      },
      "source": [
        "## Part 3: Comparing Œµ-greedy Methods (Figure 2.2)\n",
        "\n",
        "### Experimental Design\n",
        "\n",
        "We compare three Œµ-greedy variants to understand the exploration-exploitation trade-off:\n",
        "\n",
        "1. **Greedy (Œµ = 0)**: Pure exploitation, no exploration\n",
        "2. **Œµ-greedy (Œµ = 0.01)**: 1% exploration, 99% exploitation  \n",
        "3. **Œµ-greedy (Œµ = 0.1)**: 10% exploration, 90% exploitation\n",
        "\n",
        "### Expected Behaviors\n",
        "\n",
        "- **Greedy**: Fast initial learning but often gets stuck on suboptimal actions\n",
        "- **Small Œµ**: Good balance, slow but steady improvement\n",
        "- **Large Œµ**: More exploration, potentially higher long-term performance but more variability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLafxSUa-EsL"
      },
      "source": [
        "# ============================================\n",
        "# CELL 9: Run Œµ-greedy Experiments\n",
        "# Purpose: Generate data for Figure 2.2 comparison\n",
        "# ============================================\n",
        "\n",
        "def run_epsilon_greedy_experiment(epsilon: float, runs: int = 2000,\n",
        "                                 steps: int = 1000) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run Œµ-greedy bandit experiment with multiple independent runs\n",
        "\n",
        "    This function implements the standard experimental protocol:\n",
        "    1. For each run, create a new bandit problem (different q* values)\n",
        "    2. Run the Œµ-greedy algorithm for specified steps\n",
        "    3. Track rewards and optimal action selections\n",
        "    4. Average across all runs for statistical significance\n",
        "\n",
        "    Args:\n",
        "        epsilon: Exploration probability\n",
        "        runs: Number of independent experiments\n",
        "        steps: Number of time steps per run\n",
        "\n",
        "    Returns:\n",
        "        (average_rewards, optimal_action_percentage)\n",
        "    \"\"\"\n",
        "    # Initialize storage for all runs\n",
        "    all_rewards = np.zeros((runs, steps))\n",
        "    all_optimal = np.zeros((runs, steps))\n",
        "\n",
        "    # Run multiple independent experiments\n",
        "    for run in range(runs):\n",
        "        # Create a new bandit problem for this run\n",
        "        q_true = create_bandit()\n",
        "        optimal_action = np.argmax(q_true)  # The truly best action\n",
        "\n",
        "        # Initialize the agent's estimates and counts\n",
        "        Q = np.zeros(K)  # Action value estimates Q_t(a)\n",
        "        N = np.zeros(K)  # Number of times each action selected\n",
        "\n",
        "        # Run the Œµ-greedy algorithm for specified steps\n",
        "        for step in range(steps):\n",
        "            # Select action using Œµ-greedy policy\n",
        "            action = epsilon_greedy(Q, epsilon)\n",
        "\n",
        "            # Get reward from environment\n",
        "            reward = get_reward(action, q_true)\n",
        "\n",
        "            # Update action value estimates\n",
        "            update_estimates(Q, N, action, reward)\n",
        "\n",
        "            # Record results for analysis\n",
        "            all_rewards[run, step] = reward\n",
        "            all_optimal[run, step] = (action == optimal_action)\n",
        "\n",
        "    # Calculate averages across all runs\n",
        "    avg_rewards = np.mean(all_rewards, axis=0)\n",
        "    pct_optimal = np.mean(all_optimal, axis=0) * 100\n",
        "\n",
        "    return avg_rewards, pct_optimal\n",
        "\n",
        "# Run experiments for different epsilon values\n",
        "pretty_print(\"üî¨ Running Œµ-greedy Experiments\",\n",
        "             f\"Testing Œµ = 0, 0.01, 0.1 with {RUNS} runs of {STEPS} steps each.<br>\" +\n",
        "             \"This may take a moment...\")\n",
        "\n",
        "results = {}\n",
        "epsilon_values = [(0, 'greedy'), (0.01, 'epsilon_01'), (0.1, 'epsilon_1')]\n",
        "\n",
        "for eps, name in epsilon_values:\n",
        "    print(f\"  Running Œµ = {eps}...\")\n",
        "    avg_reward, pct_optimal = run_epsilon_greedy_experiment(eps, RUNS, STEPS)\n",
        "    results[eps] = (avg_reward, pct_optimal, name)\n",
        "\n",
        "    # Show some statistics\n",
        "    final_avg_reward = avg_reward[-1]\n",
        "    final_optimal_pct = pct_optimal[-1]\n",
        "    pretty_print(f\"üìä Results for Œµ = {eps}\",\n",
        "                f\"Final average reward: {final_avg_reward:.3f}<br>\" +\n",
        "                f\"Final optimal action %: {final_optimal_pct:.1f}%\")\n",
        "\n",
        "pretty_print(\"‚úÖ Experiments Complete!\", \"Ready to visualize the results in Figure 2.2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvnGwaXM-EsL"
      },
      "source": [
        "# ============================================\n",
        "# CELL 10: Plot Figure 2.2 - Œµ-greedy Comparison\n",
        "# Purpose: Reproduce Figure 2.2 from Sutton & Barto\n",
        "# ============================================\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n",
        "\n",
        "# Plot average rewards (top panel)\n",
        "for eps in [0.1, 0.01, 0]:  # Order for correct layering\n",
        "    avg_reward, _, name = results[eps]\n",
        "    label = f'Œµ = {eps} (greedy)' if eps == 0 else f'Œµ = {eps}'\n",
        "    ax1.plot(avg_reward, color=COLORS[name], label=label, linewidth=1.5)\n",
        "\n",
        "ax1.set_ylabel('Average\\nreward', fontsize=11)\n",
        "ax1.set_xlim(0, 1000)\n",
        "ax1.set_ylim(0, 1.5)\n",
        "ax1.legend(loc='lower right')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot optimal action percentage (bottom panel)\n",
        "for eps in [0.1, 0.01, 0]:  # Order for correct layering\n",
        "    _, pct_optimal, name = results[eps]\n",
        "    label = f'Œµ = {eps} (greedy)' if eps == 0 else f'Œµ = {eps}'\n",
        "    ax2.plot(pct_optimal, color=COLORS[name], label=label, linewidth=1.5)\n",
        "\n",
        "ax2.set_xlabel('Steps', fontsize=11)\n",
        "ax2.set_ylabel('%\\nOptimal\\naction', fontsize=11)\n",
        "ax2.set_xlim(0, 1000)\n",
        "ax2.set_ylim(0, 100)\n",
        "ax2.legend(loc='lower right')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "fig.suptitle('Figure 2.2: Average performance of Œµ-greedy action-value methods\\non the 10-armed testbed',\n",
        "             fontsize=12, y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze the results\n",
        "analysis_text = \"<strong>Key Observations:</strong><br>\"\n",
        "analysis_text += \"‚Ä¢ <strong>Greedy (Œµ=0):</strong> Fast initial improvement but plateaus early due to no exploration<br>\"\n",
        "analysis_text += \"‚Ä¢ <strong>Œµ=0.01:</strong> Slower start but steady long-term improvement<br>\"\n",
        "analysis_text += \"‚Ä¢ <strong>Œµ=0.1:</strong> More exploration leads to better long-term performance<br><br>\"\n",
        "analysis_text += \"<strong>Trade-off Insight:</strong> More exploration sacrifices short-term reward for long-term optimality\"\n",
        "\n",
        "pretty_print(\"üéØ Figure 2.2 Analysis\", analysis_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4nhYjVC-EsM"
      },
      "source": [
        "## Part 4: Optimistic Initial Values (Figure 2.3)\n",
        "\n",
        "### Theoretical Motivation\n",
        "\n",
        "**Optimistic initialization** is an elegant way to encourage exploration without explicit randomness. The key insights:\n",
        "\n",
        "1. **Optimism in the face of uncertainty**: Start with unrealistically high value estimates\n",
        "2. **Disappointment drives exploration**: When optimistic estimates are too high, the agent will try other actions\n",
        "3. **Automatic exploration**: Even greedy action selection becomes exploratory initially\n",
        "\n",
        "### Mathematical Framework\n",
        "\n",
        "Instead of initializing $Q_1(a) = 0$, we use $Q_1(a) = Q_0$ where $Q_0 > \\max_a q_*(a)$.\n",
        "\n",
        "**Constant Step Size**: For non-stationary environments, we use:\n",
        "$$Q_{n+1} = Q_n + \\alpha[R_n - Q_n] = (1-\\alpha)Q_n + \\alpha R_n$$\n",
        "\n",
        "This is a weighted average giving weight $(1-\\alpha)^{n-1}$ to the initial estimate and $\\alpha(1-\\alpha)^{n-i-1}$ to the $i$-th reward.\n",
        "\n",
        "### Why It Works\n",
        "\n",
        "- **Initial pessimism**: All actions appear equally good initially, encouraging exploration\n",
        "- **Natural convergence**: As true rewards are observed, estimates converge to realistic values\n",
        "- **No explicit randomness**: Pure greedy selection, but optimism creates exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65jv3SnI-EsM"
      },
      "source": [
        "# ============================================\n",
        "# CELL 11: Optimistic Initial Values Experiment\n",
        "# Purpose: Generate data for Figure 2.3\n",
        "# ============================================\n",
        "\n",
        "def run_optimistic_experiment(epsilon: float, initial_Q: float, alpha: float = 0.1,\n",
        "                             runs: int = 2000, steps: int = 1000) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Run experiment with specified initial values and constant step size\n",
        "\n",
        "    Key differences from sample average method:\n",
        "    1. Initial estimates can be set to any value (optimistic or realistic)\n",
        "    2. Uses constant step size Œ± = 0.1 instead of decreasing 1/n\n",
        "    3. Recent rewards have more influence than distant ones\n",
        "\n",
        "    Mathematical update rule:\n",
        "    Q_n+1 = Q_n + Œ±[R_n - Q_n]\n",
        "         = (1-Œ±)Q_n + Œ±R_n\n",
        "\n",
        "    This is exponential recency-weighted average, giving weight:\n",
        "    - (1-Œ±)^n to initial estimate Q_1\n",
        "    - Œ±(1-Œ±)^(n-i) to i-th reward\n",
        "\n",
        "    Args:\n",
        "        epsilon: Exploration probability\n",
        "        initial_Q: Initial action value estimates Q_1(a)\n",
        "        alpha: Constant step size parameter\n",
        "        runs: Number of independent experiments\n",
        "        steps: Number of time steps per run\n",
        "\n",
        "    Returns:\n",
        "        Percentage of optimal actions at each step\n",
        "    \"\"\"\n",
        "    all_optimal = np.zeros((runs, steps))\n",
        "\n",
        "    for run in range(runs):\n",
        "        # Initialize new bandit problem\n",
        "        q_true = create_bandit()\n",
        "        optimal_action = np.argmax(q_true)\n",
        "\n",
        "        # Initialize with specified initial values\n",
        "        # This is the key difference: optimistic vs realistic initialization\n",
        "        Q = np.ones(K) * initial_Q  # All actions start with same estimate\n",
        "        N = np.zeros(K)  # Track selections (though not used with constant Œ±)\n",
        "\n",
        "        # Run the algorithm\n",
        "        for step in range(steps):\n",
        "            # Select action using Œµ-greedy policy\n",
        "            action = epsilon_greedy(Q, epsilon)\n",
        "\n",
        "            # Get reward from environment\n",
        "            reward = get_reward(action, q_true)\n",
        "\n",
        "            # Update with constant step size (key difference)\n",
        "            update_estimates(Q, N, action, reward, alpha=alpha)\n",
        "\n",
        "            # Record if optimal action was selected\n",
        "            all_optimal[run, step] = (action == optimal_action)\n",
        "\n",
        "    return np.mean(all_optimal, axis=0) * 100\n",
        "\n",
        "# Run the two key experiments for Figure 2.3\n",
        "pretty_print(\"üöÄ Running Optimistic Initialization Experiments\",\n",
        "             \"Comparing optimistic greedy vs realistic Œµ-greedy with constant step size Œ±=0.1\")\n",
        "\n",
        "print(\"  Experiment 1: Optimistic greedy (Q‚ÇÅ=5, Œµ=0)...\")\n",
        "optimistic_greedy = run_optimistic_experiment(epsilon=0, initial_Q=5, alpha=0.1)\n",
        "\n",
        "print(\"  Experiment 2: Realistic Œµ-greedy (Q‚ÇÅ=0, Œµ=0.1)...\")\n",
        "realistic_egreedy = run_optimistic_experiment(epsilon=0.1, initial_Q=0, alpha=0.1)\n",
        "\n",
        "# Compare final performance\n",
        "opt_final = optimistic_greedy[-1]\n",
        "real_final = realistic_egreedy[-1]\n",
        "\n",
        "pretty_print(\"üìä Final Performance Comparison\",\n",
        "             f\"Optimistic Greedy (Q‚ÇÅ=5, Œµ=0): {opt_final:.1f}% optimal actions<br>\" +\n",
        "             f\"Realistic Œµ-greedy (Q‚ÇÅ=0, Œµ=0.1): {real_final:.1f}% optimal actions<br>\" +\n",
        "             f\"<strong>Winner:</strong> {'Optimistic' if opt_final > real_final else 'Realistic'} method\")\n",
        "\n",
        "print(\"‚úÖ Optimistic initialization experiments complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM-wgCrj-EsN"
      },
      "source": [
        "# ============================================\n",
        "# CELL 12: Plot Figure 2.3 - Optimistic vs Realistic\n",
        "# Purpose: Reproduce Figure 2.3 from Sutton & Barto\n",
        "# ============================================\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Plot optimistic greedy method\n",
        "plt.plot(optimistic_greedy, color=COLORS['optimistic'],\n",
        "         label='Optimistic, greedy\\n$Q_1 = 5, Œµ = 0$', linewidth=1.5)\n",
        "\n",
        "# Plot realistic Œµ-greedy method\n",
        "plt.plot(realistic_egreedy, color=COLORS['realistic'],\n",
        "         label='Realistic, Œµ-greedy\\n$Q_1 = 0, Œµ = 0.1$', linewidth=1.5)\n",
        "\n",
        "plt.xlabel('Steps', fontsize=11)\n",
        "plt.ylabel('%\\nOptimal\\naction', fontsize=11)\n",
        "plt.xlim(0, 1000)\n",
        "plt.ylim(0, 100)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.title('Figure 2.3: The effect of optimistic initial action-value estimates on the 10-armed testbed.\\n' +\n",
        "          'Both methods used a constant step-size parameter, Œ± = 0.1.',\n",
        "          fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed analysis of the results\n",
        "analysis_text = \"<strong>Why Optimistic Initialization Works:</strong><br>\"\n",
        "analysis_text += \"1. <strong>Initial Phase:</strong> All actions seem equally good (Q‚ÇÅ=5), encouraging systematic exploration<br>\"\n",
        "analysis_text += \"2. <strong>Disappointment:</strong> Actions give lower rewards than expected, agent tries others<br>\"\n",
        "analysis_text += \"3. <strong>Convergence:</strong> Eventually finds and sticks with optimal action<br><br>\"\n",
        "analysis_text += \"<strong>Key Insight:</strong> Optimism creates exploration without explicit randomness!<br>\"\n",
        "analysis_text += \"<strong>Trade-off:</strong> Better long-term performance but requires tuning initial values\"\n",
        "\n",
        "pretty_print(\"üéØ Figure 2.3 Analysis\", analysis_text)\n",
        "\n",
        "# Show the exploration pattern differences\n",
        "pattern_text = \"<strong>Exploration Patterns:</strong><br>\"\n",
        "pattern_text += \"‚Ä¢ <strong>Optimistic:</strong> Systematic early exploration, then focused exploitation<br>\"\n",
        "pattern_text += \"‚Ä¢ <strong>Œµ-greedy:</strong> Random exploration throughout, more consistent but less efficient<br><br>\"\n",
        "pattern_text += \"<strong>When to Use Each:</strong><br>\"\n",
        "pattern_text += \"‚Ä¢ <strong>Optimistic:</strong> When you can set good initial values and want fast convergence<br>\"\n",
        "pattern_text += \"‚Ä¢ <strong>Œµ-greedy:</strong> When problem characteristics are unknown or non-stationary\"\n",
        "\n",
        "pretty_print(\"üîç Method Comparison\", pattern_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zmouew8-EsN"
      },
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Lab Summary</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>Key Findings:</strong></p>\n",
        "        <ul style=\"margin: 10px 0; padding-left: 20px;\">\n",
        "            <li><strong>Exploration is Essential:</strong> Pure greedy methods often fail to find optimal actions</li>\n",
        "            <li><strong>Œµ-greedy Balance:</strong> Œµ = 0.1 provides good long-term performance in stationary environments</li>\n",
        "            <li><strong>Optimistic Initialization:</strong> Can outperform Œµ-greedy by encouraging systematic early exploration</li>\n",
        "            <li><strong>Step Size Matters:</strong> Constant Œ± gives more weight to recent observations</li>\n",
        "        </ul>\n",
        "        \n",
        "        <p><strong>Algorithmic Insights:</strong></p>\n",
        "        <ul style=\"margin: 10px 0; padding-left: 20px;\">\n",
        "            <li><strong>Sample Average:</strong> Q_n+1 = Q_n + (1/n)[R_n - Q_n] converges to true values</li>\n",
        "            <li><strong>Constant Step Size:</strong> Q_n+1 = Q_n + Œ±[R_n - Q_n] adapts to recent changes</li>\n",
        "            <li><strong>Exploration Strategies:</strong> Random (Œµ-greedy) vs Systematic (optimistic)</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li><strong>Scalability:</strong> How would performance change with different numbers of arms (k = 2, 100, 1000)?</li>\n",
        "        <li><strong>Non-stationarity:</strong> What happens when true action values q*(a) change over time?</li>\n",
        "        <li><strong>Adaptive Œµ:</strong> How could you adapt Œµ over time for better performance?</li>\n",
        "        <li><strong>Initialization:</strong> How do you choose good optimistic initial values in practice?</li>\n",
        "        <li><strong>Applications:</strong> Where would you use each method in real-world scenarios?</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #e8f5e8; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #28a745;\">\n",
        "    <h3 style=\"color: #28a745; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Extensions and Advanced Topics</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>Beyond Basic Bandits:</strong></p>\n",
        "        <ul style=\"margin: 10px 0; padding-left: 20px;\">\n",
        "            <li><strong>Upper Confidence Bound (UCB):</strong> A_t = argmax[Q_t(a) + c‚àö(ln t/N_t(a))]</li>\n",
        "            <li><strong>Thompson Sampling:</strong> Bayesian approach using probability matching</li>\n",
        "            <li><strong>Gradient Bandits:</strong> Learn action preferences rather than values</li>\n",
        "            <li><strong>Contextual Bandits:</strong> Actions depend on observed context/state</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">üéì End of Lab 2: Multi-Armed Bandits | Next: Finite Markov Decision Processes</p>\n",
        "</div>"
      ]
    }
  ]
}